{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Thunder speech A Hackable speech recognition library. What to expect from this project: End-to-end speech recognition models Simple fine-tuning to new languages Inference support as a first-class feature Developer oriented api What it's not: A general-purpose speech toolkit A collection of complex systems that require thousands of gpu-hours and expert knowledge, only focusing on the state-of-the-art results Quick usage guide Install Install the library from PyPI: pip install thunder-speech Load the model and train it from thunder.registry import load_pretrained from thunder.quartznet.compatibility import QuartznetCheckpoint # Tab completion works to discover other QuartznetCheckpoint.* module = load_pretrained ( QuartznetCheckpoint . QuartzNet5x5LS_En ) # It also accepts the string identifier module = load_pretrained ( \"QuartzNet5x5LS_En\" ) # Or models from the huggingface hub module = load_pretrained ( \"facebook/wav2vec2-large-960h\" ) Export to a pure pytorch model using torchscript module . to_torchscript ( \"model_ready_for_inference.pt\" ) # Optional step: also export audio loading pipeline from thunder.data.dataset import AudioFileLoader loader = AudioFileLoader ( sample_rate = 16000 ) scripted_loader = torch . jit . script ( loader ) scripted_loader . save ( \"audio_loader.pt\" ) Run inference in production import torch import torchaudio model = torch . jit . load ( \"model_ready_for_inference.pt\" ) loader = torch . jit . load ( \"audio_loader.pt\" ) # Open audio audio = loader ( \"audio_file.wav\" ) # transcriptions is a list of strings with the captions. transcriptions = model . predict ( audio ) More quick tips If you want to know how to access the raw probabilities and decode manually or fine-tune the models you can access the documentation here . Contributing The first step to contribute is to do an editable installation of the library: git clone https://github.com/scart97/thunder-speech.git cd thunder-speech poetry install pre-commit install Then, make sure that everything is working. You can run the test suit, that is based on pytest: RUN_SLOW=1 poetry run pytest Here the RUN_SLOW flag is used to run all the tests, including the ones that might download checkpoints or do small training runs and are marked as slow. If you don't have a CUDA capable gpu, some tests will be unconditionally skipped. Influences This library has heavy influence of the best practices in the pytorch ecosystem. The original model code, including checkpoints, is based on the NeMo ASR toolkit. From there also came the inspiration for the fine-tuning and prediction api's. The data loading and processing is loosely based on my experience using fast.ai. It tries to decouple transforms that happen at the item level from the ones that are efficiently implemented for the whole batch at the GPU. Also, the idea that default parameters should be great. The overall organization of code and decoupling follows the pytorch-lightning ideals, with self-contained modules that try to reduce the boilerplate necessary. Finally, the transformers library inspired the simple model implementations, with a clear separation in folders containing the specific code that you need to understand each architecture and preprocessing, and their strong test suit.","title":"Home"},{"location":"#thunder-speech","text":"A Hackable speech recognition library. What to expect from this project: End-to-end speech recognition models Simple fine-tuning to new languages Inference support as a first-class feature Developer oriented api What it's not: A general-purpose speech toolkit A collection of complex systems that require thousands of gpu-hours and expert knowledge, only focusing on the state-of-the-art results","title":"Thunder speech"},{"location":"#quick-usage-guide","text":"","title":"Quick usage guide"},{"location":"#install","text":"Install the library from PyPI: pip install thunder-speech","title":"Install"},{"location":"#load-the-model-and-train-it","text":"from thunder.registry import load_pretrained from thunder.quartznet.compatibility import QuartznetCheckpoint # Tab completion works to discover other QuartznetCheckpoint.* module = load_pretrained ( QuartznetCheckpoint . QuartzNet5x5LS_En ) # It also accepts the string identifier module = load_pretrained ( \"QuartzNet5x5LS_En\" ) # Or models from the huggingface hub module = load_pretrained ( \"facebook/wav2vec2-large-960h\" )","title":"Load the model and train it"},{"location":"#export-to-a-pure-pytorch-model-using-torchscript","text":"module . to_torchscript ( \"model_ready_for_inference.pt\" ) # Optional step: also export audio loading pipeline from thunder.data.dataset import AudioFileLoader loader = AudioFileLoader ( sample_rate = 16000 ) scripted_loader = torch . jit . script ( loader ) scripted_loader . save ( \"audio_loader.pt\" )","title":"Export to a pure pytorch model using torchscript"},{"location":"#run-inference-in-production","text":"import torch import torchaudio model = torch . jit . load ( \"model_ready_for_inference.pt\" ) loader = torch . jit . load ( \"audio_loader.pt\" ) # Open audio audio = loader ( \"audio_file.wav\" ) # transcriptions is a list of strings with the captions. transcriptions = model . predict ( audio )","title":"Run inference in production"},{"location":"#more-quick-tips","text":"If you want to know how to access the raw probabilities and decode manually or fine-tune the models you can access the documentation here .","title":"More quick tips"},{"location":"#contributing","text":"The first step to contribute is to do an editable installation of the library: git clone https://github.com/scart97/thunder-speech.git cd thunder-speech poetry install pre-commit install Then, make sure that everything is working. You can run the test suit, that is based on pytest: RUN_SLOW=1 poetry run pytest Here the RUN_SLOW flag is used to run all the tests, including the ones that might download checkpoints or do small training runs and are marked as slow. If you don't have a CUDA capable gpu, some tests will be unconditionally skipped.","title":"Contributing"},{"location":"#influences","text":"This library has heavy influence of the best practices in the pytorch ecosystem. The original model code, including checkpoints, is based on the NeMo ASR toolkit. From there also came the inspiration for the fine-tuning and prediction api's. The data loading and processing is loosely based on my experience using fast.ai. It tries to decouple transforms that happen at the item level from the ones that are efficiently implemented for the whole batch at the GPU. Also, the idea that default parameters should be great. The overall organization of code and decoupling follows the pytorch-lightning ideals, with self-contained modules that try to reduce the boilerplate necessary. Finally, the transformers library inspired the simple model implementations, with a clear separation in folders containing the specific code that you need to understand each architecture and preprocessing, and their strong test suit.","title":"Influences"},{"location":"Custom%20Data/","text":"Writing a custom data pipeline There's a BaseSpeechDataset class that can be used as base to load the data. The library expects that each element in the dataset will be a tuple (audio_tensor, text_label), where the audio tensor has shape (channels, time) and text_label is the corresponding label as a string. The BaseSpeechDataset has two important properties: 1. A list (or iterable) .items , that has all the metadata to load every item in the dataset 2. The .loader module. That is a pytorch class that uses torchaudio to load audio tensors and can apply resampling and mono conversion. It was designed to be exported independently of the dataset, so that the same data loading can be used during inference. To get each element in the dataset, the following code is used, and each function call can be overwritten to control functionality: class BaseSpeechDataset ( Dataset ): def __getitem__ ( self , index : int ) -> Tuple [ Tensor , str ]: item = self . get_item ( index ) # Dealing with input audio , sr = self . open_audio ( item ) audio = self . preprocess_audio ( audio , sr ) # Dealing with output text = self . open_text ( item ) text = self . preprocess_text ( text ) return audio , text The flow of loading the data happens as follows: self.get_item is called with a specific index. It uses self.items to return the specific metadata to that example All the metadata is sent to self.open_audio . The relevant subset is used to load the audio tensor and corresponding sample rate, using self.loader.open_audio(...) Inside self.preprocess_audio the audio tensor is resampled and converted to mono if necessary using self.loader.preprocess_audio(...) . At this point, any augmentation that happens at the signal level to individual items can be applied. Only the audio tensor is returned, because it's assumed that every audio in the dataset will be resampled to the same sample rate self.open_text uses the same metadata to open the corresponding text label self.preprocess_text can be used to apply any transform directly to the text. Common options are lower case, expanding contractions ( I'm becomes I am ), expanding numbers ( 42 becomes forty two ) and removing punctuation Example: Loading data from nemo This example will implement thunder.data.datamodule.ManifestDatamodule and thunder.data.dataset.ManifestSpeechDataset . Load source The nemo manifest file follows the Json lines format, where each line is a valid json containing the metadata relevant to one example: {\"audio_filepath\": \"commonvoice/pt/train/22026127.mp3\", \"duration\": 4.32, \"text\": \"Quatro\"} {\"audio_filepath\": \"commonvoice/pt/train/23920071.mp3\", \"duration\": 2.256, \"text\": \"Oito\"} {\"audio_filepath\": \"commonvoice/pt/train/20272843.mp3\", \"duration\": 2.544, \"text\": \"Eu vou desligar\"} We can load this using the stdlib json and pathlib modules: from pathlib import Path import json file = Path ( \"manifest.json\" ) # Reading from the manifest file items = [ json . loads ( line ) for line in file . read_text () . strip () . splitlines ()] The result is a list, where each element is a dictionary with the relevant data to a single example in the dataset. Let's start to wrap this code inside a BaseSpeechDataset : from pathlib import Path import json from thunder.data.dataset import BaseSpeechDataset class ManifestSpeechDataset ( BaseSpeechDataset ): def __init__ ( self , file : Union [ str , Path ], force_mono : bool , sample_rate : int ): file = Path ( file ) items = [ json . loads ( line ) for line in file . read_text () . strip () . splitlines ()] super () . __init__ ( items , force_mono = force_mono , sample_rate = sample_rate ) Load audio We know that the \"audio_filepath\" key is related to the input: from pathlib import Path import json from thunder.data.dataset import BaseSpeechDataset class ManifestSpeechDataset ( BaseSpeechDataset ): def __init__ ( self , file : Union [ str , Path ], force_mono : bool , sample_rate : int ): file = Path ( file ) items = [ json . loads ( line ) for line in file . read_text () . strip () . splitlines ()] super () . __init__ ( items , force_mono = force_mono , sample_rate = sample_rate ) def open_audio ( self , item : dict ) -> Tuple [ Tensor , int ]: return self . loader . open_audio ( item [ \"audio_filepath\" ]) Load text The text is already loaded inside the \"text\" key: from pathlib import Path import json from thunder.data.dataset import BaseSpeechDataset class ManifestSpeechDataset ( BaseSpeechDataset ): def __init__ ( self , file : Union [ str , Path ], force_mono : bool , sample_rate : int ): file = Path ( file ) items = [ json . loads ( line ) for line in file . read_text () . strip () . splitlines ()] super () . __init__ ( items , force_mono = force_mono , sample_rate = sample_rate ) def open_audio ( self , item : dict ) -> Tuple [ Tensor , int ]: return self . loader . open_audio ( item [ \"audio_filepath\" ]) def open_text ( self , item : dict ) -> str : return item [ \"text\" ] Fix text The only text processing that will be applied in this example is transforming all the characters to lowercase: from pathlib import Path import json from thunder.data.dataset import BaseSpeechDataset class ManifestSpeechDataset ( BaseSpeechDataset ): def __init__ ( self , file : Union [ str , Path ], force_mono : bool , sample_rate : int ): file = Path ( file ) items = [ json . loads ( line ) for line in file . read_text () . strip () . splitlines ()] super () . __init__ ( items , force_mono = force_mono , sample_rate = sample_rate ) def open_audio ( self , item : dict ) -> Tuple [ Tensor , int ]: return self . loader . open_audio ( item [ \"audio_filepath\" ]) def open_text ( self , item : dict ) -> str : return item [ \"text\" ] def preprocess_text ( self , text : str ) -> str : return text . lower () Datamodule with sources Just wrap the datasets inside a BaseDataModule . Implement get_dataset to return the dataset for each split. from thunder.data.datamodule import BaseDataModule class ManifestDatamodule ( BaseDataModule ): def __init__ ( self , train_manifest : str , val_manifest : str , test_manifest : str , force_mono : bool = True , sample_rate : int = 16000 , batch_size : int = 10 , num_workers : int = 8 , ): super () . __init__ ( batch_size = batch_size , num_workers = num_workers , ) self . manifest_mapping = { \"train\" : train_manifest , \"valid\" : val_manifest , \"test\" : test_manifest , } self . force_mono = force_mono self . sample_rate = sample_rate def get_dataset ( self , split : str ) -> ManifestSpeechDataset : return ManifestSpeechDataset ( self . manifest_mapping [ split ], self . force_mono , self . sample_rate ) Using the datamodule datamodule = ManifestDatamodule ( \"train_manifest.json\" , \"val_manifest.json\" , \"test_manifest.json\" , batch_size = 32 )","title":"Writing a custom data pipeline"},{"location":"Custom%20Data/#writing-a-custom-data-pipeline","text":"There's a BaseSpeechDataset class that can be used as base to load the data. The library expects that each element in the dataset will be a tuple (audio_tensor, text_label), where the audio tensor has shape (channels, time) and text_label is the corresponding label as a string. The BaseSpeechDataset has two important properties: 1. A list (or iterable) .items , that has all the metadata to load every item in the dataset 2. The .loader module. That is a pytorch class that uses torchaudio to load audio tensors and can apply resampling and mono conversion. It was designed to be exported independently of the dataset, so that the same data loading can be used during inference. To get each element in the dataset, the following code is used, and each function call can be overwritten to control functionality: class BaseSpeechDataset ( Dataset ): def __getitem__ ( self , index : int ) -> Tuple [ Tensor , str ]: item = self . get_item ( index ) # Dealing with input audio , sr = self . open_audio ( item ) audio = self . preprocess_audio ( audio , sr ) # Dealing with output text = self . open_text ( item ) text = self . preprocess_text ( text ) return audio , text The flow of loading the data happens as follows: self.get_item is called with a specific index. It uses self.items to return the specific metadata to that example All the metadata is sent to self.open_audio . The relevant subset is used to load the audio tensor and corresponding sample rate, using self.loader.open_audio(...) Inside self.preprocess_audio the audio tensor is resampled and converted to mono if necessary using self.loader.preprocess_audio(...) . At this point, any augmentation that happens at the signal level to individual items can be applied. Only the audio tensor is returned, because it's assumed that every audio in the dataset will be resampled to the same sample rate self.open_text uses the same metadata to open the corresponding text label self.preprocess_text can be used to apply any transform directly to the text. Common options are lower case, expanding contractions ( I'm becomes I am ), expanding numbers ( 42 becomes forty two ) and removing punctuation","title":"Writing a custom data pipeline"},{"location":"Custom%20Data/#example-loading-data-from-nemo","text":"This example will implement thunder.data.datamodule.ManifestDatamodule and thunder.data.dataset.ManifestSpeechDataset .","title":"Example: Loading data from nemo"},{"location":"Custom%20Data/#load-source","text":"The nemo manifest file follows the Json lines format, where each line is a valid json containing the metadata relevant to one example: {\"audio_filepath\": \"commonvoice/pt/train/22026127.mp3\", \"duration\": 4.32, \"text\": \"Quatro\"} {\"audio_filepath\": \"commonvoice/pt/train/23920071.mp3\", \"duration\": 2.256, \"text\": \"Oito\"} {\"audio_filepath\": \"commonvoice/pt/train/20272843.mp3\", \"duration\": 2.544, \"text\": \"Eu vou desligar\"} We can load this using the stdlib json and pathlib modules: from pathlib import Path import json file = Path ( \"manifest.json\" ) # Reading from the manifest file items = [ json . loads ( line ) for line in file . read_text () . strip () . splitlines ()] The result is a list, where each element is a dictionary with the relevant data to a single example in the dataset. Let's start to wrap this code inside a BaseSpeechDataset : from pathlib import Path import json from thunder.data.dataset import BaseSpeechDataset class ManifestSpeechDataset ( BaseSpeechDataset ): def __init__ ( self , file : Union [ str , Path ], force_mono : bool , sample_rate : int ): file = Path ( file ) items = [ json . loads ( line ) for line in file . read_text () . strip () . splitlines ()] super () . __init__ ( items , force_mono = force_mono , sample_rate = sample_rate )","title":"Load source"},{"location":"Custom%20Data/#load-audio","text":"We know that the \"audio_filepath\" key is related to the input: from pathlib import Path import json from thunder.data.dataset import BaseSpeechDataset class ManifestSpeechDataset ( BaseSpeechDataset ): def __init__ ( self , file : Union [ str , Path ], force_mono : bool , sample_rate : int ): file = Path ( file ) items = [ json . loads ( line ) for line in file . read_text () . strip () . splitlines ()] super () . __init__ ( items , force_mono = force_mono , sample_rate = sample_rate ) def open_audio ( self , item : dict ) -> Tuple [ Tensor , int ]: return self . loader . open_audio ( item [ \"audio_filepath\" ])","title":"Load audio"},{"location":"Custom%20Data/#load-text","text":"The text is already loaded inside the \"text\" key: from pathlib import Path import json from thunder.data.dataset import BaseSpeechDataset class ManifestSpeechDataset ( BaseSpeechDataset ): def __init__ ( self , file : Union [ str , Path ], force_mono : bool , sample_rate : int ): file = Path ( file ) items = [ json . loads ( line ) for line in file . read_text () . strip () . splitlines ()] super () . __init__ ( items , force_mono = force_mono , sample_rate = sample_rate ) def open_audio ( self , item : dict ) -> Tuple [ Tensor , int ]: return self . loader . open_audio ( item [ \"audio_filepath\" ]) def open_text ( self , item : dict ) -> str : return item [ \"text\" ]","title":"Load text"},{"location":"Custom%20Data/#fix-text","text":"The only text processing that will be applied in this example is transforming all the characters to lowercase: from pathlib import Path import json from thunder.data.dataset import BaseSpeechDataset class ManifestSpeechDataset ( BaseSpeechDataset ): def __init__ ( self , file : Union [ str , Path ], force_mono : bool , sample_rate : int ): file = Path ( file ) items = [ json . loads ( line ) for line in file . read_text () . strip () . splitlines ()] super () . __init__ ( items , force_mono = force_mono , sample_rate = sample_rate ) def open_audio ( self , item : dict ) -> Tuple [ Tensor , int ]: return self . loader . open_audio ( item [ \"audio_filepath\" ]) def open_text ( self , item : dict ) -> str : return item [ \"text\" ] def preprocess_text ( self , text : str ) -> str : return text . lower ()","title":"Fix text"},{"location":"Custom%20Data/#datamodule-with-sources","text":"Just wrap the datasets inside a BaseDataModule . Implement get_dataset to return the dataset for each split. from thunder.data.datamodule import BaseDataModule class ManifestDatamodule ( BaseDataModule ): def __init__ ( self , train_manifest : str , val_manifest : str , test_manifest : str , force_mono : bool = True , sample_rate : int = 16000 , batch_size : int = 10 , num_workers : int = 8 , ): super () . __init__ ( batch_size = batch_size , num_workers = num_workers , ) self . manifest_mapping = { \"train\" : train_manifest , \"valid\" : val_manifest , \"test\" : test_manifest , } self . force_mono = force_mono self . sample_rate = sample_rate def get_dataset ( self , split : str ) -> ManifestSpeechDataset : return ManifestSpeechDataset ( self . manifest_mapping [ split ], self . force_mono , self . sample_rate )","title":"Datamodule with sources"},{"location":"Custom%20Data/#using-the-datamodule","text":"datamodule = ManifestDatamodule ( \"train_manifest.json\" , \"val_manifest.json\" , \"test_manifest.json\" , batch_size = 32 )","title":"Using the datamodule"},{"location":"Ultimate%20guide/","text":"The ultimate guide to speech recognition This guide is meant to give you all the steps necessary to achieve a decent (but not necessarily state-of-the-art) speech recognition system in a new language. Gathering the data Speech recognition systems are really sensitive to the quality of data used to train them. Also, they usually require from hundreds to thousands of hours depending on the quality expected. Some good sources for data are Mozilla commonvoice , the OpenSLR project or Tatoeba . After you download some initial data, there's a number of data quality problems that are expected and need to be fixed if you want to increase the performance of the trained models. First, list all the audio files by increasing size and check if there's any corrupted file (usually they're very small). Remove them from the training data. Then install sox , that's the best tool to inspect and convert audio files. It should come with a basic tool to inspect any file in the terminal, called soxi . As an example: $ soxi example_file.wav Input File : 'example_file.wav' Channels : 1 Sample Rate : 16000 Precision : 16-bit Duration : 00:00:04.27 = 94053 samples ~ 319.908 CDDA sectors File Size : 188k Bit Rate : 353k Sample Encoding: 16-bit Signed Integer PCM That's the usual format of files used in speech recognition research. Wav files, encoded with a 16-bit PCM codec and a sample rate of 16 kHz. The file format and codec can vary and will only affect the quality of the audio, but the sample rate is the essential one. Trained models will only predict well on audios that have the same sample rate as the data the model was trained on. Any file with a different sample rate must be resampled at the file level with sox or after loading with torchaudio. Sox has more capabilities than just listing audio metadata. It can read almost any file format and convert to others. If you have a mp3 file at 44.1 kHz, and want to convert into the usual wav format above, you can use: sox input_file.mp3 -r 16000 -c 1 -b 16 output_file.wav The flags used represent: -r 16000 : resample to a 16kHz sample rate -c 1 : convert to mono (1 channel) -b 16 : convert to PCM 16-bit output_file.wav : Sox understands that the output will be wav just by the file extension Ideally all the training and inference audio files should have the same characteristics, so it's a good idea to transform them into a common format before training. As the wav format does not have any compression, the resulting data will demand a huge HDD space. If that's a problem, you can instead convert the files to mp3, that way you lose a small percentage of the performance but can achieve up to 10x smaller dataset sizes. Now take a look at the labels. We are searching for a number of different problems here: Strange symbols: can easily find if you list all unique characters in the dataset Text in another language: remove these files Additional info that should not be there, like speaker identification as part of the transcription (common in subtitles) Regional/temporal differences that can cause the same words to have multiple written forms: mixing data from multiple countries that speak the same language, or using labels that came from old books Try to fix those label problems, or remove them from the training set if you have lots of data. Don't spend weeks just looking at the data, but have a small subset that you can trust is properly cleaned, even if that means manually labeling again. After you train the first couple of models, it's possible to use the model itself to help find problems in the training data. Loading the data The first step to train a speech recognition model is to load the collected data into the specific format the model expects. Usually, data from different sources will have unique ways that the label is encoded. Sometimes it's multiple .txt files, one for each audio. Another popular option is to have some .csv or .json file with the metadata of multiple examples. The recommendation here is that you convert all the labels to the same format before training. This will simplify the data loading, and any tools that you build to inspect the data can be shared between datasets. There's no obvious choice here, but the nemo manifest format has some pros: It's easy to save and version the manifest, to have fully reproducible training An increasing number of tools support it, including NeMo and Label Studio . The code to load the metadata is simple and intuitive It can store additional metadata as necessary In this format, each of the train/validation/test splits has one file containing the metadata. It has the extension .json , and contains one json in each line with the relevant data to one example, following the Json lines format: {\"audio_filepath\": \"commonvoice/pt/train/22026127.mp3\", \"duration\": 4.32, \"text\": \"Quatro\"} {\"audio_filepath\": \"commonvoice/pt/train/23920071.mp3\", \"duration\": 2.256, \"text\": \"Oito\"} {\"audio_filepath\": \"commonvoice/pt/train/20272843.mp3\", \"duration\": 2.544, \"text\": \"Eu vou desligar\"} These three keys for each example are required: audio_filepath : Contains the path to the input audio duration : has the duration of the audio, in seconds text : that's the corresponding label Make sure that each example starts and end on the same line. This is one example of invalid manifest: {\"audio_filepath\": \"example1.mp3\", \"duration\": 1.0, \"text\": \"This label starts in one line but has multiple line breaks making this manifest invalid\"} {\"audio_filepath\": \"example2.mp3\", \"duration\": 2.0, \"text\": \"this label is really long similar to the one above it, but it's contained into a single line making it valid\"} To load this data, the corresponding LightningModule is already implemented: from thunder.data.datamodule import ManifestDatamodule datamodule = ManifestDatamodule ( \"train_manifest.json\" , \"val_manifest.json\" , \"test_manifest.json\" , batch_size = 32 ) First train For this first train, you should only try to overfit one batch. This is the simplest test, and if you can't get past it then anything more complex that you try will be wasted time. To do it, try to load a training dataset with only one batch worth of data. The validation/test sets can be as usual, you will ignore them at this step. As we are using pytorch lightning, there's a trainer flag to limit the number of training batches ( overfit_batches=1 ) that can be used. Before you run the training, disable any augmentation, regularization and advanced stuff like learning rate scheduling. You can start with either a pretrained model, or a clean new one, but either way don't freeze any parameters, just let it all train. Start the training, and you should see the loss follow a pattern where, the more time you let it run, the lower the final value will be. This means that small bumps will happen, but it will always recover and keep going down. The ideal point is where you run the prediction on the batch that you overfit, and the model doesn't make a single mistake. Expected train loss: The validation loss is not important at this stage, we are trying to overfit on purpose to check if the model is learning correctly. It's possible to notice that it improved in the first epoch, but instantly went back up and keep increasing as the training goes. Expected validation loss: Note that if you're using the overfit_batches flag, the same training batch will be used during the validation step, and the validation loss/metrics will follow the same pattern as the training loss instead. Some problems that can happen: The train loss doesn't go below a certain value : Check if it' always the same batch being trained. It can happen when you forget to disable the shuffling in the train dataloader. The train loss is negative : There's a blank in the target text, find and remove it. Blanks should only be produced by the model, never at the labels. There are no predictions at all : let it train for more time Still, there are no predictions after a long time : Check if the target texts are being processed correctly. Inside the training step, decode the target text and assert that it returns what you expect The train loss just keep increasing : try to lower the learning rate Second train Now repeat the first training, but with around 10 hours of data. This number depends on the hardware that you have available, but something that gives you 2 minute epochs is a good amount. This time, you're not trying to overfit anymore. The validation loss will start to get lower, and the metrics will improve compared to the first training. Quickly, the model will reach the point where the data is enough, and it will start to overfit to the training data. Expected train loss: Expected validation loss: At this point the model is still overfitting, but it should be way less than the first train. The objective of this training round is to confirm that the model performance improves as the amount of training data increases, and also to find any problems that were not caught when using only a single batch. Scaling to the whole dataset Now that we confirmed the model is training properly, it's time to use all the training data available. Remember that speech recognition models are really sensitive to the quality of the labels, and transcription errors across the dataset can make the training unstable. The improvements from cleaning the labels and collecting more data can be an order of magnitude higher than using the new fancy SOTA model of the month with mislabeled data. Some tips at this step: break long audios : more than 25 seconds is usually bad for each train example, it can cause catastrophic forgetting on sequence based models, or out-of-memory errors Use the model to find problems : After the first few models are trained, they can be used to aid while cleaning the data: Sort by train loss descending and manually check the files. This will show the examples where the model is having the most difficult to learn, and that can be caused by bad data or outliers Sort by Character Error Rate (CER) descending and manually check the files. Similar to the test above, but this time sorting by the character error rate. Most of the top audios will be the same, but this test can also show new problematic ones. Sort by CER ascending on the validation/test set to find possible data leak. If there's some data repeated between train and validation/test, the metrics will be lower compared to samples that are new to the model. Watch for the loss spikes during training : Sometimes the loss will have spikes as show in the next figure, where it went from around 0.1 to 1.1 during a single step. This can be caused by mislabeled data, try to log the full batch when this happens and check manually. Reducing overfit To reduce the overfit in trained models, follow the recipe given by fast.ai , in this order of priority: Add more data Data augmentation Generalizable architectures Regularization Reduce architecture complexity While adding more data, try to follow the tips in the Scaling to the whole dataset section to use trained models to find problem in the new data. Also, when doing any of the other steps, it's best to repeat the Second train to quickly confirm that the modified model is still training properly. Deploy! Finally, it's time to deploy the trained model. This includes exporting the model, writing the data pipelines, building the server/mobile app where the model will run and monitoring. The recommended way to export models is using torchscript. This will ensure that they preserve correct behaviour, and every model implemented in this library is rigorously tested to be compatible with it. Pytorch tracing and onnx are also tested to export, but those methods have limitations around variable input shapes. The audio loading and preprocessing is also implemented as a nn.Module , so that it can be scripted and exported for inference. It exists as the .loader attribute inside the datasets, or can be directly instantiated from thunder.data.dataset.AudioFileLoader . This way, there's no need to reimplement the input pipeline when deploying the model. As a bonus, the exported models using torchscript remove the dependency on thunder, only pytorch is necessary to run the model, and optionally torchaudio to load the audio data using AudioFileLoader . It's possible to use the torchscript model inside a mobile app. Before exporting, one patch need to be applied so that operations that are not natively supported on mobile are changed to compatible implementations. One example of implementing a simple speech recognition app can be found here . To learn more about deployment and MLOps, there are free courses from made with ml and full stack deep learning that go in depth.","title":"The ultimate guide to speech recognition"},{"location":"Ultimate%20guide/#the-ultimate-guide-to-speech-recognition","text":"This guide is meant to give you all the steps necessary to achieve a decent (but not necessarily state-of-the-art) speech recognition system in a new language.","title":"The ultimate guide to speech recognition"},{"location":"Ultimate%20guide/#gathering-the-data","text":"Speech recognition systems are really sensitive to the quality of data used to train them. Also, they usually require from hundreds to thousands of hours depending on the quality expected. Some good sources for data are Mozilla commonvoice , the OpenSLR project or Tatoeba . After you download some initial data, there's a number of data quality problems that are expected and need to be fixed if you want to increase the performance of the trained models. First, list all the audio files by increasing size and check if there's any corrupted file (usually they're very small). Remove them from the training data. Then install sox , that's the best tool to inspect and convert audio files. It should come with a basic tool to inspect any file in the terminal, called soxi . As an example: $ soxi example_file.wav Input File : 'example_file.wav' Channels : 1 Sample Rate : 16000 Precision : 16-bit Duration : 00:00:04.27 = 94053 samples ~ 319.908 CDDA sectors File Size : 188k Bit Rate : 353k Sample Encoding: 16-bit Signed Integer PCM That's the usual format of files used in speech recognition research. Wav files, encoded with a 16-bit PCM codec and a sample rate of 16 kHz. The file format and codec can vary and will only affect the quality of the audio, but the sample rate is the essential one. Trained models will only predict well on audios that have the same sample rate as the data the model was trained on. Any file with a different sample rate must be resampled at the file level with sox or after loading with torchaudio. Sox has more capabilities than just listing audio metadata. It can read almost any file format and convert to others. If you have a mp3 file at 44.1 kHz, and want to convert into the usual wav format above, you can use: sox input_file.mp3 -r 16000 -c 1 -b 16 output_file.wav The flags used represent: -r 16000 : resample to a 16kHz sample rate -c 1 : convert to mono (1 channel) -b 16 : convert to PCM 16-bit output_file.wav : Sox understands that the output will be wav just by the file extension Ideally all the training and inference audio files should have the same characteristics, so it's a good idea to transform them into a common format before training. As the wav format does not have any compression, the resulting data will demand a huge HDD space. If that's a problem, you can instead convert the files to mp3, that way you lose a small percentage of the performance but can achieve up to 10x smaller dataset sizes. Now take a look at the labels. We are searching for a number of different problems here: Strange symbols: can easily find if you list all unique characters in the dataset Text in another language: remove these files Additional info that should not be there, like speaker identification as part of the transcription (common in subtitles) Regional/temporal differences that can cause the same words to have multiple written forms: mixing data from multiple countries that speak the same language, or using labels that came from old books Try to fix those label problems, or remove them from the training set if you have lots of data. Don't spend weeks just looking at the data, but have a small subset that you can trust is properly cleaned, even if that means manually labeling again. After you train the first couple of models, it's possible to use the model itself to help find problems in the training data.","title":"Gathering the data"},{"location":"Ultimate%20guide/#loading-the-data","text":"The first step to train a speech recognition model is to load the collected data into the specific format the model expects. Usually, data from different sources will have unique ways that the label is encoded. Sometimes it's multiple .txt files, one for each audio. Another popular option is to have some .csv or .json file with the metadata of multiple examples. The recommendation here is that you convert all the labels to the same format before training. This will simplify the data loading, and any tools that you build to inspect the data can be shared between datasets. There's no obvious choice here, but the nemo manifest format has some pros: It's easy to save and version the manifest, to have fully reproducible training An increasing number of tools support it, including NeMo and Label Studio . The code to load the metadata is simple and intuitive It can store additional metadata as necessary In this format, each of the train/validation/test splits has one file containing the metadata. It has the extension .json , and contains one json in each line with the relevant data to one example, following the Json lines format: {\"audio_filepath\": \"commonvoice/pt/train/22026127.mp3\", \"duration\": 4.32, \"text\": \"Quatro\"} {\"audio_filepath\": \"commonvoice/pt/train/23920071.mp3\", \"duration\": 2.256, \"text\": \"Oito\"} {\"audio_filepath\": \"commonvoice/pt/train/20272843.mp3\", \"duration\": 2.544, \"text\": \"Eu vou desligar\"} These three keys for each example are required: audio_filepath : Contains the path to the input audio duration : has the duration of the audio, in seconds text : that's the corresponding label Make sure that each example starts and end on the same line. This is one example of invalid manifest: {\"audio_filepath\": \"example1.mp3\", \"duration\": 1.0, \"text\": \"This label starts in one line but has multiple line breaks making this manifest invalid\"} {\"audio_filepath\": \"example2.mp3\", \"duration\": 2.0, \"text\": \"this label is really long similar to the one above it, but it's contained into a single line making it valid\"} To load this data, the corresponding LightningModule is already implemented: from thunder.data.datamodule import ManifestDatamodule datamodule = ManifestDatamodule ( \"train_manifest.json\" , \"val_manifest.json\" , \"test_manifest.json\" , batch_size = 32 )","title":"Loading the data"},{"location":"Ultimate%20guide/#first-train","text":"For this first train, you should only try to overfit one batch. This is the simplest test, and if you can't get past it then anything more complex that you try will be wasted time. To do it, try to load a training dataset with only one batch worth of data. The validation/test sets can be as usual, you will ignore them at this step. As we are using pytorch lightning, there's a trainer flag to limit the number of training batches ( overfit_batches=1 ) that can be used. Before you run the training, disable any augmentation, regularization and advanced stuff like learning rate scheduling. You can start with either a pretrained model, or a clean new one, but either way don't freeze any parameters, just let it all train. Start the training, and you should see the loss follow a pattern where, the more time you let it run, the lower the final value will be. This means that small bumps will happen, but it will always recover and keep going down. The ideal point is where you run the prediction on the batch that you overfit, and the model doesn't make a single mistake. Expected train loss: The validation loss is not important at this stage, we are trying to overfit on purpose to check if the model is learning correctly. It's possible to notice that it improved in the first epoch, but instantly went back up and keep increasing as the training goes. Expected validation loss: Note that if you're using the overfit_batches flag, the same training batch will be used during the validation step, and the validation loss/metrics will follow the same pattern as the training loss instead. Some problems that can happen: The train loss doesn't go below a certain value : Check if it' always the same batch being trained. It can happen when you forget to disable the shuffling in the train dataloader. The train loss is negative : There's a blank in the target text, find and remove it. Blanks should only be produced by the model, never at the labels. There are no predictions at all : let it train for more time Still, there are no predictions after a long time : Check if the target texts are being processed correctly. Inside the training step, decode the target text and assert that it returns what you expect The train loss just keep increasing : try to lower the learning rate","title":"First train"},{"location":"Ultimate%20guide/#second-train","text":"Now repeat the first training, but with around 10 hours of data. This number depends on the hardware that you have available, but something that gives you 2 minute epochs is a good amount. This time, you're not trying to overfit anymore. The validation loss will start to get lower, and the metrics will improve compared to the first training. Quickly, the model will reach the point where the data is enough, and it will start to overfit to the training data. Expected train loss: Expected validation loss: At this point the model is still overfitting, but it should be way less than the first train. The objective of this training round is to confirm that the model performance improves as the amount of training data increases, and also to find any problems that were not caught when using only a single batch.","title":"Second train"},{"location":"Ultimate%20guide/#scaling-to-the-whole-dataset","text":"Now that we confirmed the model is training properly, it's time to use all the training data available. Remember that speech recognition models are really sensitive to the quality of the labels, and transcription errors across the dataset can make the training unstable. The improvements from cleaning the labels and collecting more data can be an order of magnitude higher than using the new fancy SOTA model of the month with mislabeled data. Some tips at this step: break long audios : more than 25 seconds is usually bad for each train example, it can cause catastrophic forgetting on sequence based models, or out-of-memory errors Use the model to find problems : After the first few models are trained, they can be used to aid while cleaning the data: Sort by train loss descending and manually check the files. This will show the examples where the model is having the most difficult to learn, and that can be caused by bad data or outliers Sort by Character Error Rate (CER) descending and manually check the files. Similar to the test above, but this time sorting by the character error rate. Most of the top audios will be the same, but this test can also show new problematic ones. Sort by CER ascending on the validation/test set to find possible data leak. If there's some data repeated between train and validation/test, the metrics will be lower compared to samples that are new to the model. Watch for the loss spikes during training : Sometimes the loss will have spikes as show in the next figure, where it went from around 0.1 to 1.1 during a single step. This can be caused by mislabeled data, try to log the full batch when this happens and check manually.","title":"Scaling to the whole dataset"},{"location":"Ultimate%20guide/#reducing-overfit","text":"To reduce the overfit in trained models, follow the recipe given by fast.ai , in this order of priority: Add more data Data augmentation Generalizable architectures Regularization Reduce architecture complexity While adding more data, try to follow the tips in the Scaling to the whole dataset section to use trained models to find problem in the new data. Also, when doing any of the other steps, it's best to repeat the Second train to quickly confirm that the modified model is still training properly.","title":"Reducing overfit"},{"location":"Ultimate%20guide/#deploy","text":"Finally, it's time to deploy the trained model. This includes exporting the model, writing the data pipelines, building the server/mobile app where the model will run and monitoring. The recommended way to export models is using torchscript. This will ensure that they preserve correct behaviour, and every model implemented in this library is rigorously tested to be compatible with it. Pytorch tracing and onnx are also tested to export, but those methods have limitations around variable input shapes. The audio loading and preprocessing is also implemented as a nn.Module , so that it can be scripted and exported for inference. It exists as the .loader attribute inside the datasets, or can be directly instantiated from thunder.data.dataset.AudioFileLoader . This way, there's no need to reimplement the input pipeline when deploying the model. As a bonus, the exported models using torchscript remove the dependency on thunder, only pytorch is necessary to run the model, and optionally torchaudio to load the audio data using AudioFileLoader . It's possible to use the torchscript model inside a mobile app. Before exporting, one patch need to be applied so that operations that are not natively supported on mobile are changed to compatible implementations. One example of implementing a simple speech recognition app can be found here . To learn more about deployment and MLOps, there are free courses from made with ml and full stack deep learning that go in depth.","title":"Deploy!"},{"location":"quick%20reference%20guide/","text":"Quick reference guide How to load a Quartznet/Citrinet .nemo file? from thunder.quartznet.compatibility import load_quartznet_checkpoint from thunder.citrinet.compatibility import load_citrinet_checkpoint module = load_quartznet_checkpoint ( \"/path/to/quartznet.nemo\" ) module = load_citrinet_checkpoint ( \"/path/to/citrinet.nemo\" ) How to export models with special restrictions? Case 1: Using Quartznet or Citrinet on platforms that doesnt support FFT (android, onnx): from thunder.registry import load_pretrained from thunder.quartznet.transform import patch_stft import torch module = load_pretrained ( \"QuartzNet5x5LS_En\" ) module . audio_transform = patch_stft ( module . audio_transform ) module . to_torchscript ( \"model_ready_for_inference.pt\" ) Case 2: Wav2vec 2.0 using torchscript from thunder.registry import load_pretrained from thunder.huggingface.compatibility import prepare_scriptable_wav2vec module = load_pretrained ( \"facebook/wav2vec2-large-960h\" ) module = prepare_scriptable_wav2vec ( module ) module . to_torchscript ( \"model_ready_for_inference.pt\" ) What if I want the probabilities instead of the captions during inference? Instead of model.predict(audio) , use just model(audio) import torch import torchaudio model = torch . jit . load ( \"model_ready_for_inference.pt\" ) loader = torch . jit . load ( \"audio_loader.pt\" ) # Open audio audio = loader ( \"audio_file.wav\" ) probs = model ( audio ) # If you also want the transcriptions: transcriptions = model . text_transform . decode_prediction ( probs . argmax ( 1 )) How to finetune a model if I already have the nemo manifests prepared? import pytorch_lightning as pl from thunder.data.datamodule import ManifestDatamodule from thunder.registry import load_pretrained from thunder.callbacks import FinetuneEncoderDecoder dm = ManifestDatamodule ( train_manifest = \"/path/to/train_manifest.json\" , val_manifest = \"/path/to/val_manifest.json\" , test_manifest = \"/path/to/test_manifest.json\" , ) model = load_pretrained ( \"QuartzNet5x5LS_En\" ) trainer = pl . Trainer ( gpus =- 1 , # Use all gpus max_epochs = 10 , callbacks = [ FinetuneEncoderDecoder ( unfreeze_encoder_at_epoch = 1 )], ) trainer . fit ( model = model , datamodule = dm ) How to get the tokens from my dataset? from thunder.text_processing.tokenizer import char_tokenizer , get_most_frequent_tokens my_datamodule = CustomDatamodule ( ... ) my_datamodule . prepare_data () my_datamodule . setup ( None ) train_corpus = \" \" . join ( my_datamodule . train_dataset . all_outputs ()) tokens = get_most_frequent_tokens ( train_corpus , char_tokenizer )","title":"Quick reference guide"},{"location":"quick%20reference%20guide/#quick-reference-guide","text":"","title":"Quick reference guide"},{"location":"quick%20reference%20guide/#how-to-load-a-quartznetcitrinet-nemo-file","text":"from thunder.quartznet.compatibility import load_quartznet_checkpoint from thunder.citrinet.compatibility import load_citrinet_checkpoint module = load_quartznet_checkpoint ( \"/path/to/quartznet.nemo\" ) module = load_citrinet_checkpoint ( \"/path/to/citrinet.nemo\" )","title":"How to load a Quartznet/Citrinet .nemo file?"},{"location":"quick%20reference%20guide/#how-to-export-models-with-special-restrictions","text":"Case 1: Using Quartznet or Citrinet on platforms that doesnt support FFT (android, onnx): from thunder.registry import load_pretrained from thunder.quartznet.transform import patch_stft import torch module = load_pretrained ( \"QuartzNet5x5LS_En\" ) module . audio_transform = patch_stft ( module . audio_transform ) module . to_torchscript ( \"model_ready_for_inference.pt\" ) Case 2: Wav2vec 2.0 using torchscript from thunder.registry import load_pretrained from thunder.huggingface.compatibility import prepare_scriptable_wav2vec module = load_pretrained ( \"facebook/wav2vec2-large-960h\" ) module = prepare_scriptable_wav2vec ( module ) module . to_torchscript ( \"model_ready_for_inference.pt\" )","title":"How to export models with special restrictions?"},{"location":"quick%20reference%20guide/#what-if-i-want-the-probabilities-instead-of-the-captions-during-inference","text":"Instead of model.predict(audio) , use just model(audio) import torch import torchaudio model = torch . jit . load ( \"model_ready_for_inference.pt\" ) loader = torch . jit . load ( \"audio_loader.pt\" ) # Open audio audio = loader ( \"audio_file.wav\" ) probs = model ( audio ) # If you also want the transcriptions: transcriptions = model . text_transform . decode_prediction ( probs . argmax ( 1 ))","title":"What if I want the probabilities instead of the captions during inference?"},{"location":"quick%20reference%20guide/#how-to-finetune-a-model-if-i-already-have-the-nemo-manifests-prepared","text":"import pytorch_lightning as pl from thunder.data.datamodule import ManifestDatamodule from thunder.registry import load_pretrained from thunder.callbacks import FinetuneEncoderDecoder dm = ManifestDatamodule ( train_manifest = \"/path/to/train_manifest.json\" , val_manifest = \"/path/to/val_manifest.json\" , test_manifest = \"/path/to/test_manifest.json\" , ) model = load_pretrained ( \"QuartzNet5x5LS_En\" ) trainer = pl . Trainer ( gpus =- 1 , # Use all gpus max_epochs = 10 , callbacks = [ FinetuneEncoderDecoder ( unfreeze_encoder_at_epoch = 1 )], ) trainer . fit ( model = model , datamodule = dm )","title":"How to finetune a model if I already have the nemo manifests prepared?"},{"location":"quick%20reference%20guide/#how-to-get-the-tokens-from-my-dataset","text":"from thunder.text_processing.tokenizer import char_tokenizer , get_most_frequent_tokens my_datamodule = CustomDatamodule ( ... ) my_datamodule . prepare_data () my_datamodule . setup ( None ) train_corpus = \" \" . join ( my_datamodule . train_dataset . all_outputs ()) tokens = get_most_frequent_tokens ( train_corpus , char_tokenizer )","title":"How to get the tokens from my dataset?"},{"location":"api/CTC%20Loss/","text":"Functionality to calculate the ctc loss. calculate_ctc ( probabilities , y , prob_lengths , y_lengths , blank_idx ) Calculates the ctc loss based on model probabilities (also called emissions) and labels. PARAMETER DESCRIPTION probabilities Output of the model, before any softmax operation. Shape [batch, #vocab, time] TYPE: Tensor y Tensor containing the corresponding labels. Shape [batch] TYPE: Tensor prob_lengths Lengths of each element in the input. Shape [batch] TYPE: Tensor y_lengths Lenghts of each element in the output. Should NOT be normalized. TYPE: Tensor blank_idx Index of the blank token in the vocab. TYPE: int RETURNS DESCRIPTION Tensor Loss tensor that can be backpropagated. Source code in thunder/ctc_loss.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 def calculate_ctc ( probabilities : Tensor , y : Tensor , prob_lengths : Tensor , y_lengths : Tensor , blank_idx : int , ) -> Tensor : \"\"\"Calculates the ctc loss based on model probabilities (also called emissions) and labels. Args: probabilities: Output of the model, before any softmax operation. Shape [batch, #vocab, time] y: Tensor containing the corresponding labels. Shape [batch] prob_lengths: Lengths of each element in the input. Shape [batch] y_lengths: Lenghts of each element in the output. Should NOT be normalized. blank_idx: Index of the blank token in the vocab. Returns: Loss tensor that can be backpropagated. \"\"\" # Change from (batch, #vocab, time) to (time, batch, #vocab) probabilities = probabilities . permute ( 2 , 0 , 1 ) logprobs = log_softmax ( probabilities , dim = 2 ) return ctc_loss ( logprobs , y , prob_lengths . long (), y_lengths , blank = blank_idx , reduction = \"mean\" , zero_infinity = True , )","title":"CTC Loss"},{"location":"api/CTC%20Loss/#thunder.ctc_loss.calculate_ctc","text":"Calculates the ctc loss based on model probabilities (also called emissions) and labels. PARAMETER DESCRIPTION probabilities Output of the model, before any softmax operation. Shape [batch, #vocab, time] TYPE: Tensor y Tensor containing the corresponding labels. Shape [batch] TYPE: Tensor prob_lengths Lengths of each element in the input. Shape [batch] TYPE: Tensor y_lengths Lenghts of each element in the output. Should NOT be normalized. TYPE: Tensor blank_idx Index of the blank token in the vocab. TYPE: int RETURNS DESCRIPTION Tensor Loss tensor that can be backpropagated. Source code in thunder/ctc_loss.py 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 def calculate_ctc ( probabilities : Tensor , y : Tensor , prob_lengths : Tensor , y_lengths : Tensor , blank_idx : int , ) -> Tensor : \"\"\"Calculates the ctc loss based on model probabilities (also called emissions) and labels. Args: probabilities: Output of the model, before any softmax operation. Shape [batch, #vocab, time] y: Tensor containing the corresponding labels. Shape [batch] prob_lengths: Lengths of each element in the input. Shape [batch] y_lengths: Lenghts of each element in the output. Should NOT be normalized. blank_idx: Index of the blank token in the vocab. Returns: Loss tensor that can be backpropagated. \"\"\" # Change from (batch, #vocab, time) to (time, batch, #vocab) probabilities = probabilities . permute ( 2 , 0 , 1 ) logprobs = log_softmax ( probabilities , dim = 2 ) return ctc_loss ( logprobs , y , prob_lengths . long (), y_lengths , blank = blank_idx , reduction = \"mean\" , zero_infinity = True , )","title":"calculate_ctc()"},{"location":"api/blocks/","text":"Building blocks that can be shared across all models. Masked Bases: nn . Module Wrapper to mix normal modules with others that take 2 inputs Source code in thunder/blocks.py 105 106 107 108 109 110 111 112 113 114 115 class Masked ( nn . Module ): \"\"\"Wrapper to mix normal modules with others that take 2 inputs\"\"\" def __init__ ( self , * layers ): super () . __init__ () self . layer = nn . Sequential ( * layers ) def forward ( self , audio : torch . Tensor , audio_lengths : torch . Tensor ) -> Tuple [ torch . Tensor , torch . Tensor ]: return self . layer ( audio ), audio_lengths MultiSequential Bases: nn . Sequential nn.Sequential equivalent with 2 inputs/outputs Source code in thunder/blocks.py 94 95 96 97 98 99 100 101 102 class MultiSequential ( nn . Sequential ): \"\"\"nn.Sequential equivalent with 2 inputs/outputs\"\"\" def forward ( self , audio : torch . Tensor , audio_lengths : torch . Tensor ) -> Tuple [ torch . Tensor , torch . Tensor ]: for module in self . children (): audio , audio_lengths = module ( audio , audio_lengths ) return audio , audio_lengths SwapLastDimension Bases: nn . Module Layer that swap the last two dimensions of the data. Source code in thunder/blocks.py 217 218 219 220 221 class SwapLastDimension ( nn . Module ): \"\"\"Layer that swap the last two dimensions of the data.\"\"\" def forward ( self , x : Tensor ) -> Tensor : return x . transpose ( - 1 , - 2 ) conv1d_decoder ( decoder_input_channels , num_classes ) Decoder that uses one conv1d layer PARAMETER DESCRIPTION num_classes Number of output classes of the model. It's the size of the vocabulary, excluding the blank symbol. TYPE: int decoder_input_channels Number of input channels of the decoder. That is the number of channels of the features created by the encoder. TYPE: int RETURNS DESCRIPTION nn . Module Pytorch model of the decoder Source code in thunder/blocks.py 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 def conv1d_decoder ( decoder_input_channels : int , num_classes : int ) -> nn . Module : \"\"\"Decoder that uses one conv1d layer Args: num_classes: Number of output classes of the model. It's the size of the vocabulary, excluding the blank symbol. decoder_input_channels: Number of input channels of the decoder. That is the number of channels of the features created by the encoder. Returns: Pytorch model of the decoder \"\"\" decoder = nn . Conv1d ( decoder_input_channels , num_classes , kernel_size = 1 , bias = True , ) nn . init . xavier_uniform_ ( decoder . weight , gain = 1.0 ) return decoder convolution_stft ( input_data , n_fft = 1024 , hop_length = 512 , win_length = 1024 , window = torch . hann_window ( 1024 , periodic = False ), center = True , return_complex = False ) Implements the stft operation using the convolution method. This is one alternative to make possible to export code using this operation to onnx and arm based environments. The signature shuld follow the same as torch.stft, making it possible to just swap the two. The code is based on https://github.com/pseeth/torch-stft Source code in thunder/blocks.py 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 def convolution_stft ( input_data : torch . Tensor , n_fft : int = 1024 , hop_length : int = 512 , win_length : int = 1024 , window : torch . Tensor = torch . hann_window ( 1024 , periodic = False ), center : bool = True , return_complex : bool = False , ) -> torch . Tensor : \"\"\"Implements the stft operation using the convolution method. This is one alternative to make possible to export code using this operation to onnx and arm based environments. The signature shuld follow the same as torch.stft, making it possible to just swap the two. The code is based on https://github.com/pseeth/torch-stft \"\"\" assert n_fft >= win_length pad_amount = int ( n_fft / 2 ) window = window . to ( input_data . device ) fourier_basis = _fourier_matrix ( n_fft , device = input_data . device ) cutoff = int (( n_fft / 2 + 1 )) fourier_basis = torch . stack ( [ torch . real ( fourier_basis [: cutoff , :]), torch . imag ( fourier_basis [: cutoff , :])] ) . reshape ( - 1 , n_fft ) forward_basis = fourier_basis [:, None , :] . float () window_pad = ( n_fft - win_length ) // 2 window_pad2 = n_fft - ( window_pad + win_length ) fft_window = torch . nn . functional . pad ( window , [ window_pad , window_pad2 ]) # window the bases forward_basis *= fft_window forward_basis = forward_basis . float () num_batches = input_data . shape [ 0 ] num_samples = input_data . shape [ - 1 ] # similar to librosa, reflect-pad the input input_data = input_data . view ( num_batches , 1 , num_samples ) input_data = F . pad ( input_data . unsqueeze ( 1 ), ( pad_amount , pad_amount , 0 , 0 ), mode = \"reflect\" , ) input_data = input_data . squeeze ( 1 ) forward_transform = F . conv1d ( input_data , forward_basis , stride = hop_length , padding = 0 ) cutoff = int (( n_fft / 2 ) + 1 ) real_part = forward_transform [:, : cutoff , :] imag_part = forward_transform [:, cutoff :, :] return torch . stack (( real_part , imag_part ), dim =- 1 ) get_same_padding ( kernel_size , stride , dilation ) Calculates the padding size to obtain same padding. Same padding means that the output will have the shape input_shape / stride. That means, for stride = 1 the output shape is the same as the input, and stride = 2 gives an output that is half of the input shape. PARAMETER DESCRIPTION kernel_size convolution kernel size. Only tested to be correct with odd values. TYPE: int stride convolution stride TYPE: int dilation convolution dilation TYPE: int RAISES DESCRIPTION ValueError Only stride or dilation may be greater than 1 RETURNS DESCRIPTION int padding value to obtain same padding. Source code in thunder/blocks.py 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 def get_same_padding ( kernel_size : int , stride : int , dilation : int ) -> int : \"\"\"Calculates the padding size to obtain same padding. Same padding means that the output will have the shape input_shape / stride. That means, for stride = 1 the output shape is the same as the input, and stride = 2 gives an output that is half of the input shape. Args: kernel_size: convolution kernel size. Only tested to be correct with odd values. stride: convolution stride dilation: convolution dilation Raises: ValueError: Only stride or dilation may be greater than 1 Returns: padding value to obtain same padding. \"\"\" if stride > 1 and dilation > 1 : raise ValueError ( \"Only stride OR dilation may be greater than 1\" ) if dilation > 1 : return ( dilation * ( kernel_size - 1 ) + 1 ) // 2 return kernel_size // 2 lengths_to_mask ( lengths , max_length ) Convert from integer lengths of each element to mask representation PARAMETER DESCRIPTION lengths lengths of each element in the batch TYPE: torch . Tensor max_length maximum length expected. Can be greater than lengths.max() TYPE: int RETURNS DESCRIPTION torch . Tensor Corresponding boolean mask indicating the valid region of the tensor. Source code in thunder/blocks.py 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 def lengths_to_mask ( lengths : torch . Tensor , max_length : int ) -> torch . Tensor : \"\"\"Convert from integer lengths of each element to mask representation Args: lengths: lengths of each element in the batch max_length: maximum length expected. Can be greater than lengths.max() Returns: Corresponding boolean mask indicating the valid region of the tensor. \"\"\" lengths = lengths . type ( torch . long ) mask = torch . arange ( max_length , device = lengths . device ) . expand ( lengths . shape [ 0 ], max_length ) < lengths . unsqueeze ( 1 ) return mask linear_decoder ( decoder_input_channels , num_classes , decoder_dropout ) Decoder that uses a linear layer with dropout PARAMETER DESCRIPTION decoder_dropout Amount of dropout to be used in the decoder TYPE: float decoder_input_channels Number of input channels of the decoder. That is the number of channels of the features created by the encoder. TYPE: int num_classes Number of output classes of the model. It's the size of the vocabulary, excluding the blank symbol. TYPE: int RETURNS DESCRIPTION nn . Module Module that represents the decoder. Source code in thunder/blocks.py 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 def linear_decoder ( decoder_input_channels : int , num_classes : int , decoder_dropout : float ) -> nn . Module : \"\"\"Decoder that uses a linear layer with dropout Args: decoder_dropout: Amount of dropout to be used in the decoder decoder_input_channels: Number of input channels of the decoder. That is the number of channels of the features created by the encoder. num_classes: Number of output classes of the model. It's the size of the vocabulary, excluding the blank symbol. Returns: Module that represents the decoder. \"\"\" # SwapLastDimension is necessary to # change from (batch, time, #vocab) to (batch, #vocab, time) # that is expected by the rest of the library return nn . Sequential ( nn . Dropout ( decoder_dropout ), nn . Linear ( decoder_input_channels , num_classes ), SwapLastDimension (), ) normalize_tensor ( input_values , mask = None , div_guard = 1e-07 , dim =- 1 ) Normalize tensor values, optionally using some mask to define the valid region. PARAMETER DESCRIPTION input_values input tensor to be normalized TYPE: torch . Tensor mask Optional mask describing the valid elements. TYPE: Optional [ torch . Tensor ] DEFAULT: None div_guard value used to prevent division by zero when normalizing. TYPE: float DEFAULT: 1e-07 dim dimension used to calculate the mean and variance. TYPE: int DEFAULT: -1 RETURNS DESCRIPTION torch . Tensor Normalized tensor Source code in thunder/blocks.py 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 def normalize_tensor ( input_values : torch . Tensor , mask : Optional [ torch . Tensor ] = None , div_guard : float = 1e-7 , dim : int = - 1 , ) -> torch . Tensor : \"\"\"Normalize tensor values, optionally using some mask to define the valid region. Args: input_values: input tensor to be normalized mask: Optional mask describing the valid elements. div_guard: value used to prevent division by zero when normalizing. dim: dimension used to calculate the mean and variance. Returns: Normalized tensor \"\"\" # Vectorized implementation of (x - x.mean()) / x.std() considering only the valid mask if mask is not None : # Number of valid elements num_elements = mask . sum ( dim = dim , keepdim = True ) . detach () # Mean is sum over number of elements x_mean = input_values . sum ( dim = dim , keepdim = True ) . detach () / num_elements # std numerator: sum of squared differences to the mean numerator = ( input_values - x_mean ) . pow ( 2 ) . sum ( dim = dim , keepdim = True ) . detach () x_std = ( numerator / num_elements ) . sqrt () # using the div_guard to prevent division by zero normalized = ( input_values - x_mean ) / ( x_std + div_guard ) # Cleaning elements outside of valid mask return torch . masked_fill ( normalized , ~ mask . type ( torch . bool ), 0.0 ) mean = input_values . mean ( dim = dim , keepdim = True ) . detach () std = ( input_values . var ( dim = dim , keepdim = True ) . detach () + div_guard ) . sqrt () return ( input_values - mean ) / std","title":"Blocks"},{"location":"api/blocks/#thunder.blocks.Masked","text":"Bases: nn . Module Wrapper to mix normal modules with others that take 2 inputs Source code in thunder/blocks.py 105 106 107 108 109 110 111 112 113 114 115 class Masked ( nn . Module ): \"\"\"Wrapper to mix normal modules with others that take 2 inputs\"\"\" def __init__ ( self , * layers ): super () . __init__ () self . layer = nn . Sequential ( * layers ) def forward ( self , audio : torch . Tensor , audio_lengths : torch . Tensor ) -> Tuple [ torch . Tensor , torch . Tensor ]: return self . layer ( audio ), audio_lengths","title":"Masked"},{"location":"api/blocks/#thunder.blocks.MultiSequential","text":"Bases: nn . Sequential nn.Sequential equivalent with 2 inputs/outputs Source code in thunder/blocks.py 94 95 96 97 98 99 100 101 102 class MultiSequential ( nn . Sequential ): \"\"\"nn.Sequential equivalent with 2 inputs/outputs\"\"\" def forward ( self , audio : torch . Tensor , audio_lengths : torch . Tensor ) -> Tuple [ torch . Tensor , torch . Tensor ]: for module in self . children (): audio , audio_lengths = module ( audio , audio_lengths ) return audio , audio_lengths","title":"MultiSequential"},{"location":"api/blocks/#thunder.blocks.SwapLastDimension","text":"Bases: nn . Module Layer that swap the last two dimensions of the data. Source code in thunder/blocks.py 217 218 219 220 221 class SwapLastDimension ( nn . Module ): \"\"\"Layer that swap the last two dimensions of the data.\"\"\" def forward ( self , x : Tensor ) -> Tensor : return x . transpose ( - 1 , - 2 )","title":"SwapLastDimension"},{"location":"api/blocks/#thunder.blocks.conv1d_decoder","text":"Decoder that uses one conv1d layer PARAMETER DESCRIPTION num_classes Number of output classes of the model. It's the size of the vocabulary, excluding the blank symbol. TYPE: int decoder_input_channels Number of input channels of the decoder. That is the number of channels of the features created by the encoder. TYPE: int RETURNS DESCRIPTION nn . Module Pytorch model of the decoder Source code in thunder/blocks.py 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 def conv1d_decoder ( decoder_input_channels : int , num_classes : int ) -> nn . Module : \"\"\"Decoder that uses one conv1d layer Args: num_classes: Number of output classes of the model. It's the size of the vocabulary, excluding the blank symbol. decoder_input_channels: Number of input channels of the decoder. That is the number of channels of the features created by the encoder. Returns: Pytorch model of the decoder \"\"\" decoder = nn . Conv1d ( decoder_input_channels , num_classes , kernel_size = 1 , bias = True , ) nn . init . xavier_uniform_ ( decoder . weight , gain = 1.0 ) return decoder","title":"conv1d_decoder()"},{"location":"api/blocks/#thunder.blocks.convolution_stft","text":"Implements the stft operation using the convolution method. This is one alternative to make possible to export code using this operation to onnx and arm based environments. The signature shuld follow the same as torch.stft, making it possible to just swap the two. The code is based on https://github.com/pseeth/torch-stft Source code in thunder/blocks.py 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 def convolution_stft ( input_data : torch . Tensor , n_fft : int = 1024 , hop_length : int = 512 , win_length : int = 1024 , window : torch . Tensor = torch . hann_window ( 1024 , periodic = False ), center : bool = True , return_complex : bool = False , ) -> torch . Tensor : \"\"\"Implements the stft operation using the convolution method. This is one alternative to make possible to export code using this operation to onnx and arm based environments. The signature shuld follow the same as torch.stft, making it possible to just swap the two. The code is based on https://github.com/pseeth/torch-stft \"\"\" assert n_fft >= win_length pad_amount = int ( n_fft / 2 ) window = window . to ( input_data . device ) fourier_basis = _fourier_matrix ( n_fft , device = input_data . device ) cutoff = int (( n_fft / 2 + 1 )) fourier_basis = torch . stack ( [ torch . real ( fourier_basis [: cutoff , :]), torch . imag ( fourier_basis [: cutoff , :])] ) . reshape ( - 1 , n_fft ) forward_basis = fourier_basis [:, None , :] . float () window_pad = ( n_fft - win_length ) // 2 window_pad2 = n_fft - ( window_pad + win_length ) fft_window = torch . nn . functional . pad ( window , [ window_pad , window_pad2 ]) # window the bases forward_basis *= fft_window forward_basis = forward_basis . float () num_batches = input_data . shape [ 0 ] num_samples = input_data . shape [ - 1 ] # similar to librosa, reflect-pad the input input_data = input_data . view ( num_batches , 1 , num_samples ) input_data = F . pad ( input_data . unsqueeze ( 1 ), ( pad_amount , pad_amount , 0 , 0 ), mode = \"reflect\" , ) input_data = input_data . squeeze ( 1 ) forward_transform = F . conv1d ( input_data , forward_basis , stride = hop_length , padding = 0 ) cutoff = int (( n_fft / 2 ) + 1 ) real_part = forward_transform [:, : cutoff , :] imag_part = forward_transform [:, cutoff :, :] return torch . stack (( real_part , imag_part ), dim =- 1 )","title":"convolution_stft()"},{"location":"api/blocks/#thunder.blocks.get_same_padding","text":"Calculates the padding size to obtain same padding. Same padding means that the output will have the shape input_shape / stride. That means, for stride = 1 the output shape is the same as the input, and stride = 2 gives an output that is half of the input shape. PARAMETER DESCRIPTION kernel_size convolution kernel size. Only tested to be correct with odd values. TYPE: int stride convolution stride TYPE: int dilation convolution dilation TYPE: int RAISES DESCRIPTION ValueError Only stride or dilation may be greater than 1 RETURNS DESCRIPTION int padding value to obtain same padding. Source code in thunder/blocks.py 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 def get_same_padding ( kernel_size : int , stride : int , dilation : int ) -> int : \"\"\"Calculates the padding size to obtain same padding. Same padding means that the output will have the shape input_shape / stride. That means, for stride = 1 the output shape is the same as the input, and stride = 2 gives an output that is half of the input shape. Args: kernel_size: convolution kernel size. Only tested to be correct with odd values. stride: convolution stride dilation: convolution dilation Raises: ValueError: Only stride or dilation may be greater than 1 Returns: padding value to obtain same padding. \"\"\" if stride > 1 and dilation > 1 : raise ValueError ( \"Only stride OR dilation may be greater than 1\" ) if dilation > 1 : return ( dilation * ( kernel_size - 1 ) + 1 ) // 2 return kernel_size // 2","title":"get_same_padding()"},{"location":"api/blocks/#thunder.blocks.lengths_to_mask","text":"Convert from integer lengths of each element to mask representation PARAMETER DESCRIPTION lengths lengths of each element in the batch TYPE: torch . Tensor max_length maximum length expected. Can be greater than lengths.max() TYPE: int RETURNS DESCRIPTION torch . Tensor Corresponding boolean mask indicating the valid region of the tensor. Source code in thunder/blocks.py 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 def lengths_to_mask ( lengths : torch . Tensor , max_length : int ) -> torch . Tensor : \"\"\"Convert from integer lengths of each element to mask representation Args: lengths: lengths of each element in the batch max_length: maximum length expected. Can be greater than lengths.max() Returns: Corresponding boolean mask indicating the valid region of the tensor. \"\"\" lengths = lengths . type ( torch . long ) mask = torch . arange ( max_length , device = lengths . device ) . expand ( lengths . shape [ 0 ], max_length ) < lengths . unsqueeze ( 1 ) return mask","title":"lengths_to_mask()"},{"location":"api/blocks/#thunder.blocks.linear_decoder","text":"Decoder that uses a linear layer with dropout PARAMETER DESCRIPTION decoder_dropout Amount of dropout to be used in the decoder TYPE: float decoder_input_channels Number of input channels of the decoder. That is the number of channels of the features created by the encoder. TYPE: int num_classes Number of output classes of the model. It's the size of the vocabulary, excluding the blank symbol. TYPE: int RETURNS DESCRIPTION nn . Module Module that represents the decoder. Source code in thunder/blocks.py 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 def linear_decoder ( decoder_input_channels : int , num_classes : int , decoder_dropout : float ) -> nn . Module : \"\"\"Decoder that uses a linear layer with dropout Args: decoder_dropout: Amount of dropout to be used in the decoder decoder_input_channels: Number of input channels of the decoder. That is the number of channels of the features created by the encoder. num_classes: Number of output classes of the model. It's the size of the vocabulary, excluding the blank symbol. Returns: Module that represents the decoder. \"\"\" # SwapLastDimension is necessary to # change from (batch, time, #vocab) to (batch, #vocab, time) # that is expected by the rest of the library return nn . Sequential ( nn . Dropout ( decoder_dropout ), nn . Linear ( decoder_input_channels , num_classes ), SwapLastDimension (), )","title":"linear_decoder()"},{"location":"api/blocks/#thunder.blocks.normalize_tensor","text":"Normalize tensor values, optionally using some mask to define the valid region. PARAMETER DESCRIPTION input_values input tensor to be normalized TYPE: torch . Tensor mask Optional mask describing the valid elements. TYPE: Optional [ torch . Tensor ] DEFAULT: None div_guard value used to prevent division by zero when normalizing. TYPE: float DEFAULT: 1e-07 dim dimension used to calculate the mean and variance. TYPE: int DEFAULT: -1 RETURNS DESCRIPTION torch . Tensor Normalized tensor Source code in thunder/blocks.py 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 def normalize_tensor ( input_values : torch . Tensor , mask : Optional [ torch . Tensor ] = None , div_guard : float = 1e-7 , dim : int = - 1 , ) -> torch . Tensor : \"\"\"Normalize tensor values, optionally using some mask to define the valid region. Args: input_values: input tensor to be normalized mask: Optional mask describing the valid elements. div_guard: value used to prevent division by zero when normalizing. dim: dimension used to calculate the mean and variance. Returns: Normalized tensor \"\"\" # Vectorized implementation of (x - x.mean()) / x.std() considering only the valid mask if mask is not None : # Number of valid elements num_elements = mask . sum ( dim = dim , keepdim = True ) . detach () # Mean is sum over number of elements x_mean = input_values . sum ( dim = dim , keepdim = True ) . detach () / num_elements # std numerator: sum of squared differences to the mean numerator = ( input_values - x_mean ) . pow ( 2 ) . sum ( dim = dim , keepdim = True ) . detach () x_std = ( numerator / num_elements ) . sqrt () # using the div_guard to prevent division by zero normalized = ( input_values - x_mean ) / ( x_std + div_guard ) # Cleaning elements outside of valid mask return torch . masked_fill ( normalized , ~ mask . type ( torch . bool ), 0.0 ) mean = input_values . mean ( dim = dim , keepdim = True ) . detach () std = ( input_values . var ( dim = dim , keepdim = True ) . detach () + div_guard ) . sqrt () return ( input_values - mean ) / std","title":"normalize_tensor()"},{"location":"api/callbacks/","text":"Helper callback functionality, not essential to research FinetuneEncoderDecoder Bases: BaseFinetuning Source code in thunder/callbacks.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 class FinetuneEncoderDecoder ( BaseFinetuning ): def __init__ ( self , unfreeze_encoder_at_epoch : int = 1 , encoder_initial_lr_div : float = 10 , train_batchnorm : bool = True , ): \"\"\" Finetune a encoder model based on a learning rate. Args: unfreeze_encoder_at_epoch: Epoch at which the encoder will be unfreezed. encoder_initial_lr_div: Used to scale down the encoder learning rate compared to rest of model. train_batchnorm: Make Batch Normalization trainable at the beginning of train. \"\"\" super () . __init__ () self . unfreeze_encoder_at_epoch = unfreeze_encoder_at_epoch self . encoder_initial_lr_div = encoder_initial_lr_div self . train_batchnorm = train_batchnorm def on_fit_start ( self , trainer : pl . Trainer , pl_module : pl . LightningModule ): \"\"\"Check if the LightningModule has the necessary attribute before the train starts Args: trainer: Lightning Trainer pl_module: Lightning Module used during train Raises: Exception: If LightningModule has no nn.Module `encoder` attribute. \"\"\" if hasattr ( pl_module , \"encoder\" ) and isinstance ( pl_module . encoder , nn . Module ): return raise Exception ( \"The LightningModule should have a nn.Module `encoder` attribute\" ) def freeze_before_training ( self , pl_module : pl . LightningModule ): \"\"\"Freeze the encoder initially before the train starts. Args: pl_module: Lightning Module \"\"\" self . freeze ( pl_module . encoder , train_bn = self . train_batchnorm ) def finetune_function ( self , pl_module : pl . LightningModule , epoch : int , optimizer : Optimizer , opt_idx : int , ): \"\"\"Unfreezes the encoder at the specified epoch Args: pl_module: Lightning Module epoch: epoch number optimizer: optimizer used during training opt_idx: optimizer index \"\"\" if epoch == self . unfreeze_encoder_at_epoch : self . unfreeze_and_add_param_group ( pl_module . encoder , optimizer , initial_denom_lr = self . encoder_initial_lr_div , train_bn = not self . train_batchnorm , ) __init__ ( unfreeze_encoder_at_epoch = 1 , encoder_initial_lr_div = 10 , train_batchnorm = True ) Finetune a encoder model based on a learning rate. PARAMETER DESCRIPTION unfreeze_encoder_at_epoch Epoch at which the encoder will be unfreezed. TYPE: int DEFAULT: 1 encoder_initial_lr_div Used to scale down the encoder learning rate compared to rest of model. TYPE: float DEFAULT: 10 train_batchnorm Make Batch Normalization trainable at the beginning of train. TYPE: bool DEFAULT: True Source code in thunder/callbacks.py 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 def __init__ ( self , unfreeze_encoder_at_epoch : int = 1 , encoder_initial_lr_div : float = 10 , train_batchnorm : bool = True , ): \"\"\" Finetune a encoder model based on a learning rate. Args: unfreeze_encoder_at_epoch: Epoch at which the encoder will be unfreezed. encoder_initial_lr_div: Used to scale down the encoder learning rate compared to rest of model. train_batchnorm: Make Batch Normalization trainable at the beginning of train. \"\"\" super () . __init__ () self . unfreeze_encoder_at_epoch = unfreeze_encoder_at_epoch self . encoder_initial_lr_div = encoder_initial_lr_div self . train_batchnorm = train_batchnorm finetune_function ( pl_module , epoch , optimizer , opt_idx ) Unfreezes the encoder at the specified epoch PARAMETER DESCRIPTION pl_module Lightning Module TYPE: pl . LightningModule epoch epoch number TYPE: int optimizer optimizer used during training TYPE: Optimizer opt_idx optimizer index TYPE: int Source code in thunder/callbacks.py 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 def finetune_function ( self , pl_module : pl . LightningModule , epoch : int , optimizer : Optimizer , opt_idx : int , ): \"\"\"Unfreezes the encoder at the specified epoch Args: pl_module: Lightning Module epoch: epoch number optimizer: optimizer used during training opt_idx: optimizer index \"\"\" if epoch == self . unfreeze_encoder_at_epoch : self . unfreeze_and_add_param_group ( pl_module . encoder , optimizer , initial_denom_lr = self . encoder_initial_lr_div , train_bn = not self . train_batchnorm , ) freeze_before_training ( pl_module ) Freeze the encoder initially before the train starts. PARAMETER DESCRIPTION pl_module Lightning Module TYPE: pl . LightningModule Source code in thunder/callbacks.py 56 57 58 59 60 61 62 def freeze_before_training ( self , pl_module : pl . LightningModule ): \"\"\"Freeze the encoder initially before the train starts. Args: pl_module: Lightning Module \"\"\" self . freeze ( pl_module . encoder , train_bn = self . train_batchnorm ) on_fit_start ( trainer , pl_module ) Check if the LightningModule has the necessary attribute before the train starts PARAMETER DESCRIPTION trainer Lightning Trainer TYPE: pl . Trainer pl_module Lightning Module used during train TYPE: pl . LightningModule RAISES DESCRIPTION Exception If LightningModule has no nn.Module encoder attribute. Source code in thunder/callbacks.py 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 def on_fit_start ( self , trainer : pl . Trainer , pl_module : pl . LightningModule ): \"\"\"Check if the LightningModule has the necessary attribute before the train starts Args: trainer: Lightning Trainer pl_module: Lightning Module used during train Raises: Exception: If LightningModule has no nn.Module `encoder` attribute. \"\"\" if hasattr ( pl_module , \"encoder\" ) and isinstance ( pl_module . encoder , nn . Module ): return raise Exception ( \"The LightningModule should have a nn.Module `encoder` attribute\" )","title":"Callbacks"},{"location":"api/callbacks/#thunder.callbacks.FinetuneEncoderDecoder","text":"Bases: BaseFinetuning Source code in thunder/callbacks.py 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 class FinetuneEncoderDecoder ( BaseFinetuning ): def __init__ ( self , unfreeze_encoder_at_epoch : int = 1 , encoder_initial_lr_div : float = 10 , train_batchnorm : bool = True , ): \"\"\" Finetune a encoder model based on a learning rate. Args: unfreeze_encoder_at_epoch: Epoch at which the encoder will be unfreezed. encoder_initial_lr_div: Used to scale down the encoder learning rate compared to rest of model. train_batchnorm: Make Batch Normalization trainable at the beginning of train. \"\"\" super () . __init__ () self . unfreeze_encoder_at_epoch = unfreeze_encoder_at_epoch self . encoder_initial_lr_div = encoder_initial_lr_div self . train_batchnorm = train_batchnorm def on_fit_start ( self , trainer : pl . Trainer , pl_module : pl . LightningModule ): \"\"\"Check if the LightningModule has the necessary attribute before the train starts Args: trainer: Lightning Trainer pl_module: Lightning Module used during train Raises: Exception: If LightningModule has no nn.Module `encoder` attribute. \"\"\" if hasattr ( pl_module , \"encoder\" ) and isinstance ( pl_module . encoder , nn . Module ): return raise Exception ( \"The LightningModule should have a nn.Module `encoder` attribute\" ) def freeze_before_training ( self , pl_module : pl . LightningModule ): \"\"\"Freeze the encoder initially before the train starts. Args: pl_module: Lightning Module \"\"\" self . freeze ( pl_module . encoder , train_bn = self . train_batchnorm ) def finetune_function ( self , pl_module : pl . LightningModule , epoch : int , optimizer : Optimizer , opt_idx : int , ): \"\"\"Unfreezes the encoder at the specified epoch Args: pl_module: Lightning Module epoch: epoch number optimizer: optimizer used during training opt_idx: optimizer index \"\"\" if epoch == self . unfreeze_encoder_at_epoch : self . unfreeze_and_add_param_group ( pl_module . encoder , optimizer , initial_denom_lr = self . encoder_initial_lr_div , train_bn = not self . train_batchnorm , )","title":"FinetuneEncoderDecoder"},{"location":"api/callbacks/#thunder.callbacks.FinetuneEncoderDecoder.__init__","text":"Finetune a encoder model based on a learning rate. PARAMETER DESCRIPTION unfreeze_encoder_at_epoch Epoch at which the encoder will be unfreezed. TYPE: int DEFAULT: 1 encoder_initial_lr_div Used to scale down the encoder learning rate compared to rest of model. TYPE: float DEFAULT: 10 train_batchnorm Make Batch Normalization trainable at the beginning of train. TYPE: bool DEFAULT: True Source code in thunder/callbacks.py 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 def __init__ ( self , unfreeze_encoder_at_epoch : int = 1 , encoder_initial_lr_div : float = 10 , train_batchnorm : bool = True , ): \"\"\" Finetune a encoder model based on a learning rate. Args: unfreeze_encoder_at_epoch: Epoch at which the encoder will be unfreezed. encoder_initial_lr_div: Used to scale down the encoder learning rate compared to rest of model. train_batchnorm: Make Batch Normalization trainable at the beginning of train. \"\"\" super () . __init__ () self . unfreeze_encoder_at_epoch = unfreeze_encoder_at_epoch self . encoder_initial_lr_div = encoder_initial_lr_div self . train_batchnorm = train_batchnorm","title":"__init__()"},{"location":"api/callbacks/#thunder.callbacks.FinetuneEncoderDecoder.finetune_function","text":"Unfreezes the encoder at the specified epoch PARAMETER DESCRIPTION pl_module Lightning Module TYPE: pl . LightningModule epoch epoch number TYPE: int optimizer optimizer used during training TYPE: Optimizer opt_idx optimizer index TYPE: int Source code in thunder/callbacks.py 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 def finetune_function ( self , pl_module : pl . LightningModule , epoch : int , optimizer : Optimizer , opt_idx : int , ): \"\"\"Unfreezes the encoder at the specified epoch Args: pl_module: Lightning Module epoch: epoch number optimizer: optimizer used during training opt_idx: optimizer index \"\"\" if epoch == self . unfreeze_encoder_at_epoch : self . unfreeze_and_add_param_group ( pl_module . encoder , optimizer , initial_denom_lr = self . encoder_initial_lr_div , train_bn = not self . train_batchnorm , )","title":"finetune_function()"},{"location":"api/callbacks/#thunder.callbacks.FinetuneEncoderDecoder.freeze_before_training","text":"Freeze the encoder initially before the train starts. PARAMETER DESCRIPTION pl_module Lightning Module TYPE: pl . LightningModule Source code in thunder/callbacks.py 56 57 58 59 60 61 62 def freeze_before_training ( self , pl_module : pl . LightningModule ): \"\"\"Freeze the encoder initially before the train starts. Args: pl_module: Lightning Module \"\"\" self . freeze ( pl_module . encoder , train_bn = self . train_batchnorm )","title":"freeze_before_training()"},{"location":"api/callbacks/#thunder.callbacks.FinetuneEncoderDecoder.on_fit_start","text":"Check if the LightningModule has the necessary attribute before the train starts PARAMETER DESCRIPTION trainer Lightning Trainer TYPE: pl . Trainer pl_module Lightning Module used during train TYPE: pl . LightningModule RAISES DESCRIPTION Exception If LightningModule has no nn.Module encoder attribute. Source code in thunder/callbacks.py 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 def on_fit_start ( self , trainer : pl . Trainer , pl_module : pl . LightningModule ): \"\"\"Check if the LightningModule has the necessary attribute before the train starts Args: trainer: Lightning Trainer pl_module: Lightning Module used during train Raises: Exception: If LightningModule has no nn.Module `encoder` attribute. \"\"\" if hasattr ( pl_module , \"encoder\" ) and isinstance ( pl_module . encoder , nn . Module ): return raise Exception ( \"The LightningModule should have a nn.Module `encoder` attribute\" )","title":"on_fit_start()"},{"location":"api/finetune/","text":"Module that implements easy finetuning of any model in the library. FinetuneCTCModule Bases: BaseCTCModule Source code in thunder/finetune.py 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 class FinetuneCTCModule ( BaseCTCModule ): def __init__ ( self , checkpoint_name : str , checkpoint_kwargs : Dict [ str , Any ] = None , decoder_class : Type [ nn . Module ] = None , decoder_kwargs : Dict [ str , Any ] = None , tokens : List [ str ] = None , text_kwargs : Dict [ str , Any ] = None , optimizer_class : Type [ torch . optim . Optimizer ] = torch . optim . AdamW , optimizer_kwargs : Dict [ str , Any ] = None , lr_scheduler_class : SchedulerBuilderType = None , lr_scheduler_kwargs : Dict [ str , Any ] = None , ): \"\"\"Generic finetune module, load any combination of encoder/decoder and custom tokens Args: checkpoint_name: Name of the base checkpoint to load checkpoint_kwargs: Additional kwargs to the checkpoint loading function. decoder_class: Optional class to override the loaded checkpoint. decoder_kwargs: Additional kwargs to the decoder_class. tokens: If passed a list of tokens, the decoder from the base checkpoint will be replaced by the one in decoder_class, and a new text transform will be build using those tokens. text_kwargs: Additional kwargs to the text_tranform class, when tokens is not None. optimizer_class: Optimizer to use during training. optimizer_kwargs: Optional extra kwargs to the optimizer. lr_scheduler_class: Optional class to use a learning rate scheduler with the optimizer. lr_scheduler_kwargs: Optional extra kwargs to the learning rate scheduler. \"\"\" self . save_hyperparameters () checkpoint_kwargs = checkpoint_kwargs or {} decoder_kwargs = decoder_kwargs or {} text_kwargs = text_kwargs or {} checkpoint_data = load_pretrained ( checkpoint_name , ** checkpoint_kwargs ) # Changing the decoder layer and text processing if decoder_class is None : text_transform = checkpoint_data . text_transform decoder = checkpoint_data . decoder else : assert tokens is not None , \"Expecting vocabulary to not be empty\" text_transform = BatchTextTransformer ( tokens , ** text_kwargs ) decoder = decoder_class ( checkpoint_data . encoder_final_dimension , text_transform . num_tokens , ** decoder_kwargs , ) super () . __init__ ( encoder = checkpoint_data . encoder , decoder = decoder , audio_transform = checkpoint_data . audio_transform , text_transform = text_transform , optimizer_class = optimizer_class , optimizer_kwargs = optimizer_kwargs , lr_scheduler_class = lr_scheduler_class , lr_scheduler_kwargs = lr_scheduler_kwargs , ) __init__ ( checkpoint_name , checkpoint_kwargs = None , decoder_class = None , decoder_kwargs = None , tokens = None , text_kwargs = None , optimizer_class = torch . optim . AdamW , optimizer_kwargs = None , lr_scheduler_class = None , lr_scheduler_kwargs = None ) Generic finetune module, load any combination of encoder/decoder and custom tokens PARAMETER DESCRIPTION checkpoint_name Name of the base checkpoint to load TYPE: str checkpoint_kwargs Additional kwargs to the checkpoint loading function. TYPE: Dict [ str , Any ] DEFAULT: None decoder_class Optional class to override the loaded checkpoint. TYPE: Type [ nn . Module ] DEFAULT: None decoder_kwargs Additional kwargs to the decoder_class. TYPE: Dict [ str , Any ] DEFAULT: None tokens If passed a list of tokens, the decoder from the base checkpoint will be replaced by the one in decoder_class, and a new text transform will be build using those tokens. TYPE: List [ str ] DEFAULT: None text_kwargs Additional kwargs to the text_tranform class, when tokens is not None. TYPE: Dict [ str , Any ] DEFAULT: None optimizer_class Optimizer to use during training. TYPE: Type [ torch . optim . Optimizer ] DEFAULT: torch.optim.AdamW optimizer_kwargs Optional extra kwargs to the optimizer. TYPE: Dict [ str , Any ] DEFAULT: None lr_scheduler_class Optional class to use a learning rate scheduler with the optimizer. TYPE: SchedulerBuilderType DEFAULT: None lr_scheduler_kwargs Optional extra kwargs to the learning rate scheduler. TYPE: Dict [ str , Any ] DEFAULT: None Source code in thunder/finetune.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 def __init__ ( self , checkpoint_name : str , checkpoint_kwargs : Dict [ str , Any ] = None , decoder_class : Type [ nn . Module ] = None , decoder_kwargs : Dict [ str , Any ] = None , tokens : List [ str ] = None , text_kwargs : Dict [ str , Any ] = None , optimizer_class : Type [ torch . optim . Optimizer ] = torch . optim . AdamW , optimizer_kwargs : Dict [ str , Any ] = None , lr_scheduler_class : SchedulerBuilderType = None , lr_scheduler_kwargs : Dict [ str , Any ] = None , ): \"\"\"Generic finetune module, load any combination of encoder/decoder and custom tokens Args: checkpoint_name: Name of the base checkpoint to load checkpoint_kwargs: Additional kwargs to the checkpoint loading function. decoder_class: Optional class to override the loaded checkpoint. decoder_kwargs: Additional kwargs to the decoder_class. tokens: If passed a list of tokens, the decoder from the base checkpoint will be replaced by the one in decoder_class, and a new text transform will be build using those tokens. text_kwargs: Additional kwargs to the text_tranform class, when tokens is not None. optimizer_class: Optimizer to use during training. optimizer_kwargs: Optional extra kwargs to the optimizer. lr_scheduler_class: Optional class to use a learning rate scheduler with the optimizer. lr_scheduler_kwargs: Optional extra kwargs to the learning rate scheduler. \"\"\" self . save_hyperparameters () checkpoint_kwargs = checkpoint_kwargs or {} decoder_kwargs = decoder_kwargs or {} text_kwargs = text_kwargs or {} checkpoint_data = load_pretrained ( checkpoint_name , ** checkpoint_kwargs ) # Changing the decoder layer and text processing if decoder_class is None : text_transform = checkpoint_data . text_transform decoder = checkpoint_data . decoder else : assert tokens is not None , \"Expecting vocabulary to not be empty\" text_transform = BatchTextTransformer ( tokens , ** text_kwargs ) decoder = decoder_class ( checkpoint_data . encoder_final_dimension , text_transform . num_tokens , ** decoder_kwargs , ) super () . __init__ ( encoder = checkpoint_data . encoder , decoder = decoder , audio_transform = checkpoint_data . audio_transform , text_transform = text_transform , optimizer_class = optimizer_class , optimizer_kwargs = optimizer_kwargs , lr_scheduler_class = lr_scheduler_class , lr_scheduler_kwargs = lr_scheduler_kwargs , )","title":"Finetune"},{"location":"api/finetune/#thunder.finetune.FinetuneCTCModule","text":"Bases: BaseCTCModule Source code in thunder/finetune.py 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 class FinetuneCTCModule ( BaseCTCModule ): def __init__ ( self , checkpoint_name : str , checkpoint_kwargs : Dict [ str , Any ] = None , decoder_class : Type [ nn . Module ] = None , decoder_kwargs : Dict [ str , Any ] = None , tokens : List [ str ] = None , text_kwargs : Dict [ str , Any ] = None , optimizer_class : Type [ torch . optim . Optimizer ] = torch . optim . AdamW , optimizer_kwargs : Dict [ str , Any ] = None , lr_scheduler_class : SchedulerBuilderType = None , lr_scheduler_kwargs : Dict [ str , Any ] = None , ): \"\"\"Generic finetune module, load any combination of encoder/decoder and custom tokens Args: checkpoint_name: Name of the base checkpoint to load checkpoint_kwargs: Additional kwargs to the checkpoint loading function. decoder_class: Optional class to override the loaded checkpoint. decoder_kwargs: Additional kwargs to the decoder_class. tokens: If passed a list of tokens, the decoder from the base checkpoint will be replaced by the one in decoder_class, and a new text transform will be build using those tokens. text_kwargs: Additional kwargs to the text_tranform class, when tokens is not None. optimizer_class: Optimizer to use during training. optimizer_kwargs: Optional extra kwargs to the optimizer. lr_scheduler_class: Optional class to use a learning rate scheduler with the optimizer. lr_scheduler_kwargs: Optional extra kwargs to the learning rate scheduler. \"\"\" self . save_hyperparameters () checkpoint_kwargs = checkpoint_kwargs or {} decoder_kwargs = decoder_kwargs or {} text_kwargs = text_kwargs or {} checkpoint_data = load_pretrained ( checkpoint_name , ** checkpoint_kwargs ) # Changing the decoder layer and text processing if decoder_class is None : text_transform = checkpoint_data . text_transform decoder = checkpoint_data . decoder else : assert tokens is not None , \"Expecting vocabulary to not be empty\" text_transform = BatchTextTransformer ( tokens , ** text_kwargs ) decoder = decoder_class ( checkpoint_data . encoder_final_dimension , text_transform . num_tokens , ** decoder_kwargs , ) super () . __init__ ( encoder = checkpoint_data . encoder , decoder = decoder , audio_transform = checkpoint_data . audio_transform , text_transform = text_transform , optimizer_class = optimizer_class , optimizer_kwargs = optimizer_kwargs , lr_scheduler_class = lr_scheduler_class , lr_scheduler_kwargs = lr_scheduler_kwargs , )","title":"FinetuneCTCModule"},{"location":"api/finetune/#thunder.finetune.FinetuneCTCModule.__init__","text":"Generic finetune module, load any combination of encoder/decoder and custom tokens PARAMETER DESCRIPTION checkpoint_name Name of the base checkpoint to load TYPE: str checkpoint_kwargs Additional kwargs to the checkpoint loading function. TYPE: Dict [ str , Any ] DEFAULT: None decoder_class Optional class to override the loaded checkpoint. TYPE: Type [ nn . Module ] DEFAULT: None decoder_kwargs Additional kwargs to the decoder_class. TYPE: Dict [ str , Any ] DEFAULT: None tokens If passed a list of tokens, the decoder from the base checkpoint will be replaced by the one in decoder_class, and a new text transform will be build using those tokens. TYPE: List [ str ] DEFAULT: None text_kwargs Additional kwargs to the text_tranform class, when tokens is not None. TYPE: Dict [ str , Any ] DEFAULT: None optimizer_class Optimizer to use during training. TYPE: Type [ torch . optim . Optimizer ] DEFAULT: torch.optim.AdamW optimizer_kwargs Optional extra kwargs to the optimizer. TYPE: Dict [ str , Any ] DEFAULT: None lr_scheduler_class Optional class to use a learning rate scheduler with the optimizer. TYPE: SchedulerBuilderType DEFAULT: None lr_scheduler_kwargs Optional extra kwargs to the learning rate scheduler. TYPE: Dict [ str , Any ] DEFAULT: None Source code in thunder/finetune.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 def __init__ ( self , checkpoint_name : str , checkpoint_kwargs : Dict [ str , Any ] = None , decoder_class : Type [ nn . Module ] = None , decoder_kwargs : Dict [ str , Any ] = None , tokens : List [ str ] = None , text_kwargs : Dict [ str , Any ] = None , optimizer_class : Type [ torch . optim . Optimizer ] = torch . optim . AdamW , optimizer_kwargs : Dict [ str , Any ] = None , lr_scheduler_class : SchedulerBuilderType = None , lr_scheduler_kwargs : Dict [ str , Any ] = None , ): \"\"\"Generic finetune module, load any combination of encoder/decoder and custom tokens Args: checkpoint_name: Name of the base checkpoint to load checkpoint_kwargs: Additional kwargs to the checkpoint loading function. decoder_class: Optional class to override the loaded checkpoint. decoder_kwargs: Additional kwargs to the decoder_class. tokens: If passed a list of tokens, the decoder from the base checkpoint will be replaced by the one in decoder_class, and a new text transform will be build using those tokens. text_kwargs: Additional kwargs to the text_tranform class, when tokens is not None. optimizer_class: Optimizer to use during training. optimizer_kwargs: Optional extra kwargs to the optimizer. lr_scheduler_class: Optional class to use a learning rate scheduler with the optimizer. lr_scheduler_kwargs: Optional extra kwargs to the learning rate scheduler. \"\"\" self . save_hyperparameters () checkpoint_kwargs = checkpoint_kwargs or {} decoder_kwargs = decoder_kwargs or {} text_kwargs = text_kwargs or {} checkpoint_data = load_pretrained ( checkpoint_name , ** checkpoint_kwargs ) # Changing the decoder layer and text processing if decoder_class is None : text_transform = checkpoint_data . text_transform decoder = checkpoint_data . decoder else : assert tokens is not None , \"Expecting vocabulary to not be empty\" text_transform = BatchTextTransformer ( tokens , ** text_kwargs ) decoder = decoder_class ( checkpoint_data . encoder_final_dimension , text_transform . num_tokens , ** decoder_kwargs , ) super () . __init__ ( encoder = checkpoint_data . encoder , decoder = decoder , audio_transform = checkpoint_data . audio_transform , text_transform = text_transform , optimizer_class = optimizer_class , optimizer_kwargs = optimizer_kwargs , lr_scheduler_class = lr_scheduler_class , lr_scheduler_kwargs = lr_scheduler_kwargs , )","title":"__init__()"},{"location":"api/module/","text":"Base module to train ctc models BaseCTCModule Bases: pl . LightningModule Source code in thunder/module.py 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 class BaseCTCModule ( pl . LightningModule ): def __init__ ( self , encoder : nn . Module , decoder : nn . Module , audio_transform : nn . Module , text_transform : BatchTextTransformer , optimizer_class : Type [ torch . optim . Optimizer ] = torch . optim . AdamW , optimizer_kwargs : Dict = None , lr_scheduler_class : SchedulerBuilderType = None , lr_scheduler_kwargs : Dict = None , encoder_final_dimension : int = None , ): \"\"\"Base module for all systems that follow the same CTC training procedure. Args: encoder: Encoder part of the model decoder: Decoder part of the model audio_transform: Transforms raw audio into the features the encoder expects text_transform: Class that encodes and decodes all textual representation optimizer_class: Optimizer to use during training. optimizer_kwargs: Optional extra kwargs to the optimizer. lr_scheduler_class: Optional class to use a learning rate scheduler with the optimizer. lr_scheduler_kwargs: Optional extra kwargs to the learning rate scheduler. encoder_final_dimension: number of features in the encoder output. \"\"\" super () . __init__ () self . encoder = encoder self . decoder = decoder self . audio_transform = audio_transform self . text_transform = text_transform self . optimizer_class = optimizer_class self . optimizer_kwargs = optimizer_kwargs or {} self . lr_scheduler_class = lr_scheduler_class self . lr_scheduler_kwargs = lr_scheduler_kwargs or {} self . lr_scheduler_interval = self . lr_scheduler_kwargs . pop ( \"interval\" , \"step\" ) self . encoder_final_dimension = encoder_final_dimension # Metrics self . validation_cer = CharErrorRate () self . validation_wer = WordErrorRate () self . example_input_array = ( torch . randn (( 10 , 16000 )), torch . randint ( 100 , 16000 , ( 10 ,)), ) def forward ( self , x : Tensor , lengths : Tensor ) -> Tuple [ Tensor , Optional [ Tensor ]]: \"\"\"Process the audio tensor to create the predictions. Args: x: Audio tensor of shape [batch_size, time] Returns: Tensor with the predictions. \"\"\" features , feature_lengths = self . audio_transform ( x , lengths ) encoded , out_lengths = self . encoder ( features , feature_lengths ) return self . decoder ( encoded ), out_lengths @torch . jit . export def predict ( self , x : Tensor ) -> List [ str ]: \"\"\"Use this function during inference to predict. Args: x: Audio tensor of shape [batch_size, time] Returns: A list of strings, each one contains the corresponding transcription to the original batch element. \"\"\" audio_lengths = torch . tensor ( x . shape [ 0 ] * [ x . shape [ - 1 ]], device = x . device ) pred , _ = self ( x , audio_lengths ) return self . text_transform . decode_prediction ( pred . argmax ( 1 )) def training_step ( self , batch : Tuple [ torch . Tensor , torch . Tensor , List [ str ]], batch_idx : int ) -> torch . Tensor : \"\"\"Training step. Check the original lightning docs for more information. Args: batch: Tuple containing the batched audios, normalized lengths and the corresponding text labels. batch_idx: Batch index Returns: Training loss for that batch \"\"\" audio , audio_lengths , texts = batch y , y_lengths = self . text_transform . encode ( texts , device = self . device ) probabilities , prob_lengths = self ( audio , audio_lengths ) loss = calculate_ctc ( probabilities , y , prob_lengths , y_lengths , self . text_transform . vocab . blank_idx , ) self . log ( \"loss/train_loss\" , loss ) return loss def validation_step ( self , batch : Tuple [ torch . Tensor , torch . Tensor , List [ str ]], batch_idx : int ) -> torch . Tensor : \"\"\"Validation step. Check the original lightning docs for more information. Args: batch: Tuple containing the batched audios, normalized lengths and the corresponding text labels. batch_idx: Batch index Returns: Validation loss for that batch \"\"\" audio , audio_lengths , texts = batch y , y_lengths = self . text_transform . encode ( texts , device = self . device ) probabilities , prob_lengths = self ( audio , audio_lengths ) loss = calculate_ctc ( probabilities , y , prob_lengths , y_lengths , self . text_transform . vocab . blank_idx , ) decoded_preds = self . text_transform . decode_prediction ( probabilities . argmax ( 1 )) decoded_targets = self . text_transform . decode_prediction ( y , remove_repeated = False ) self . validation_cer ( decoded_preds , decoded_targets ) self . validation_wer ( decoded_preds , decoded_targets ) self . log ( \"loss/val_loss\" , loss ) self . log ( \"metrics/cer\" , self . validation_cer , on_epoch = True ) self . log ( \"metrics/wer\" , self . validation_wer , on_epoch = True ) return loss def _update_special_optimizer_arg ( self , original_kwargs : Dict ) -> Dict : updated_kwargs = original_kwargs . copy () total_steps_arg = updated_kwargs . pop ( \"total_steps_arg\" , None ) if total_steps_arg : updated_kwargs [ total_steps_arg ] = self . trainer . estimated_stepping_batches return updated_kwargs def configure_optimizers ( self ) -> Union [ torch . optim . Optimizer , Dict [ str , Any ]]: optim_kwargs = self . _update_special_optimizer_arg ( self . optimizer_kwargs ) optimizer = self . optimizer_class ( filter ( lambda p : p . requires_grad , self . parameters ()), ** optim_kwargs ) if not self . lr_scheduler_class : return optimizer scheduler_kwargs = self . _update_special_optimizer_arg ( self . lr_scheduler_kwargs ) lr_scheduler = self . lr_scheduler_class ( optimizer , ** scheduler_kwargs ) return { \"optimizer\" : optimizer , \"lr_scheduler\" : { \"scheduler\" : lr_scheduler , \"interval\" : self . lr_scheduler_interval , }, } __init__ ( encoder , decoder , audio_transform , text_transform , optimizer_class = torch . optim . AdamW , optimizer_kwargs = None , lr_scheduler_class = None , lr_scheduler_kwargs = None , encoder_final_dimension = None ) Base module for all systems that follow the same CTC training procedure. PARAMETER DESCRIPTION encoder Encoder part of the model TYPE: nn . Module decoder Decoder part of the model TYPE: nn . Module audio_transform Transforms raw audio into the features the encoder expects TYPE: nn . Module text_transform Class that encodes and decodes all textual representation TYPE: BatchTextTransformer optimizer_class Optimizer to use during training. TYPE: Type [ torch . optim . Optimizer ] DEFAULT: torch.optim.AdamW optimizer_kwargs Optional extra kwargs to the optimizer. TYPE: Dict DEFAULT: None lr_scheduler_class Optional class to use a learning rate scheduler with the optimizer. TYPE: SchedulerBuilderType DEFAULT: None lr_scheduler_kwargs Optional extra kwargs to the learning rate scheduler. TYPE: Dict DEFAULT: None encoder_final_dimension number of features in the encoder output. TYPE: int DEFAULT: None Source code in thunder/module.py 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 def __init__ ( self , encoder : nn . Module , decoder : nn . Module , audio_transform : nn . Module , text_transform : BatchTextTransformer , optimizer_class : Type [ torch . optim . Optimizer ] = torch . optim . AdamW , optimizer_kwargs : Dict = None , lr_scheduler_class : SchedulerBuilderType = None , lr_scheduler_kwargs : Dict = None , encoder_final_dimension : int = None , ): \"\"\"Base module for all systems that follow the same CTC training procedure. Args: encoder: Encoder part of the model decoder: Decoder part of the model audio_transform: Transforms raw audio into the features the encoder expects text_transform: Class that encodes and decodes all textual representation optimizer_class: Optimizer to use during training. optimizer_kwargs: Optional extra kwargs to the optimizer. lr_scheduler_class: Optional class to use a learning rate scheduler with the optimizer. lr_scheduler_kwargs: Optional extra kwargs to the learning rate scheduler. encoder_final_dimension: number of features in the encoder output. \"\"\" super () . __init__ () self . encoder = encoder self . decoder = decoder self . audio_transform = audio_transform self . text_transform = text_transform self . optimizer_class = optimizer_class self . optimizer_kwargs = optimizer_kwargs or {} self . lr_scheduler_class = lr_scheduler_class self . lr_scheduler_kwargs = lr_scheduler_kwargs or {} self . lr_scheduler_interval = self . lr_scheduler_kwargs . pop ( \"interval\" , \"step\" ) self . encoder_final_dimension = encoder_final_dimension # Metrics self . validation_cer = CharErrorRate () self . validation_wer = WordErrorRate () self . example_input_array = ( torch . randn (( 10 , 16000 )), torch . randint ( 100 , 16000 , ( 10 ,)), ) forward ( x , lengths ) Process the audio tensor to create the predictions. PARAMETER DESCRIPTION x Audio tensor of shape [batch_size, time] TYPE: Tensor RETURNS DESCRIPTION Tuple [ Tensor , Optional [ Tensor ]] Tensor with the predictions. Source code in thunder/module.py 74 75 76 77 78 79 80 81 82 83 84 85 def forward ( self , x : Tensor , lengths : Tensor ) -> Tuple [ Tensor , Optional [ Tensor ]]: \"\"\"Process the audio tensor to create the predictions. Args: x: Audio tensor of shape [batch_size, time] Returns: Tensor with the predictions. \"\"\" features , feature_lengths = self . audio_transform ( x , lengths ) encoded , out_lengths = self . encoder ( features , feature_lengths ) return self . decoder ( encoded ), out_lengths predict ( x ) Use this function during inference to predict. PARAMETER DESCRIPTION x Audio tensor of shape [batch_size, time] TYPE: Tensor RETURNS DESCRIPTION List [ str ] A list of strings, each one contains the corresponding transcription to the original batch element. Source code in thunder/module.py 87 88 89 90 91 92 93 94 95 96 97 98 99 @torch . jit . export def predict ( self , x : Tensor ) -> List [ str ]: \"\"\"Use this function during inference to predict. Args: x: Audio tensor of shape [batch_size, time] Returns: A list of strings, each one contains the corresponding transcription to the original batch element. \"\"\" audio_lengths = torch . tensor ( x . shape [ 0 ] * [ x . shape [ - 1 ]], device = x . device ) pred , _ = self ( x , audio_lengths ) return self . text_transform . decode_prediction ( pred . argmax ( 1 )) training_step ( batch , batch_idx ) Training step. Check the original lightning docs for more information. PARAMETER DESCRIPTION batch Tuple containing the batched audios, normalized lengths and the corresponding text labels. TYPE: Tuple [ torch . Tensor , torch . Tensor , List [ str ]] batch_idx Batch index TYPE: int RETURNS DESCRIPTION torch . Tensor Training loss for that batch Source code in thunder/module.py 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 def training_step ( self , batch : Tuple [ torch . Tensor , torch . Tensor , List [ str ]], batch_idx : int ) -> torch . Tensor : \"\"\"Training step. Check the original lightning docs for more information. Args: batch: Tuple containing the batched audios, normalized lengths and the corresponding text labels. batch_idx: Batch index Returns: Training loss for that batch \"\"\" audio , audio_lengths , texts = batch y , y_lengths = self . text_transform . encode ( texts , device = self . device ) probabilities , prob_lengths = self ( audio , audio_lengths ) loss = calculate_ctc ( probabilities , y , prob_lengths , y_lengths , self . text_transform . vocab . blank_idx , ) self . log ( \"loss/train_loss\" , loss ) return loss validation_step ( batch , batch_idx ) Validation step. Check the original lightning docs for more information. PARAMETER DESCRIPTION batch Tuple containing the batched audios, normalized lengths and the corresponding text labels. TYPE: Tuple [ torch . Tensor , torch . Tensor , List [ str ]] batch_idx Batch index TYPE: int RETURNS DESCRIPTION torch . Tensor Validation loss for that batch Source code in thunder/module.py 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 def validation_step ( self , batch : Tuple [ torch . Tensor , torch . Tensor , List [ str ]], batch_idx : int ) -> torch . Tensor : \"\"\"Validation step. Check the original lightning docs for more information. Args: batch: Tuple containing the batched audios, normalized lengths and the corresponding text labels. batch_idx: Batch index Returns: Validation loss for that batch \"\"\" audio , audio_lengths , texts = batch y , y_lengths = self . text_transform . encode ( texts , device = self . device ) probabilities , prob_lengths = self ( audio , audio_lengths ) loss = calculate_ctc ( probabilities , y , prob_lengths , y_lengths , self . text_transform . vocab . blank_idx , ) decoded_preds = self . text_transform . decode_prediction ( probabilities . argmax ( 1 )) decoded_targets = self . text_transform . decode_prediction ( y , remove_repeated = False ) self . validation_cer ( decoded_preds , decoded_targets ) self . validation_wer ( decoded_preds , decoded_targets ) self . log ( \"loss/val_loss\" , loss ) self . log ( \"metrics/cer\" , self . validation_cer , on_epoch = True ) self . log ( \"metrics/wer\" , self . validation_wer , on_epoch = True ) return loss","title":"Module"},{"location":"api/module/#thunder.module.BaseCTCModule","text":"Bases: pl . LightningModule Source code in thunder/module.py 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 class BaseCTCModule ( pl . LightningModule ): def __init__ ( self , encoder : nn . Module , decoder : nn . Module , audio_transform : nn . Module , text_transform : BatchTextTransformer , optimizer_class : Type [ torch . optim . Optimizer ] = torch . optim . AdamW , optimizer_kwargs : Dict = None , lr_scheduler_class : SchedulerBuilderType = None , lr_scheduler_kwargs : Dict = None , encoder_final_dimension : int = None , ): \"\"\"Base module for all systems that follow the same CTC training procedure. Args: encoder: Encoder part of the model decoder: Decoder part of the model audio_transform: Transforms raw audio into the features the encoder expects text_transform: Class that encodes and decodes all textual representation optimizer_class: Optimizer to use during training. optimizer_kwargs: Optional extra kwargs to the optimizer. lr_scheduler_class: Optional class to use a learning rate scheduler with the optimizer. lr_scheduler_kwargs: Optional extra kwargs to the learning rate scheduler. encoder_final_dimension: number of features in the encoder output. \"\"\" super () . __init__ () self . encoder = encoder self . decoder = decoder self . audio_transform = audio_transform self . text_transform = text_transform self . optimizer_class = optimizer_class self . optimizer_kwargs = optimizer_kwargs or {} self . lr_scheduler_class = lr_scheduler_class self . lr_scheduler_kwargs = lr_scheduler_kwargs or {} self . lr_scheduler_interval = self . lr_scheduler_kwargs . pop ( \"interval\" , \"step\" ) self . encoder_final_dimension = encoder_final_dimension # Metrics self . validation_cer = CharErrorRate () self . validation_wer = WordErrorRate () self . example_input_array = ( torch . randn (( 10 , 16000 )), torch . randint ( 100 , 16000 , ( 10 ,)), ) def forward ( self , x : Tensor , lengths : Tensor ) -> Tuple [ Tensor , Optional [ Tensor ]]: \"\"\"Process the audio tensor to create the predictions. Args: x: Audio tensor of shape [batch_size, time] Returns: Tensor with the predictions. \"\"\" features , feature_lengths = self . audio_transform ( x , lengths ) encoded , out_lengths = self . encoder ( features , feature_lengths ) return self . decoder ( encoded ), out_lengths @torch . jit . export def predict ( self , x : Tensor ) -> List [ str ]: \"\"\"Use this function during inference to predict. Args: x: Audio tensor of shape [batch_size, time] Returns: A list of strings, each one contains the corresponding transcription to the original batch element. \"\"\" audio_lengths = torch . tensor ( x . shape [ 0 ] * [ x . shape [ - 1 ]], device = x . device ) pred , _ = self ( x , audio_lengths ) return self . text_transform . decode_prediction ( pred . argmax ( 1 )) def training_step ( self , batch : Tuple [ torch . Tensor , torch . Tensor , List [ str ]], batch_idx : int ) -> torch . Tensor : \"\"\"Training step. Check the original lightning docs for more information. Args: batch: Tuple containing the batched audios, normalized lengths and the corresponding text labels. batch_idx: Batch index Returns: Training loss for that batch \"\"\" audio , audio_lengths , texts = batch y , y_lengths = self . text_transform . encode ( texts , device = self . device ) probabilities , prob_lengths = self ( audio , audio_lengths ) loss = calculate_ctc ( probabilities , y , prob_lengths , y_lengths , self . text_transform . vocab . blank_idx , ) self . log ( \"loss/train_loss\" , loss ) return loss def validation_step ( self , batch : Tuple [ torch . Tensor , torch . Tensor , List [ str ]], batch_idx : int ) -> torch . Tensor : \"\"\"Validation step. Check the original lightning docs for more information. Args: batch: Tuple containing the batched audios, normalized lengths and the corresponding text labels. batch_idx: Batch index Returns: Validation loss for that batch \"\"\" audio , audio_lengths , texts = batch y , y_lengths = self . text_transform . encode ( texts , device = self . device ) probabilities , prob_lengths = self ( audio , audio_lengths ) loss = calculate_ctc ( probabilities , y , prob_lengths , y_lengths , self . text_transform . vocab . blank_idx , ) decoded_preds = self . text_transform . decode_prediction ( probabilities . argmax ( 1 )) decoded_targets = self . text_transform . decode_prediction ( y , remove_repeated = False ) self . validation_cer ( decoded_preds , decoded_targets ) self . validation_wer ( decoded_preds , decoded_targets ) self . log ( \"loss/val_loss\" , loss ) self . log ( \"metrics/cer\" , self . validation_cer , on_epoch = True ) self . log ( \"metrics/wer\" , self . validation_wer , on_epoch = True ) return loss def _update_special_optimizer_arg ( self , original_kwargs : Dict ) -> Dict : updated_kwargs = original_kwargs . copy () total_steps_arg = updated_kwargs . pop ( \"total_steps_arg\" , None ) if total_steps_arg : updated_kwargs [ total_steps_arg ] = self . trainer . estimated_stepping_batches return updated_kwargs def configure_optimizers ( self ) -> Union [ torch . optim . Optimizer , Dict [ str , Any ]]: optim_kwargs = self . _update_special_optimizer_arg ( self . optimizer_kwargs ) optimizer = self . optimizer_class ( filter ( lambda p : p . requires_grad , self . parameters ()), ** optim_kwargs ) if not self . lr_scheduler_class : return optimizer scheduler_kwargs = self . _update_special_optimizer_arg ( self . lr_scheduler_kwargs ) lr_scheduler = self . lr_scheduler_class ( optimizer , ** scheduler_kwargs ) return { \"optimizer\" : optimizer , \"lr_scheduler\" : { \"scheduler\" : lr_scheduler , \"interval\" : self . lr_scheduler_interval , }, }","title":"BaseCTCModule"},{"location":"api/module/#thunder.module.BaseCTCModule.__init__","text":"Base module for all systems that follow the same CTC training procedure. PARAMETER DESCRIPTION encoder Encoder part of the model TYPE: nn . Module decoder Decoder part of the model TYPE: nn . Module audio_transform Transforms raw audio into the features the encoder expects TYPE: nn . Module text_transform Class that encodes and decodes all textual representation TYPE: BatchTextTransformer optimizer_class Optimizer to use during training. TYPE: Type [ torch . optim . Optimizer ] DEFAULT: torch.optim.AdamW optimizer_kwargs Optional extra kwargs to the optimizer. TYPE: Dict DEFAULT: None lr_scheduler_class Optional class to use a learning rate scheduler with the optimizer. TYPE: SchedulerBuilderType DEFAULT: None lr_scheduler_kwargs Optional extra kwargs to the learning rate scheduler. TYPE: Dict DEFAULT: None encoder_final_dimension number of features in the encoder output. TYPE: int DEFAULT: None Source code in thunder/module.py 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 def __init__ ( self , encoder : nn . Module , decoder : nn . Module , audio_transform : nn . Module , text_transform : BatchTextTransformer , optimizer_class : Type [ torch . optim . Optimizer ] = torch . optim . AdamW , optimizer_kwargs : Dict = None , lr_scheduler_class : SchedulerBuilderType = None , lr_scheduler_kwargs : Dict = None , encoder_final_dimension : int = None , ): \"\"\"Base module for all systems that follow the same CTC training procedure. Args: encoder: Encoder part of the model decoder: Decoder part of the model audio_transform: Transforms raw audio into the features the encoder expects text_transform: Class that encodes and decodes all textual representation optimizer_class: Optimizer to use during training. optimizer_kwargs: Optional extra kwargs to the optimizer. lr_scheduler_class: Optional class to use a learning rate scheduler with the optimizer. lr_scheduler_kwargs: Optional extra kwargs to the learning rate scheduler. encoder_final_dimension: number of features in the encoder output. \"\"\" super () . __init__ () self . encoder = encoder self . decoder = decoder self . audio_transform = audio_transform self . text_transform = text_transform self . optimizer_class = optimizer_class self . optimizer_kwargs = optimizer_kwargs or {} self . lr_scheduler_class = lr_scheduler_class self . lr_scheduler_kwargs = lr_scheduler_kwargs or {} self . lr_scheduler_interval = self . lr_scheduler_kwargs . pop ( \"interval\" , \"step\" ) self . encoder_final_dimension = encoder_final_dimension # Metrics self . validation_cer = CharErrorRate () self . validation_wer = WordErrorRate () self . example_input_array = ( torch . randn (( 10 , 16000 )), torch . randint ( 100 , 16000 , ( 10 ,)), )","title":"__init__()"},{"location":"api/module/#thunder.module.BaseCTCModule.forward","text":"Process the audio tensor to create the predictions. PARAMETER DESCRIPTION x Audio tensor of shape [batch_size, time] TYPE: Tensor RETURNS DESCRIPTION Tuple [ Tensor , Optional [ Tensor ]] Tensor with the predictions. Source code in thunder/module.py 74 75 76 77 78 79 80 81 82 83 84 85 def forward ( self , x : Tensor , lengths : Tensor ) -> Tuple [ Tensor , Optional [ Tensor ]]: \"\"\"Process the audio tensor to create the predictions. Args: x: Audio tensor of shape [batch_size, time] Returns: Tensor with the predictions. \"\"\" features , feature_lengths = self . audio_transform ( x , lengths ) encoded , out_lengths = self . encoder ( features , feature_lengths ) return self . decoder ( encoded ), out_lengths","title":"forward()"},{"location":"api/module/#thunder.module.BaseCTCModule.predict","text":"Use this function during inference to predict. PARAMETER DESCRIPTION x Audio tensor of shape [batch_size, time] TYPE: Tensor RETURNS DESCRIPTION List [ str ] A list of strings, each one contains the corresponding transcription to the original batch element. Source code in thunder/module.py 87 88 89 90 91 92 93 94 95 96 97 98 99 @torch . jit . export def predict ( self , x : Tensor ) -> List [ str ]: \"\"\"Use this function during inference to predict. Args: x: Audio tensor of shape [batch_size, time] Returns: A list of strings, each one contains the corresponding transcription to the original batch element. \"\"\" audio_lengths = torch . tensor ( x . shape [ 0 ] * [ x . shape [ - 1 ]], device = x . device ) pred , _ = self ( x , audio_lengths ) return self . text_transform . decode_prediction ( pred . argmax ( 1 ))","title":"predict()"},{"location":"api/module/#thunder.module.BaseCTCModule.training_step","text":"Training step. Check the original lightning docs for more information. PARAMETER DESCRIPTION batch Tuple containing the batched audios, normalized lengths and the corresponding text labels. TYPE: Tuple [ torch . Tensor , torch . Tensor , List [ str ]] batch_idx Batch index TYPE: int RETURNS DESCRIPTION torch . Tensor Training loss for that batch Source code in thunder/module.py 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 def training_step ( self , batch : Tuple [ torch . Tensor , torch . Tensor , List [ str ]], batch_idx : int ) -> torch . Tensor : \"\"\"Training step. Check the original lightning docs for more information. Args: batch: Tuple containing the batched audios, normalized lengths and the corresponding text labels. batch_idx: Batch index Returns: Training loss for that batch \"\"\" audio , audio_lengths , texts = batch y , y_lengths = self . text_transform . encode ( texts , device = self . device ) probabilities , prob_lengths = self ( audio , audio_lengths ) loss = calculate_ctc ( probabilities , y , prob_lengths , y_lengths , self . text_transform . vocab . blank_idx , ) self . log ( \"loss/train_loss\" , loss ) return loss","title":"training_step()"},{"location":"api/module/#thunder.module.BaseCTCModule.validation_step","text":"Validation step. Check the original lightning docs for more information. PARAMETER DESCRIPTION batch Tuple containing the batched audios, normalized lengths and the corresponding text labels. TYPE: Tuple [ torch . Tensor , torch . Tensor , List [ str ]] batch_idx Batch index TYPE: int RETURNS DESCRIPTION torch . Tensor Validation loss for that batch Source code in thunder/module.py 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 def validation_step ( self , batch : Tuple [ torch . Tensor , torch . Tensor , List [ str ]], batch_idx : int ) -> torch . Tensor : \"\"\"Validation step. Check the original lightning docs for more information. Args: batch: Tuple containing the batched audios, normalized lengths and the corresponding text labels. batch_idx: Batch index Returns: Validation loss for that batch \"\"\" audio , audio_lengths , texts = batch y , y_lengths = self . text_transform . encode ( texts , device = self . device ) probabilities , prob_lengths = self ( audio , audio_lengths ) loss = calculate_ctc ( probabilities , y , prob_lengths , y_lengths , self . text_transform . vocab . blank_idx , ) decoded_preds = self . text_transform . decode_prediction ( probabilities . argmax ( 1 )) decoded_targets = self . text_transform . decode_prediction ( y , remove_repeated = False ) self . validation_cer ( decoded_preds , decoded_targets ) self . validation_wer ( decoded_preds , decoded_targets ) self . log ( \"loss/val_loss\" , loss ) self . log ( \"metrics/cer\" , self . validation_cer , on_epoch = True ) self . log ( \"metrics/wer\" , self . validation_wer , on_epoch = True ) return loss","title":"validation_step()"},{"location":"api/registry/","text":"Functionality to register the multiple checkpoints and provide a unified loading interface. load_pretrained ( checkpoint_name , ** load_kwargs ) Load data from any registered checkpoint PARAMETER DESCRIPTION checkpoint_name Base checkpoint name, like \"QuartzNet5x5LS_En\" or \"facebook/wav2vec2-large-960h\" TYPE: Union [ str , BaseCheckpoint ] RETURNS DESCRIPTION BaseCTCModule Object containing the checkpoint data (encoder, decoder, transforms and additional data). Source code in thunder/registry.py 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 def load_pretrained ( checkpoint_name : Union [ str , BaseCheckpoint ], ** load_kwargs ) -> BaseCTCModule : \"\"\"Load data from any registered checkpoint Args: checkpoint_name: Base checkpoint name, like \"QuartzNet5x5LS_En\" or \"facebook/wav2vec2-large-960h\" Returns: Object containing the checkpoint data (encoder, decoder, transforms and additional data). \"\"\" if isinstance ( checkpoint_name , BaseCheckpoint ): checkpoint_name = checkpoint_name . name # Special case when dealing with any huggingface model if \"/\" in checkpoint_name : model_data = load_huggingface_checkpoint ( checkpoint_name , ** load_kwargs ) else : load_fn = CHECKPOINT_REGISTRY [ checkpoint_name ] model_data = load_fn ( ** load_kwargs ) return model_data register_checkpoint_enum ( checkpoints , load_function ) Register all variations of some checkpoint enum with the corresponding loading function PARAMETER DESCRIPTION checkpoints Base checkpoint class TYPE: Type [ BaseCheckpoint ] load_function function to load the checkpoint, must receive one instance of checkpoints as first argument TYPE: CHECKPOINT_LOAD_FUNC_TYPE Source code in thunder/registry.py 28 29 30 31 32 33 34 35 36 37 38 39 40 def register_checkpoint_enum ( checkpoints : Type [ BaseCheckpoint ], load_function : CHECKPOINT_LOAD_FUNC_TYPE ): \"\"\"Register all variations of some checkpoint enum with the corresponding loading function Args: checkpoints: Base checkpoint class load_function: function to load the checkpoint, must receive one instance of `checkpoints` as first argument\"\"\" for checkpoint in checkpoints : CHECKPOINT_REGISTRY . update ( { checkpoint . name : partial ( load_function , checkpoint )} )","title":"Registry"},{"location":"api/registry/#thunder.registry.load_pretrained","text":"Load data from any registered checkpoint PARAMETER DESCRIPTION checkpoint_name Base checkpoint name, like \"QuartzNet5x5LS_En\" or \"facebook/wav2vec2-large-960h\" TYPE: Union [ str , BaseCheckpoint ] RETURNS DESCRIPTION BaseCTCModule Object containing the checkpoint data (encoder, decoder, transforms and additional data). Source code in thunder/registry.py 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 def load_pretrained ( checkpoint_name : Union [ str , BaseCheckpoint ], ** load_kwargs ) -> BaseCTCModule : \"\"\"Load data from any registered checkpoint Args: checkpoint_name: Base checkpoint name, like \"QuartzNet5x5LS_En\" or \"facebook/wav2vec2-large-960h\" Returns: Object containing the checkpoint data (encoder, decoder, transforms and additional data). \"\"\" if isinstance ( checkpoint_name , BaseCheckpoint ): checkpoint_name = checkpoint_name . name # Special case when dealing with any huggingface model if \"/\" in checkpoint_name : model_data = load_huggingface_checkpoint ( checkpoint_name , ** load_kwargs ) else : load_fn = CHECKPOINT_REGISTRY [ checkpoint_name ] model_data = load_fn ( ** load_kwargs ) return model_data","title":"load_pretrained()"},{"location":"api/registry/#thunder.registry.register_checkpoint_enum","text":"Register all variations of some checkpoint enum with the corresponding loading function PARAMETER DESCRIPTION checkpoints Base checkpoint class TYPE: Type [ BaseCheckpoint ] load_function function to load the checkpoint, must receive one instance of checkpoints as first argument TYPE: CHECKPOINT_LOAD_FUNC_TYPE Source code in thunder/registry.py 28 29 30 31 32 33 34 35 36 37 38 39 40 def register_checkpoint_enum ( checkpoints : Type [ BaseCheckpoint ], load_function : CHECKPOINT_LOAD_FUNC_TYPE ): \"\"\"Register all variations of some checkpoint enum with the corresponding loading function Args: checkpoints: Base checkpoint class load_function: function to load the checkpoint, must receive one instance of `checkpoints` as first argument\"\"\" for checkpoint in checkpoints : CHECKPOINT_REGISTRY . update ( { checkpoint . name : partial ( load_function , checkpoint )} )","title":"register_checkpoint_enum()"},{"location":"api/utils/","text":"Utility functions BaseCheckpoint Bases: str , Enum Base class that represents a pretrained model checkpoint. Source code in thunder/utils.py 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 class BaseCheckpoint ( str , Enum ): \"\"\"Base class that represents a pretrained model checkpoint.\"\"\" @classmethod def from_string ( cls , name : str ) -> \"BaseCheckpoint\" : \"\"\"Creates enum value from string. Helper to use with argparse/hydra Args: name: Name of the checkpoint Raises: ValueError: Name provided is not a valid checkpoint Returns: Enum value corresponding to the name \"\"\" try : return cls [ name ] except KeyError as option_does_not_exist : raise ValueError ( \"Name provided is not a valid checkpoint\" ) from option_does_not_exist from_string ( name ) Creates enum value from string. Helper to use with argparse/hydra PARAMETER DESCRIPTION name Name of the checkpoint TYPE: str RAISES DESCRIPTION ValueError Name provided is not a valid checkpoint RETURNS DESCRIPTION 'BaseCheckpoint' Enum value corresponding to the name Source code in thunder/utils.py 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 @classmethod def from_string ( cls , name : str ) -> \"BaseCheckpoint\" : \"\"\"Creates enum value from string. Helper to use with argparse/hydra Args: name: Name of the checkpoint Raises: ValueError: Name provided is not a valid checkpoint Returns: Enum value corresponding to the name \"\"\" try : return cls [ name ] except KeyError as option_does_not_exist : raise ValueError ( \"Name provided is not a valid checkpoint\" ) from option_does_not_exist audio_len ( item ) Returns the length of the audio file PARAMETER DESCRIPTION item Audio path TYPE: Union [ Path , str ] RETURNS DESCRIPTION float Lenght in seconds of the audio Source code in thunder/utils.py 31 32 33 34 35 36 37 38 39 40 41 def audio_len ( item : Union [ Path , str ]) -> float : \"\"\"Returns the length of the audio file Args: item: Audio path Returns: Lenght in seconds of the audio \"\"\" metadata = torchaudio . info ( item ) return metadata . num_frames / metadata . sample_rate chain_calls ( * funcs ) Chain multiple functions that take only one argument, producing a new function that is the result of calling the individual functions in sequence. Example: f1 = lambda x : 2 * x f2 = lambda x : 3 * x f3 = lambda x : 4 * x g = chain_calls ( f1 , f2 , f3 ) assert g ( 1 ) == 24 RETURNS DESCRIPTION Callable Single chained function Source code in thunder/utils.py 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 def chain_calls ( * funcs : List [ Callable ]) -> Callable : \"\"\"Chain multiple functions that take only one argument, producing a new function that is the result of calling the individual functions in sequence. Example: ```python f1 = lambda x: 2 * x f2 = lambda x: 3 * x f3 = lambda x: 4 * x g = chain_calls(f1, f2, f3) assert g(1) == 24 ``` Returns: Single chained function \"\"\" def call ( x , f ): return f ( x ) def _inner ( arg ): return functools . reduce ( call , funcs , arg ) return _inner download_checkpoint ( name , checkpoint_folder = None ) Download checkpoint by identifier. PARAMETER DESCRIPTION name Model identifier. Check checkpoint_archives.keys() TYPE: BaseCheckpoint checkpoint_folder Folder where the checkpoint will be saved to. TYPE: str DEFAULT: None RETURNS DESCRIPTION Path Path to the saved checkpoint file. Source code in thunder/utils.py 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 def download_checkpoint ( name : BaseCheckpoint , checkpoint_folder : str = None ) -> Path : \"\"\"Download checkpoint by identifier. Args: name: Model identifier. Check checkpoint_archives.keys() checkpoint_folder: Folder where the checkpoint will be saved to. Returns: Path to the saved checkpoint file. \"\"\" if checkpoint_folder is None : checkpoint_folder = get_default_cache_folder () url = name . value filename = url . split ( \"/\" )[ - 1 ] checkpoint_path = Path ( checkpoint_folder ) / filename if not checkpoint_path . exists (): wget . download ( url , out = str ( checkpoint_path )) return checkpoint_path get_default_cache_folder () Get the default folder where the cached stuff will be saved. RETURNS DESCRIPTION Path Path of the cache folder. Source code in thunder/utils.py 44 45 46 47 48 49 50 51 52 def get_default_cache_folder () -> Path : \"\"\"Get the default folder where the cached stuff will be saved. Returns: Path of the cache folder. \"\"\" folder = Path . home () / \".thunder\" folder . mkdir ( exist_ok = True ) return folder get_files ( directory , extension ) Find all files in directory with extension. PARAMETER DESCRIPTION directory Directory to recursively find the files TYPE: Union [ str , Path ] extension File extension to search for TYPE: str RETURNS DESCRIPTION List [ Path ] List of all the files that match the extension Source code in thunder/utils.py 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 def get_files ( directory : Union [ str , Path ], extension : str ) -> List [ Path ]: \"\"\"Find all files in directory with extension. Args: directory: Directory to recursively find the files extension: File extension to search for Returns: List of all the files that match the extension \"\"\" files_found = [] for root , _ , files in os . walk ( directory , followlinks = True ): files_found += [ Path ( root ) / f for f in files if f . endswith ( extension )] return files_found","title":"Utils"},{"location":"api/utils/#thunder.utils.BaseCheckpoint","text":"Bases: str , Enum Base class that represents a pretrained model checkpoint. Source code in thunder/utils.py 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 class BaseCheckpoint ( str , Enum ): \"\"\"Base class that represents a pretrained model checkpoint.\"\"\" @classmethod def from_string ( cls , name : str ) -> \"BaseCheckpoint\" : \"\"\"Creates enum value from string. Helper to use with argparse/hydra Args: name: Name of the checkpoint Raises: ValueError: Name provided is not a valid checkpoint Returns: Enum value corresponding to the name \"\"\" try : return cls [ name ] except KeyError as option_does_not_exist : raise ValueError ( \"Name provided is not a valid checkpoint\" ) from option_does_not_exist","title":"BaseCheckpoint"},{"location":"api/utils/#thunder.utils.BaseCheckpoint.from_string","text":"Creates enum value from string. Helper to use with argparse/hydra PARAMETER DESCRIPTION name Name of the checkpoint TYPE: str RAISES DESCRIPTION ValueError Name provided is not a valid checkpoint RETURNS DESCRIPTION 'BaseCheckpoint' Enum value corresponding to the name Source code in thunder/utils.py 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 @classmethod def from_string ( cls , name : str ) -> \"BaseCheckpoint\" : \"\"\"Creates enum value from string. Helper to use with argparse/hydra Args: name: Name of the checkpoint Raises: ValueError: Name provided is not a valid checkpoint Returns: Enum value corresponding to the name \"\"\" try : return cls [ name ] except KeyError as option_does_not_exist : raise ValueError ( \"Name provided is not a valid checkpoint\" ) from option_does_not_exist","title":"from_string()"},{"location":"api/utils/#thunder.utils.audio_len","text":"Returns the length of the audio file PARAMETER DESCRIPTION item Audio path TYPE: Union [ Path , str ] RETURNS DESCRIPTION float Lenght in seconds of the audio Source code in thunder/utils.py 31 32 33 34 35 36 37 38 39 40 41 def audio_len ( item : Union [ Path , str ]) -> float : \"\"\"Returns the length of the audio file Args: item: Audio path Returns: Lenght in seconds of the audio \"\"\" metadata = torchaudio . info ( item ) return metadata . num_frames / metadata . sample_rate","title":"audio_len()"},{"location":"api/utils/#thunder.utils.chain_calls","text":"Chain multiple functions that take only one argument, producing a new function that is the result of calling the individual functions in sequence. Example: f1 = lambda x : 2 * x f2 = lambda x : 3 * x f3 = lambda x : 4 * x g = chain_calls ( f1 , f2 , f3 ) assert g ( 1 ) == 24 RETURNS DESCRIPTION Callable Single chained function Source code in thunder/utils.py 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 def chain_calls ( * funcs : List [ Callable ]) -> Callable : \"\"\"Chain multiple functions that take only one argument, producing a new function that is the result of calling the individual functions in sequence. Example: ```python f1 = lambda x: 2 * x f2 = lambda x: 3 * x f3 = lambda x: 4 * x g = chain_calls(f1, f2, f3) assert g(1) == 24 ``` Returns: Single chained function \"\"\" def call ( x , f ): return f ( x ) def _inner ( arg ): return functools . reduce ( call , funcs , arg ) return _inner","title":"chain_calls()"},{"location":"api/utils/#thunder.utils.download_checkpoint","text":"Download checkpoint by identifier. PARAMETER DESCRIPTION name Model identifier. Check checkpoint_archives.keys() TYPE: BaseCheckpoint checkpoint_folder Folder where the checkpoint will be saved to. TYPE: str DEFAULT: None RETURNS DESCRIPTION Path Path to the saved checkpoint file. Source code in thunder/utils.py 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 def download_checkpoint ( name : BaseCheckpoint , checkpoint_folder : str = None ) -> Path : \"\"\"Download checkpoint by identifier. Args: name: Model identifier. Check checkpoint_archives.keys() checkpoint_folder: Folder where the checkpoint will be saved to. Returns: Path to the saved checkpoint file. \"\"\" if checkpoint_folder is None : checkpoint_folder = get_default_cache_folder () url = name . value filename = url . split ( \"/\" )[ - 1 ] checkpoint_path = Path ( checkpoint_folder ) / filename if not checkpoint_path . exists (): wget . download ( url , out = str ( checkpoint_path )) return checkpoint_path","title":"download_checkpoint()"},{"location":"api/utils/#thunder.utils.get_default_cache_folder","text":"Get the default folder where the cached stuff will be saved. RETURNS DESCRIPTION Path Path of the cache folder. Source code in thunder/utils.py 44 45 46 47 48 49 50 51 52 def get_default_cache_folder () -> Path : \"\"\"Get the default folder where the cached stuff will be saved. Returns: Path of the cache folder. \"\"\" folder = Path . home () / \".thunder\" folder . mkdir ( exist_ok = True ) return folder","title":"get_default_cache_folder()"},{"location":"api/utils/#thunder.utils.get_files","text":"Find all files in directory with extension. PARAMETER DESCRIPTION directory Directory to recursively find the files TYPE: Union [ str , Path ] extension File extension to search for TYPE: str RETURNS DESCRIPTION List [ Path ] List of all the files that match the extension Source code in thunder/utils.py 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 def get_files ( directory : Union [ str , Path ], extension : str ) -> List [ Path ]: \"\"\"Find all files in directory with extension. Args: directory: Directory to recursively find the files extension: File extension to search for Returns: List of all the files that match the extension \"\"\" files_found = [] for root , _ , files in os . walk ( directory , followlinks = True ): files_found += [ Path ( root ) / f for f in files if f . endswith ( extension )] return files_found","title":"get_files()"},{"location":"api/Citrinet/blocks/","text":"Basic building blocks to create the Citrinet model CitrinetBlock Bases: nn . Module Source code in thunder/citrinet/blocks.py 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 class CitrinetBlock ( nn . Module ): def __init__ ( self , in_channels : int , out_channels : int , repeat : int = 5 , kernel_size : _size_1_t = ( 11 ,), stride : _size_1_t = ( 1 ,), dilation : _size_1_t = ( 1 ,), dropout : float = 0.0 , residual : bool = True , separable : bool = False , ): \"\"\"Citrinet block. This is a refactoring of the Jasperblock present on the NeMo toolkit, but simplified to only support the new citrinet model. Biggest change is that dense residual used on Jasper is not supported here. Args: in_channels: Number of input channels out_channels: Number of output channels repeat: Repetitions inside block. kernel_size: Kernel size. stride: Stride of each repetition. dilation: Dilation of each repetition. dropout: Dropout used before each activation. residual: Controls the use of residual connection. separable: Controls the use of separable convolutions. \"\"\" super () . __init__ () padding_val = get_same_padding ( kernel_size [ 0 ], 1 , dilation [ 0 ]) inplanes_loop = in_channels conv = [] for _ in range ( repeat - 1 ): conv . extend ( _get_conv_bn_layer ( inplanes_loop , out_channels , kernel_size = kernel_size , stride = ( 1 ,), # Only stride the last one dilation = dilation , padding = padding_val , separable = separable , bias = False , ) ) conv . extend ( _get_act_dropout_layer ( drop_prob = dropout )) inplanes_loop = out_channels padding_val = get_same_padding ( kernel_size [ 0 ], stride [ 0 ], dilation [ 0 ]) conv . extend ( _get_conv_bn_layer ( inplanes_loop , out_channels , kernel_size = kernel_size , stride = stride , dilation = dilation , padding = padding_val , separable = separable , bias = False , ) ) conv . append ( Masked ( SqueezeExcite ( out_channels , reduction_ratio = 8 ))) self . mconv = MultiSequential ( * conv ) if residual : stride_residual = stride if stride [ 0 ] == 1 else stride [ 0 ] self . res = MultiSequential ( * _get_conv_bn_layer ( in_channels , out_channels , kernel_size = 1 , stride = stride_residual , bias = False , ) ) else : self . res = None self . mout = MultiSequential ( * _get_act_dropout_layer ( drop_prob = dropout )) def forward ( self , x : torch . Tensor , lengths : torch . Tensor ) -> Tuple [ torch . Tensor , torch . Tensor ]: \"\"\" Args: x: Tensor of shape (batch, features, time) where #features == inplanes Returns: Result of applying the block on the input, and corresponding output lengths \"\"\" # compute forward convolutions out , lengths_out = self . mconv ( x , lengths ) # compute the residuals if self . res is not None : res_out , _ = self . res ( x , lengths ) out = out + res_out # compute the output out , lengths_out = self . mout ( out , lengths_out ) return out , lengths_out __init__ ( in_channels , out_channels , repeat = 5 , kernel_size = ( 11 ,), stride = ( 1 ,), dilation = ( 1 ,), dropout = 0.0 , residual = True , separable = False ) Citrinet block. This is a refactoring of the Jasperblock present on the NeMo toolkit, but simplified to only support the new citrinet model. Biggest change is that dense residual used on Jasper is not supported here. PARAMETER DESCRIPTION in_channels Number of input channels TYPE: int out_channels Number of output channels TYPE: int repeat Repetitions inside block. TYPE: int DEFAULT: 5 kernel_size Kernel size. TYPE: _size_1_t DEFAULT: (11,) stride Stride of each repetition. TYPE: _size_1_t DEFAULT: (1,) dilation Dilation of each repetition. TYPE: _size_1_t DEFAULT: (1,) dropout Dropout used before each activation. TYPE: float DEFAULT: 0.0 residual Controls the use of residual connection. TYPE: bool DEFAULT: True separable Controls the use of separable convolutions. TYPE: bool DEFAULT: False Source code in thunder/citrinet/blocks.py 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 def __init__ ( self , in_channels : int , out_channels : int , repeat : int = 5 , kernel_size : _size_1_t = ( 11 ,), stride : _size_1_t = ( 1 ,), dilation : _size_1_t = ( 1 ,), dropout : float = 0.0 , residual : bool = True , separable : bool = False , ): \"\"\"Citrinet block. This is a refactoring of the Jasperblock present on the NeMo toolkit, but simplified to only support the new citrinet model. Biggest change is that dense residual used on Jasper is not supported here. Args: in_channels: Number of input channels out_channels: Number of output channels repeat: Repetitions inside block. kernel_size: Kernel size. stride: Stride of each repetition. dilation: Dilation of each repetition. dropout: Dropout used before each activation. residual: Controls the use of residual connection. separable: Controls the use of separable convolutions. \"\"\" super () . __init__ () padding_val = get_same_padding ( kernel_size [ 0 ], 1 , dilation [ 0 ]) inplanes_loop = in_channels conv = [] for _ in range ( repeat - 1 ): conv . extend ( _get_conv_bn_layer ( inplanes_loop , out_channels , kernel_size = kernel_size , stride = ( 1 ,), # Only stride the last one dilation = dilation , padding = padding_val , separable = separable , bias = False , ) ) conv . extend ( _get_act_dropout_layer ( drop_prob = dropout )) inplanes_loop = out_channels padding_val = get_same_padding ( kernel_size [ 0 ], stride [ 0 ], dilation [ 0 ]) conv . extend ( _get_conv_bn_layer ( inplanes_loop , out_channels , kernel_size = kernel_size , stride = stride , dilation = dilation , padding = padding_val , separable = separable , bias = False , ) ) conv . append ( Masked ( SqueezeExcite ( out_channels , reduction_ratio = 8 ))) self . mconv = MultiSequential ( * conv ) if residual : stride_residual = stride if stride [ 0 ] == 1 else stride [ 0 ] self . res = MultiSequential ( * _get_conv_bn_layer ( in_channels , out_channels , kernel_size = 1 , stride = stride_residual , bias = False , ) ) else : self . res = None self . mout = MultiSequential ( * _get_act_dropout_layer ( drop_prob = dropout )) forward ( x , lengths ) PARAMETER DESCRIPTION x Tensor of shape (batch, features, time) where #features == inplanes TYPE: torch . Tensor RETURNS DESCRIPTION Tuple [ torch . Tensor , torch . Tensor ] Result of applying the block on the input, and corresponding output lengths Source code in thunder/citrinet/blocks.py 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 def forward ( self , x : torch . Tensor , lengths : torch . Tensor ) -> Tuple [ torch . Tensor , torch . Tensor ]: \"\"\" Args: x: Tensor of shape (batch, features, time) where #features == inplanes Returns: Result of applying the block on the input, and corresponding output lengths \"\"\" # compute forward convolutions out , lengths_out = self . mconv ( x , lengths ) # compute the residuals if self . res is not None : res_out , _ = self . res ( x , lengths ) out = out + res_out # compute the output out , lengths_out = self . mout ( out , lengths_out ) return out , lengths_out SqueezeExcite Bases: nn . Module Source code in thunder/citrinet/blocks.py 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 class SqueezeExcite ( nn . Module ): def __init__ ( self , channels : int , reduction_ratio : int , ): \"\"\" Squeeze-and-Excitation sub-module. Args: channels: Input number of channels. reduction_ratio: Reduction ratio for \"squeeze\" layer. \"\"\" super ( SqueezeExcite , self ) . __init__ () self . pool = nn . AdaptiveAvgPool1d ( 1 ) # context window = T self . fc = nn . Sequential ( nn . Linear ( channels , channels // reduction_ratio , bias = False ), nn . ReLU ( True ), nn . Linear ( channels // reduction_ratio , channels , bias = False ), ) def forward ( self , x : torch . Tensor ) -> torch . Tensor : \"\"\" Args: x: Tensor of shape [batch, channels, time] Returns: Tensor of shape [batch, channels, time] \"\"\" y = self . pool ( x ) # [B, C, T - context_window + 1] y = y . transpose ( 1 , - 1 ) # [B, T - context_window + 1, C] y = self . fc ( y ) # [B, T - context_window + 1, C] y = y . transpose ( 1 , - 1 ) # [B, C, T - context_window + 1] y = torch . sigmoid ( y ) return x * y __init__ ( channels , reduction_ratio ) Squeeze-and-Excitation sub-module. PARAMETER DESCRIPTION channels Input number of channels. TYPE: int reduction_ratio Reduction ratio for \"squeeze\" layer. TYPE: int Source code in thunder/citrinet/blocks.py 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 def __init__ ( self , channels : int , reduction_ratio : int , ): \"\"\" Squeeze-and-Excitation sub-module. Args: channels: Input number of channels. reduction_ratio: Reduction ratio for \"squeeze\" layer. \"\"\" super ( SqueezeExcite , self ) . __init__ () self . pool = nn . AdaptiveAvgPool1d ( 1 ) # context window = T self . fc = nn . Sequential ( nn . Linear ( channels , channels // reduction_ratio , bias = False ), nn . ReLU ( True ), nn . Linear ( channels // reduction_ratio , channels , bias = False ), ) forward ( x ) PARAMETER DESCRIPTION x Tensor of shape [batch, channels, time] TYPE: torch . Tensor RETURNS DESCRIPTION torch . Tensor Tensor of shape [batch, channels, time] Source code in thunder/citrinet/blocks.py 70 71 72 73 74 75 76 77 78 79 80 81 82 83 def forward ( self , x : torch . Tensor ) -> torch . Tensor : \"\"\" Args: x: Tensor of shape [batch, channels, time] Returns: Tensor of shape [batch, channels, time] \"\"\" y = self . pool ( x ) # [B, C, T - context_window + 1] y = y . transpose ( 1 , - 1 ) # [B, T - context_window + 1, C] y = self . fc ( y ) # [B, T - context_window + 1, C] y = y . transpose ( 1 , - 1 ) # [B, C, T - context_window + 1] y = torch . sigmoid ( y ) return x * y CitrinetEncoder ( filters , kernel_sizes , strides , feat_in = 80 ) Basic Citrinet encoder setup. PARAMETER DESCRIPTION filters List of filter sizes used to create the encoder blocks. TYPE: List [ int ] kernel_sizes List of kernel sizes corresponding to each filter size. TYPE: List [ int ] strides List of stride corresponding to each filter size. TYPE: List [ int ] feat_in Number of input features to the model. TYPE: int DEFAULT: 80 RETURNS DESCRIPTION nn . Module Pytorch model corresponding to the encoder. Source code in thunder/citrinet/blocks.py 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 def CitrinetEncoder ( filters : List [ int ], kernel_sizes : List [ int ], strides : List [ int ], feat_in : int = 80 ) -> nn . Module : \"\"\"Basic Citrinet encoder setup. Args: filters: List of filter sizes used to create the encoder blocks. kernel_sizes: List of kernel sizes corresponding to each filter size. strides: List of stride corresponding to each filter size. feat_in: Number of input features to the model. Returns: Pytorch model corresponding to the encoder. \"\"\" return MultiSequential ( stem ( feat_in ), * body ( filters , kernel_sizes , strides ), ) body ( filters , kernel_size , strides ) Creates the body of the Citrinet model. That is the middle part. PARAMETER DESCRIPTION filters List of filters inside each block in the body. TYPE: List [ int ] kernel_size Corresponding list of kernel sizes for each block. Should have the same length as the first argument. TYPE: List [ int ] strides Corresponding list of strides for each block. Should have the same length as the first argument. TYPE: List [ int ] RETURNS DESCRIPTION List [ CitrinetBlock ] List of layers that form the body of the network. Source code in thunder/citrinet/blocks.py 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 def body ( filters : List [ int ], kernel_size : List [ int ], strides : List [ int ], ) -> List [ CitrinetBlock ]: \"\"\"Creates the body of the Citrinet model. That is the middle part. Args: filters: List of filters inside each block in the body. kernel_size: Corresponding list of kernel sizes for each block. Should have the same length as the first argument. strides: Corresponding list of strides for each block. Should have the same length as the first argument. Returns: List of layers that form the body of the network. \"\"\" layers = [] f_in = 256 for f , k , s in zip ( filters , kernel_size , strides ): layers . append ( CitrinetBlock ( f_in , f , kernel_size = ( k ,), stride = ( s ,), separable = True ) ) f_in = f layers . append ( CitrinetBlock ( f_in , 640 , repeat = 1 , kernel_size = ( 41 ,), residual = False , separable = True , ) ) return layers stem ( feat_in ) Creates the Citrinet stem. That is the first block of the model, that process the input directly. PARAMETER DESCRIPTION feat_in Number of input features TYPE: int RETURNS DESCRIPTION CitrinetBlock Citrinet stem block Source code in thunder/citrinet/blocks.py 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 def stem ( feat_in : int ) -> CitrinetBlock : \"\"\"Creates the Citrinet stem. That is the first block of the model, that process the input directly. Args: feat_in: Number of input features Returns: Citrinet stem block \"\"\" return CitrinetBlock ( feat_in , 256 , repeat = 1 , kernel_size = ( 5 ,), residual = False , separable = True , )","title":"Blocks"},{"location":"api/Citrinet/blocks/#thunder.citrinet.blocks.CitrinetBlock","text":"Bases: nn . Module Source code in thunder/citrinet/blocks.py 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 class CitrinetBlock ( nn . Module ): def __init__ ( self , in_channels : int , out_channels : int , repeat : int = 5 , kernel_size : _size_1_t = ( 11 ,), stride : _size_1_t = ( 1 ,), dilation : _size_1_t = ( 1 ,), dropout : float = 0.0 , residual : bool = True , separable : bool = False , ): \"\"\"Citrinet block. This is a refactoring of the Jasperblock present on the NeMo toolkit, but simplified to only support the new citrinet model. Biggest change is that dense residual used on Jasper is not supported here. Args: in_channels: Number of input channels out_channels: Number of output channels repeat: Repetitions inside block. kernel_size: Kernel size. stride: Stride of each repetition. dilation: Dilation of each repetition. dropout: Dropout used before each activation. residual: Controls the use of residual connection. separable: Controls the use of separable convolutions. \"\"\" super () . __init__ () padding_val = get_same_padding ( kernel_size [ 0 ], 1 , dilation [ 0 ]) inplanes_loop = in_channels conv = [] for _ in range ( repeat - 1 ): conv . extend ( _get_conv_bn_layer ( inplanes_loop , out_channels , kernel_size = kernel_size , stride = ( 1 ,), # Only stride the last one dilation = dilation , padding = padding_val , separable = separable , bias = False , ) ) conv . extend ( _get_act_dropout_layer ( drop_prob = dropout )) inplanes_loop = out_channels padding_val = get_same_padding ( kernel_size [ 0 ], stride [ 0 ], dilation [ 0 ]) conv . extend ( _get_conv_bn_layer ( inplanes_loop , out_channels , kernel_size = kernel_size , stride = stride , dilation = dilation , padding = padding_val , separable = separable , bias = False , ) ) conv . append ( Masked ( SqueezeExcite ( out_channels , reduction_ratio = 8 ))) self . mconv = MultiSequential ( * conv ) if residual : stride_residual = stride if stride [ 0 ] == 1 else stride [ 0 ] self . res = MultiSequential ( * _get_conv_bn_layer ( in_channels , out_channels , kernel_size = 1 , stride = stride_residual , bias = False , ) ) else : self . res = None self . mout = MultiSequential ( * _get_act_dropout_layer ( drop_prob = dropout )) def forward ( self , x : torch . Tensor , lengths : torch . Tensor ) -> Tuple [ torch . Tensor , torch . Tensor ]: \"\"\" Args: x: Tensor of shape (batch, features, time) where #features == inplanes Returns: Result of applying the block on the input, and corresponding output lengths \"\"\" # compute forward convolutions out , lengths_out = self . mconv ( x , lengths ) # compute the residuals if self . res is not None : res_out , _ = self . res ( x , lengths ) out = out + res_out # compute the output out , lengths_out = self . mout ( out , lengths_out ) return out , lengths_out","title":"CitrinetBlock"},{"location":"api/Citrinet/blocks/#thunder.citrinet.blocks.CitrinetBlock.__init__","text":"Citrinet block. This is a refactoring of the Jasperblock present on the NeMo toolkit, but simplified to only support the new citrinet model. Biggest change is that dense residual used on Jasper is not supported here. PARAMETER DESCRIPTION in_channels Number of input channels TYPE: int out_channels Number of output channels TYPE: int repeat Repetitions inside block. TYPE: int DEFAULT: 5 kernel_size Kernel size. TYPE: _size_1_t DEFAULT: (11,) stride Stride of each repetition. TYPE: _size_1_t DEFAULT: (1,) dilation Dilation of each repetition. TYPE: _size_1_t DEFAULT: (1,) dropout Dropout used before each activation. TYPE: float DEFAULT: 0.0 residual Controls the use of residual connection. TYPE: bool DEFAULT: True separable Controls the use of separable convolutions. TYPE: bool DEFAULT: False Source code in thunder/citrinet/blocks.py 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 def __init__ ( self , in_channels : int , out_channels : int , repeat : int = 5 , kernel_size : _size_1_t = ( 11 ,), stride : _size_1_t = ( 1 ,), dilation : _size_1_t = ( 1 ,), dropout : float = 0.0 , residual : bool = True , separable : bool = False , ): \"\"\"Citrinet block. This is a refactoring of the Jasperblock present on the NeMo toolkit, but simplified to only support the new citrinet model. Biggest change is that dense residual used on Jasper is not supported here. Args: in_channels: Number of input channels out_channels: Number of output channels repeat: Repetitions inside block. kernel_size: Kernel size. stride: Stride of each repetition. dilation: Dilation of each repetition. dropout: Dropout used before each activation. residual: Controls the use of residual connection. separable: Controls the use of separable convolutions. \"\"\" super () . __init__ () padding_val = get_same_padding ( kernel_size [ 0 ], 1 , dilation [ 0 ]) inplanes_loop = in_channels conv = [] for _ in range ( repeat - 1 ): conv . extend ( _get_conv_bn_layer ( inplanes_loop , out_channels , kernel_size = kernel_size , stride = ( 1 ,), # Only stride the last one dilation = dilation , padding = padding_val , separable = separable , bias = False , ) ) conv . extend ( _get_act_dropout_layer ( drop_prob = dropout )) inplanes_loop = out_channels padding_val = get_same_padding ( kernel_size [ 0 ], stride [ 0 ], dilation [ 0 ]) conv . extend ( _get_conv_bn_layer ( inplanes_loop , out_channels , kernel_size = kernel_size , stride = stride , dilation = dilation , padding = padding_val , separable = separable , bias = False , ) ) conv . append ( Masked ( SqueezeExcite ( out_channels , reduction_ratio = 8 ))) self . mconv = MultiSequential ( * conv ) if residual : stride_residual = stride if stride [ 0 ] == 1 else stride [ 0 ] self . res = MultiSequential ( * _get_conv_bn_layer ( in_channels , out_channels , kernel_size = 1 , stride = stride_residual , bias = False , ) ) else : self . res = None self . mout = MultiSequential ( * _get_act_dropout_layer ( drop_prob = dropout ))","title":"__init__()"},{"location":"api/Citrinet/blocks/#thunder.citrinet.blocks.CitrinetBlock.forward","text":"PARAMETER DESCRIPTION x Tensor of shape (batch, features, time) where #features == inplanes TYPE: torch . Tensor RETURNS DESCRIPTION Tuple [ torch . Tensor , torch . Tensor ] Result of applying the block on the input, and corresponding output lengths Source code in thunder/citrinet/blocks.py 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 def forward ( self , x : torch . Tensor , lengths : torch . Tensor ) -> Tuple [ torch . Tensor , torch . Tensor ]: \"\"\" Args: x: Tensor of shape (batch, features, time) where #features == inplanes Returns: Result of applying the block on the input, and corresponding output lengths \"\"\" # compute forward convolutions out , lengths_out = self . mconv ( x , lengths ) # compute the residuals if self . res is not None : res_out , _ = self . res ( x , lengths ) out = out + res_out # compute the output out , lengths_out = self . mout ( out , lengths_out ) return out , lengths_out","title":"forward()"},{"location":"api/Citrinet/blocks/#thunder.citrinet.blocks.SqueezeExcite","text":"Bases: nn . Module Source code in thunder/citrinet/blocks.py 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 class SqueezeExcite ( nn . Module ): def __init__ ( self , channels : int , reduction_ratio : int , ): \"\"\" Squeeze-and-Excitation sub-module. Args: channels: Input number of channels. reduction_ratio: Reduction ratio for \"squeeze\" layer. \"\"\" super ( SqueezeExcite , self ) . __init__ () self . pool = nn . AdaptiveAvgPool1d ( 1 ) # context window = T self . fc = nn . Sequential ( nn . Linear ( channels , channels // reduction_ratio , bias = False ), nn . ReLU ( True ), nn . Linear ( channels // reduction_ratio , channels , bias = False ), ) def forward ( self , x : torch . Tensor ) -> torch . Tensor : \"\"\" Args: x: Tensor of shape [batch, channels, time] Returns: Tensor of shape [batch, channels, time] \"\"\" y = self . pool ( x ) # [B, C, T - context_window + 1] y = y . transpose ( 1 , - 1 ) # [B, T - context_window + 1, C] y = self . fc ( y ) # [B, T - context_window + 1, C] y = y . transpose ( 1 , - 1 ) # [B, C, T - context_window + 1] y = torch . sigmoid ( y ) return x * y","title":"SqueezeExcite"},{"location":"api/Citrinet/blocks/#thunder.citrinet.blocks.SqueezeExcite.__init__","text":"Squeeze-and-Excitation sub-module. PARAMETER DESCRIPTION channels Input number of channels. TYPE: int reduction_ratio Reduction ratio for \"squeeze\" layer. TYPE: int Source code in thunder/citrinet/blocks.py 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 def __init__ ( self , channels : int , reduction_ratio : int , ): \"\"\" Squeeze-and-Excitation sub-module. Args: channels: Input number of channels. reduction_ratio: Reduction ratio for \"squeeze\" layer. \"\"\" super ( SqueezeExcite , self ) . __init__ () self . pool = nn . AdaptiveAvgPool1d ( 1 ) # context window = T self . fc = nn . Sequential ( nn . Linear ( channels , channels // reduction_ratio , bias = False ), nn . ReLU ( True ), nn . Linear ( channels // reduction_ratio , channels , bias = False ), )","title":"__init__()"},{"location":"api/Citrinet/blocks/#thunder.citrinet.blocks.SqueezeExcite.forward","text":"PARAMETER DESCRIPTION x Tensor of shape [batch, channels, time] TYPE: torch . Tensor RETURNS DESCRIPTION torch . Tensor Tensor of shape [batch, channels, time] Source code in thunder/citrinet/blocks.py 70 71 72 73 74 75 76 77 78 79 80 81 82 83 def forward ( self , x : torch . Tensor ) -> torch . Tensor : \"\"\" Args: x: Tensor of shape [batch, channels, time] Returns: Tensor of shape [batch, channels, time] \"\"\" y = self . pool ( x ) # [B, C, T - context_window + 1] y = y . transpose ( 1 , - 1 ) # [B, T - context_window + 1, C] y = self . fc ( y ) # [B, T - context_window + 1, C] y = y . transpose ( 1 , - 1 ) # [B, C, T - context_window + 1] y = torch . sigmoid ( y ) return x * y","title":"forward()"},{"location":"api/Citrinet/blocks/#thunder.citrinet.blocks.CitrinetEncoder","text":"Basic Citrinet encoder setup. PARAMETER DESCRIPTION filters List of filter sizes used to create the encoder blocks. TYPE: List [ int ] kernel_sizes List of kernel sizes corresponding to each filter size. TYPE: List [ int ] strides List of stride corresponding to each filter size. TYPE: List [ int ] feat_in Number of input features to the model. TYPE: int DEFAULT: 80 RETURNS DESCRIPTION nn . Module Pytorch model corresponding to the encoder. Source code in thunder/citrinet/blocks.py 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 def CitrinetEncoder ( filters : List [ int ], kernel_sizes : List [ int ], strides : List [ int ], feat_in : int = 80 ) -> nn . Module : \"\"\"Basic Citrinet encoder setup. Args: filters: List of filter sizes used to create the encoder blocks. kernel_sizes: List of kernel sizes corresponding to each filter size. strides: List of stride corresponding to each filter size. feat_in: Number of input features to the model. Returns: Pytorch model corresponding to the encoder. \"\"\" return MultiSequential ( stem ( feat_in ), * body ( filters , kernel_sizes , strides ), )","title":"CitrinetEncoder()"},{"location":"api/Citrinet/blocks/#thunder.citrinet.blocks.body","text":"Creates the body of the Citrinet model. That is the middle part. PARAMETER DESCRIPTION filters List of filters inside each block in the body. TYPE: List [ int ] kernel_size Corresponding list of kernel sizes for each block. Should have the same length as the first argument. TYPE: List [ int ] strides Corresponding list of strides for each block. Should have the same length as the first argument. TYPE: List [ int ] RETURNS DESCRIPTION List [ CitrinetBlock ] List of layers that form the body of the network. Source code in thunder/citrinet/blocks.py 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 def body ( filters : List [ int ], kernel_size : List [ int ], strides : List [ int ], ) -> List [ CitrinetBlock ]: \"\"\"Creates the body of the Citrinet model. That is the middle part. Args: filters: List of filters inside each block in the body. kernel_size: Corresponding list of kernel sizes for each block. Should have the same length as the first argument. strides: Corresponding list of strides for each block. Should have the same length as the first argument. Returns: List of layers that form the body of the network. \"\"\" layers = [] f_in = 256 for f , k , s in zip ( filters , kernel_size , strides ): layers . append ( CitrinetBlock ( f_in , f , kernel_size = ( k ,), stride = ( s ,), separable = True ) ) f_in = f layers . append ( CitrinetBlock ( f_in , 640 , repeat = 1 , kernel_size = ( 41 ,), residual = False , separable = True , ) ) return layers","title":"body()"},{"location":"api/Citrinet/blocks/#thunder.citrinet.blocks.stem","text":"Creates the Citrinet stem. That is the first block of the model, that process the input directly. PARAMETER DESCRIPTION feat_in Number of input features TYPE: int RETURNS DESCRIPTION CitrinetBlock Citrinet stem block Source code in thunder/citrinet/blocks.py 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 def stem ( feat_in : int ) -> CitrinetBlock : \"\"\"Creates the Citrinet stem. That is the first block of the model, that process the input directly. Args: feat_in: Number of input features Returns: Citrinet stem block \"\"\" return CitrinetBlock ( feat_in , 256 , repeat = 1 , kernel_size = ( 5 ,), residual = False , separable = True , )","title":"stem()"},{"location":"api/Citrinet/compatibility/","text":"Helper functions to load the Citrinet model from original Nemo released checkpoint files. CitrinetCheckpoint Bases: BaseCheckpoint Note Possible values are stt_en_citrinet_256 , stt_en_citrinet_512 , stt_en_citrinet_1024 , stt_es_citrinet_512 Trained model weight checkpoints. Used by download_checkpoint and load_citrinet_checkpoint . Source code in thunder/citrinet/compatibility.py 30 31 32 33 34 35 36 37 38 39 40 41 class CitrinetCheckpoint ( BaseCheckpoint ): \"\"\"Trained model weight checkpoints. Used by [`download_checkpoint`][thunder.utils.download_checkpoint] and [`load_citrinet_checkpoint`][thunder.citrinet.compatibility.load_citrinet_checkpoint]. Note: Possible values are `stt_en_citrinet_256`,`stt_en_citrinet_512`,`stt_en_citrinet_1024`, `stt_es_citrinet_512` \"\"\" stt_en_citrinet_256 = \"https://api.ngc.nvidia.com/v2/models/nvidia/nemo/stt_en_citrinet_256/versions/1.0.0rc1/files/stt_en_citrinet_256.nemo\" stt_en_citrinet_512 = \"https://api.ngc.nvidia.com/v2/models/nvidia/nemo/stt_en_citrinet_512/versions/1.0.0rc1/files/stt_en_citrinet_512.nemo\" stt_en_citrinet_1024 = \"https://api.ngc.nvidia.com/v2/models/nvidia/nemo/stt_en_citrinet_1024/versions/1.0.0rc1/files/stt_en_citrinet_1024.nemo\" stt_es_citrinet_512 = \"https://api.ngc.nvidia.com/v2/models/nvidia/nemo/stt_es_citrinet_512/versions/1.0.0/files/stt_es_citrinet_512.nemo\" fix_vocab ( vocab_tokens ) Transform the nemo vocab tokens back to the sentencepiece sytle with the _ prefix PARAMETER DESCRIPTION vocab_tokens List of tokens in the vocabulary TYPE: List [ str ] RETURNS DESCRIPTION List [ str ] New list of tokens with the new prefix Source code in thunder/citrinet/compatibility.py 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 def fix_vocab ( vocab_tokens : List [ str ]) -> List [ str ]: \"\"\"Transform the nemo vocab tokens back to the sentencepiece sytle with the _ prefix Args: vocab_tokens: List of tokens in the vocabulary Returns: New list of tokens with the new prefix \"\"\" out_tokens = [] for token in vocab_tokens : if token . startswith ( \"##\" ): out_tokens . append ( token [ 2 :]) else : out_tokens . append ( \"\u2581\" + token ) return out_tokens load_citrinet_checkpoint ( checkpoint , save_folder = None ) Load from the original nemo checkpoint. PARAMETER DESCRIPTION checkpoint Path to local .nemo file or checkpoint to be downloaded locally and lodaded. TYPE: Union [ str , CitrinetCheckpoint ] save_folder Path to save the checkpoint when downloading it. Ignored if you pass a .nemo file as the first argument. TYPE: str DEFAULT: None RETURNS DESCRIPTION BaseCTCModule The model loaded from the checkpoint Source code in thunder/citrinet/compatibility.py 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 def load_citrinet_checkpoint ( checkpoint : Union [ str , CitrinetCheckpoint ], save_folder : str = None ) -> BaseCTCModule : \"\"\"Load from the original nemo checkpoint. Args: checkpoint: Path to local .nemo file or checkpoint to be downloaded locally and lodaded. save_folder: Path to save the checkpoint when downloading it. Ignored if you pass a .nemo file as the first argument. Returns: The model loaded from the checkpoint \"\"\" if isinstance ( checkpoint , CitrinetCheckpoint ): nemo_filepath = download_checkpoint ( checkpoint , save_folder ) else : nemo_filepath = checkpoint with TemporaryDirectory () as extract_folder : extract_archive ( str ( nemo_filepath ), extract_folder ) extract_path = Path ( extract_folder ) config_path = extract_path / \"model_config.yaml\" sentencepiece_path = str ( extract_path / \"tokenizer.model\" ) ( encoder , audio_transform , text_transform , ) = load_components_from_citrinet_config ( config_path , sentencepiece_path ) decoder = conv1d_decoder ( 640 , num_classes = text_transform . num_tokens ) weights_path = extract_path / \"model_weights.ckpt\" load_quartznet_weights ( encoder , decoder , str ( weights_path )) module = BaseCTCModule ( encoder , decoder , audio_transform , text_transform , encoder_final_dimension = 640 , ) return module . eval () load_components_from_citrinet_config ( config_path , sentencepiece_path ) Read the important parameters from the config stored inside the .nemo checkpoint. PARAMETER DESCRIPTION config_path Path to the .yaml file, usually called model_config.yaml TYPE: Union [ str , Path ] RETURNS DESCRIPTION Tuple [ nn . Module , nn . Module , BatchTextTransformer ] A tuple containing, in this order, the encoder, the audio transform and the text transform Source code in thunder/citrinet/compatibility.py 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 def load_components_from_citrinet_config ( config_path : Union [ str , Path ], sentencepiece_path : Union [ str , Path ] ) -> Tuple [ nn . Module , nn . Module , BatchTextTransformer ]: \"\"\"Read the important parameters from the config stored inside the .nemo checkpoint. Args: config_path: Path to the .yaml file, usually called model_config.yaml Returns: A tuple containing, in this order, the encoder, the audio transform and the text transform \"\"\" conf = OmegaConf . load ( config_path ) encoder_params = conf [ \"encoder\" ] quartznet_conf = OmegaConf . to_container ( encoder_params [ \"jasper\" ]) body_config = quartznet_conf [ 1 : - 1 ] filters = [ cfg [ \"filters\" ] for cfg in body_config ] kernel_sizes = [ cfg [ \"kernel\" ][ 0 ] for cfg in body_config ] strides = [ cfg [ \"stride\" ][ 0 ] for cfg in body_config ] encoder_cfg = { \"filters\" : filters , \"kernel_sizes\" : kernel_sizes , \"strides\" : strides , } preprocess = conf [ \"preprocessor\" ] preprocess_cfg = { \"sample_rate\" : preprocess [ \"sample_rate\" ], \"n_window_size\" : int ( preprocess [ \"window_size\" ] * preprocess [ \"sample_rate\" ]), \"n_window_stride\" : int ( preprocess [ \"window_stride\" ] * preprocess [ \"sample_rate\" ]), \"n_fft\" : preprocess [ \"n_fft\" ], \"nfilt\" : preprocess [ \"features\" ], \"dither\" : preprocess [ \"dither\" ], } labels = conf [ \"labels\" ] if \"labels\" in conf else conf [ \"decoder\" ][ \"vocabulary\" ] encoder = CitrinetEncoder ( ** encoder_cfg ) text_transform = BatchTextTransformer ( tokens = fix_vocab ( labels ), sentencepiece_model = sentencepiece_path , ) audio_transform = FilterbankFeatures ( ** preprocess_cfg ) return ( encoder , audio_transform , text_transform , )","title":"Compatibility"},{"location":"api/Citrinet/compatibility/#thunder.citrinet.compatibility.CitrinetCheckpoint","text":"Bases: BaseCheckpoint Note Possible values are stt_en_citrinet_256 , stt_en_citrinet_512 , stt_en_citrinet_1024 , stt_es_citrinet_512 Trained model weight checkpoints. Used by download_checkpoint and load_citrinet_checkpoint . Source code in thunder/citrinet/compatibility.py 30 31 32 33 34 35 36 37 38 39 40 41 class CitrinetCheckpoint ( BaseCheckpoint ): \"\"\"Trained model weight checkpoints. Used by [`download_checkpoint`][thunder.utils.download_checkpoint] and [`load_citrinet_checkpoint`][thunder.citrinet.compatibility.load_citrinet_checkpoint]. Note: Possible values are `stt_en_citrinet_256`,`stt_en_citrinet_512`,`stt_en_citrinet_1024`, `stt_es_citrinet_512` \"\"\" stt_en_citrinet_256 = \"https://api.ngc.nvidia.com/v2/models/nvidia/nemo/stt_en_citrinet_256/versions/1.0.0rc1/files/stt_en_citrinet_256.nemo\" stt_en_citrinet_512 = \"https://api.ngc.nvidia.com/v2/models/nvidia/nemo/stt_en_citrinet_512/versions/1.0.0rc1/files/stt_en_citrinet_512.nemo\" stt_en_citrinet_1024 = \"https://api.ngc.nvidia.com/v2/models/nvidia/nemo/stt_en_citrinet_1024/versions/1.0.0rc1/files/stt_en_citrinet_1024.nemo\" stt_es_citrinet_512 = \"https://api.ngc.nvidia.com/v2/models/nvidia/nemo/stt_es_citrinet_512/versions/1.0.0/files/stt_es_citrinet_512.nemo\"","title":"CitrinetCheckpoint"},{"location":"api/Citrinet/compatibility/#thunder.citrinet.compatibility.fix_vocab","text":"Transform the nemo vocab tokens back to the sentencepiece sytle with the _ prefix PARAMETER DESCRIPTION vocab_tokens List of tokens in the vocabulary TYPE: List [ str ] RETURNS DESCRIPTION List [ str ] New list of tokens with the new prefix Source code in thunder/citrinet/compatibility.py 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 def fix_vocab ( vocab_tokens : List [ str ]) -> List [ str ]: \"\"\"Transform the nemo vocab tokens back to the sentencepiece sytle with the _ prefix Args: vocab_tokens: List of tokens in the vocabulary Returns: New list of tokens with the new prefix \"\"\" out_tokens = [] for token in vocab_tokens : if token . startswith ( \"##\" ): out_tokens . append ( token [ 2 :]) else : out_tokens . append ( \"\u2581\" + token ) return out_tokens","title":"fix_vocab()"},{"location":"api/Citrinet/compatibility/#thunder.citrinet.compatibility.load_citrinet_checkpoint","text":"Load from the original nemo checkpoint. PARAMETER DESCRIPTION checkpoint Path to local .nemo file or checkpoint to be downloaded locally and lodaded. TYPE: Union [ str , CitrinetCheckpoint ] save_folder Path to save the checkpoint when downloading it. Ignored if you pass a .nemo file as the first argument. TYPE: str DEFAULT: None RETURNS DESCRIPTION BaseCTCModule The model loaded from the checkpoint Source code in thunder/citrinet/compatibility.py 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 def load_citrinet_checkpoint ( checkpoint : Union [ str , CitrinetCheckpoint ], save_folder : str = None ) -> BaseCTCModule : \"\"\"Load from the original nemo checkpoint. Args: checkpoint: Path to local .nemo file or checkpoint to be downloaded locally and lodaded. save_folder: Path to save the checkpoint when downloading it. Ignored if you pass a .nemo file as the first argument. Returns: The model loaded from the checkpoint \"\"\" if isinstance ( checkpoint , CitrinetCheckpoint ): nemo_filepath = download_checkpoint ( checkpoint , save_folder ) else : nemo_filepath = checkpoint with TemporaryDirectory () as extract_folder : extract_archive ( str ( nemo_filepath ), extract_folder ) extract_path = Path ( extract_folder ) config_path = extract_path / \"model_config.yaml\" sentencepiece_path = str ( extract_path / \"tokenizer.model\" ) ( encoder , audio_transform , text_transform , ) = load_components_from_citrinet_config ( config_path , sentencepiece_path ) decoder = conv1d_decoder ( 640 , num_classes = text_transform . num_tokens ) weights_path = extract_path / \"model_weights.ckpt\" load_quartznet_weights ( encoder , decoder , str ( weights_path )) module = BaseCTCModule ( encoder , decoder , audio_transform , text_transform , encoder_final_dimension = 640 , ) return module . eval ()","title":"load_citrinet_checkpoint()"},{"location":"api/Citrinet/compatibility/#thunder.citrinet.compatibility.load_components_from_citrinet_config","text":"Read the important parameters from the config stored inside the .nemo checkpoint. PARAMETER DESCRIPTION config_path Path to the .yaml file, usually called model_config.yaml TYPE: Union [ str , Path ] RETURNS DESCRIPTION Tuple [ nn . Module , nn . Module , BatchTextTransformer ] A tuple containing, in this order, the encoder, the audio transform and the text transform Source code in thunder/citrinet/compatibility.py 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 def load_components_from_citrinet_config ( config_path : Union [ str , Path ], sentencepiece_path : Union [ str , Path ] ) -> Tuple [ nn . Module , nn . Module , BatchTextTransformer ]: \"\"\"Read the important parameters from the config stored inside the .nemo checkpoint. Args: config_path: Path to the .yaml file, usually called model_config.yaml Returns: A tuple containing, in this order, the encoder, the audio transform and the text transform \"\"\" conf = OmegaConf . load ( config_path ) encoder_params = conf [ \"encoder\" ] quartznet_conf = OmegaConf . to_container ( encoder_params [ \"jasper\" ]) body_config = quartznet_conf [ 1 : - 1 ] filters = [ cfg [ \"filters\" ] for cfg in body_config ] kernel_sizes = [ cfg [ \"kernel\" ][ 0 ] for cfg in body_config ] strides = [ cfg [ \"stride\" ][ 0 ] for cfg in body_config ] encoder_cfg = { \"filters\" : filters , \"kernel_sizes\" : kernel_sizes , \"strides\" : strides , } preprocess = conf [ \"preprocessor\" ] preprocess_cfg = { \"sample_rate\" : preprocess [ \"sample_rate\" ], \"n_window_size\" : int ( preprocess [ \"window_size\" ] * preprocess [ \"sample_rate\" ]), \"n_window_stride\" : int ( preprocess [ \"window_stride\" ] * preprocess [ \"sample_rate\" ]), \"n_fft\" : preprocess [ \"n_fft\" ], \"nfilt\" : preprocess [ \"features\" ], \"dither\" : preprocess [ \"dither\" ], } labels = conf [ \"labels\" ] if \"labels\" in conf else conf [ \"decoder\" ][ \"vocabulary\" ] encoder = CitrinetEncoder ( ** encoder_cfg ) text_transform = BatchTextTransformer ( tokens = fix_vocab ( labels ), sentencepiece_model = sentencepiece_path , ) audio_transform = FilterbankFeatures ( ** preprocess_cfg ) return ( encoder , audio_transform , text_transform , )","title":"load_components_from_citrinet_config()"},{"location":"api/Data/dataloader%20utils/","text":"Helper functions used by the speech dataloaders. asr_collate ( samples ) Function that collect samples and adds padding. PARAMETER DESCRIPTION samples Samples produced by dataloader TYPE: List [ Tuple [ Tensor , str ]] RETURNS DESCRIPTION Tuple [ Tensor , Tensor , List [ str ]] Tuple containing padded audios, audio lengths (normalized to 0.0 <-> 1.0 range) and the list of corresponding transcriptions in that order. Source code in thunder/data/dataloader_utils.py 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 def asr_collate ( samples : List [ Tuple [ Tensor , str ]]) -> Tuple [ Tensor , Tensor , List [ str ]]: \"\"\"Function that collect samples and adds padding. Args: samples: Samples produced by dataloader Returns: Tuple containing padded audios, audio lengths (normalized to 0.0 <-> 1.0 range) and the list of corresponding transcriptions in that order. \"\"\" samples = sorted ( samples , key = lambda sample : sample [ 0 ] . size ( - 1 ), reverse = True ) padded_audios = pad_sequence ([ s [ 0 ] . squeeze () for s in samples ], batch_first = True ) audio_lengths = Tensor ([ s [ 0 ] . size ( - 1 ) for s in samples ]) texts = [ s [ 1 ] for s in samples ] return ( padded_audios , audio_lengths , texts )","title":"Dataloader utils"},{"location":"api/Data/dataloader%20utils/#thunder.data.dataloader_utils.asr_collate","text":"Function that collect samples and adds padding. PARAMETER DESCRIPTION samples Samples produced by dataloader TYPE: List [ Tuple [ Tensor , str ]] RETURNS DESCRIPTION Tuple [ Tensor , Tensor , List [ str ]] Tuple containing padded audios, audio lengths (normalized to 0.0 <-> 1.0 range) and the list of corresponding transcriptions in that order. Source code in thunder/data/dataloader_utils.py 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 def asr_collate ( samples : List [ Tuple [ Tensor , str ]]) -> Tuple [ Tensor , Tensor , List [ str ]]: \"\"\"Function that collect samples and adds padding. Args: samples: Samples produced by dataloader Returns: Tuple containing padded audios, audio lengths (normalized to 0.0 <-> 1.0 range) and the list of corresponding transcriptions in that order. \"\"\" samples = sorted ( samples , key = lambda sample : sample [ 0 ] . size ( - 1 ), reverse = True ) padded_audios = pad_sequence ([ s [ 0 ] . squeeze () for s in samples ], batch_first = True ) audio_lengths = Tensor ([ s [ 0 ] . size ( - 1 ) for s in samples ]) texts = [ s [ 1 ] for s in samples ] return ( padded_audios , audio_lengths , texts )","title":"asr_collate()"},{"location":"api/Data/datamodule/","text":"Implements pytorch lightning's Datamodule for audio datasets. BaseDataModule Bases: LightningDataModule Source code in thunder/data/datamodule.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 class BaseDataModule ( LightningDataModule ): def __init__ ( self , batch_size : int = 10 , num_workers : int = 8 , ): super () . __init__ () self . batch_size = batch_size self . num_workers = num_workers def get_dataset ( self , split : str ) -> BaseSpeechDataset : \"\"\"Function to get the corresponding dataset to the specified split. This should be implemented by subclasses. Args: split: One of \"train\", \"valid\" or \"test\". Returns: The corresponding dataset. \"\"\" raise NotImplementedError () def setup ( self , stage : Optional [ str ] = None ): if stage in ( None , \"fit\" ): self . train_dataset = self . get_dataset ( split = \"train\" ) self . val_dataset = self . get_dataset ( split = \"valid\" ) if stage in ( None , \"test\" ): self . test_dataset = self . get_dataset ( split = \"test\" ) def train_dataloader ( self ) -> DataLoader : return DataLoader ( self . train_dataset , batch_size = self . batch_size , collate_fn = asr_collate , num_workers = self . num_workers , shuffle = True , pin_memory = True , ) def val_dataloader ( self ) -> DataLoader : return DataLoader ( self . val_dataset , batch_size = self . batch_size , shuffle = False , collate_fn = asr_collate , num_workers = self . num_workers , pin_memory = True , ) def test_dataloader ( self ) -> DataLoader : return DataLoader ( self . test_dataset , batch_size = self . batch_size , shuffle = False , collate_fn = asr_collate , num_workers = self . num_workers , pin_memory = True , ) @property def steps_per_epoch ( self ) -> int : \"\"\"Number of steps for each training epoch. Used for learning rate scheduling. Returns: Number of steps \"\"\" return len ( self . train_dataset ) // self . batch_size get_dataset ( split ) Function to get the corresponding dataset to the specified split. This should be implemented by subclasses. PARAMETER DESCRIPTION split One of \"train\", \"valid\" or \"test\". TYPE: str RETURNS DESCRIPTION BaseSpeechDataset The corresponding dataset. Source code in thunder/data/datamodule.py 31 32 33 34 35 36 37 38 39 40 41 def get_dataset ( self , split : str ) -> BaseSpeechDataset : \"\"\"Function to get the corresponding dataset to the specified split. This should be implemented by subclasses. Args: split: One of \"train\", \"valid\" or \"test\". Returns: The corresponding dataset. \"\"\" raise NotImplementedError () steps_per_epoch () Number of steps for each training epoch. Used for learning rate scheduling. RETURNS DESCRIPTION int Number of steps Source code in thunder/data/datamodule.py 80 81 82 83 84 85 86 87 @property def steps_per_epoch ( self ) -> int : \"\"\"Number of steps for each training epoch. Used for learning rate scheduling. Returns: Number of steps \"\"\" return len ( self . train_dataset ) // self . batch_size ManifestDatamodule Bases: BaseDataModule Source code in thunder/data/datamodule.py 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 class ManifestDatamodule ( BaseDataModule ): def __init__ ( self , train_manifest : str , val_manifest : str , test_manifest : str , force_mono : bool = True , sample_rate : int = 16000 , batch_size : int = 10 , num_workers : int = 8 , ): \"\"\"Datamodule compatible with the NEMO manifest data format. Args: train_manifest: Training manifest file val_manifest: Validation manifest file test_manifest: Test manifest file force_mono: Check [`ManifestSpeechDataset`][thunder.data.dataset.ManifestSpeechDataset] sample_rate: Check [`ManifestSpeechDataset`][thunder.data.dataset.ManifestSpeechDataset] batch_size: Batch size used by dataloader num_workers: Number of workers used by dataloader \"\"\" super () . __init__ ( batch_size = batch_size , num_workers = num_workers , ) self . manifest_mapping = { \"train\" : train_manifest , \"valid\" : val_manifest , \"test\" : test_manifest , } self . force_mono = force_mono self . sample_rate = sample_rate def get_dataset ( self , split : str ) -> ManifestSpeechDataset : return ManifestSpeechDataset ( self . manifest_mapping [ split ], self . force_mono , self . sample_rate ) __init__ ( train_manifest , val_manifest , test_manifest , force_mono = True , sample_rate = 16000 , batch_size = 10 , num_workers = 8 ) Datamodule compatible with the NEMO manifest data format. PARAMETER DESCRIPTION train_manifest Training manifest file TYPE: str val_manifest Validation manifest file TYPE: str test_manifest Test manifest file TYPE: str force_mono Check ManifestSpeechDataset TYPE: bool DEFAULT: True sample_rate Check ManifestSpeechDataset TYPE: int DEFAULT: 16000 batch_size Batch size used by dataloader TYPE: int DEFAULT: 10 num_workers Number of workers used by dataloader TYPE: int DEFAULT: 8 Source code in thunder/data/datamodule.py 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 def __init__ ( self , train_manifest : str , val_manifest : str , test_manifest : str , force_mono : bool = True , sample_rate : int = 16000 , batch_size : int = 10 , num_workers : int = 8 , ): \"\"\"Datamodule compatible with the NEMO manifest data format. Args: train_manifest: Training manifest file val_manifest: Validation manifest file test_manifest: Test manifest file force_mono: Check [`ManifestSpeechDataset`][thunder.data.dataset.ManifestSpeechDataset] sample_rate: Check [`ManifestSpeechDataset`][thunder.data.dataset.ManifestSpeechDataset] batch_size: Batch size used by dataloader num_workers: Number of workers used by dataloader \"\"\" super () . __init__ ( batch_size = batch_size , num_workers = num_workers , ) self . manifest_mapping = { \"train\" : train_manifest , \"valid\" : val_manifest , \"test\" : test_manifest , } self . force_mono = force_mono self . sample_rate = sample_rate","title":"Datamodule"},{"location":"api/Data/datamodule/#thunder.data.datamodule.BaseDataModule","text":"Bases: LightningDataModule Source code in thunder/data/datamodule.py 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 class BaseDataModule ( LightningDataModule ): def __init__ ( self , batch_size : int = 10 , num_workers : int = 8 , ): super () . __init__ () self . batch_size = batch_size self . num_workers = num_workers def get_dataset ( self , split : str ) -> BaseSpeechDataset : \"\"\"Function to get the corresponding dataset to the specified split. This should be implemented by subclasses. Args: split: One of \"train\", \"valid\" or \"test\". Returns: The corresponding dataset. \"\"\" raise NotImplementedError () def setup ( self , stage : Optional [ str ] = None ): if stage in ( None , \"fit\" ): self . train_dataset = self . get_dataset ( split = \"train\" ) self . val_dataset = self . get_dataset ( split = \"valid\" ) if stage in ( None , \"test\" ): self . test_dataset = self . get_dataset ( split = \"test\" ) def train_dataloader ( self ) -> DataLoader : return DataLoader ( self . train_dataset , batch_size = self . batch_size , collate_fn = asr_collate , num_workers = self . num_workers , shuffle = True , pin_memory = True , ) def val_dataloader ( self ) -> DataLoader : return DataLoader ( self . val_dataset , batch_size = self . batch_size , shuffle = False , collate_fn = asr_collate , num_workers = self . num_workers , pin_memory = True , ) def test_dataloader ( self ) -> DataLoader : return DataLoader ( self . test_dataset , batch_size = self . batch_size , shuffle = False , collate_fn = asr_collate , num_workers = self . num_workers , pin_memory = True , ) @property def steps_per_epoch ( self ) -> int : \"\"\"Number of steps for each training epoch. Used for learning rate scheduling. Returns: Number of steps \"\"\" return len ( self . train_dataset ) // self . batch_size","title":"BaseDataModule"},{"location":"api/Data/datamodule/#thunder.data.datamodule.BaseDataModule.get_dataset","text":"Function to get the corresponding dataset to the specified split. This should be implemented by subclasses. PARAMETER DESCRIPTION split One of \"train\", \"valid\" or \"test\". TYPE: str RETURNS DESCRIPTION BaseSpeechDataset The corresponding dataset. Source code in thunder/data/datamodule.py 31 32 33 34 35 36 37 38 39 40 41 def get_dataset ( self , split : str ) -> BaseSpeechDataset : \"\"\"Function to get the corresponding dataset to the specified split. This should be implemented by subclasses. Args: split: One of \"train\", \"valid\" or \"test\". Returns: The corresponding dataset. \"\"\" raise NotImplementedError ()","title":"get_dataset()"},{"location":"api/Data/datamodule/#thunder.data.datamodule.BaseDataModule.steps_per_epoch","text":"Number of steps for each training epoch. Used for learning rate scheduling. RETURNS DESCRIPTION int Number of steps Source code in thunder/data/datamodule.py 80 81 82 83 84 85 86 87 @property def steps_per_epoch ( self ) -> int : \"\"\"Number of steps for each training epoch. Used for learning rate scheduling. Returns: Number of steps \"\"\" return len ( self . train_dataset ) // self . batch_size","title":"steps_per_epoch()"},{"location":"api/Data/datamodule/#thunder.data.datamodule.ManifestDatamodule","text":"Bases: BaseDataModule Source code in thunder/data/datamodule.py 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 class ManifestDatamodule ( BaseDataModule ): def __init__ ( self , train_manifest : str , val_manifest : str , test_manifest : str , force_mono : bool = True , sample_rate : int = 16000 , batch_size : int = 10 , num_workers : int = 8 , ): \"\"\"Datamodule compatible with the NEMO manifest data format. Args: train_manifest: Training manifest file val_manifest: Validation manifest file test_manifest: Test manifest file force_mono: Check [`ManifestSpeechDataset`][thunder.data.dataset.ManifestSpeechDataset] sample_rate: Check [`ManifestSpeechDataset`][thunder.data.dataset.ManifestSpeechDataset] batch_size: Batch size used by dataloader num_workers: Number of workers used by dataloader \"\"\" super () . __init__ ( batch_size = batch_size , num_workers = num_workers , ) self . manifest_mapping = { \"train\" : train_manifest , \"valid\" : val_manifest , \"test\" : test_manifest , } self . force_mono = force_mono self . sample_rate = sample_rate def get_dataset ( self , split : str ) -> ManifestSpeechDataset : return ManifestSpeechDataset ( self . manifest_mapping [ split ], self . force_mono , self . sample_rate )","title":"ManifestDatamodule"},{"location":"api/Data/datamodule/#thunder.data.datamodule.ManifestDatamodule.__init__","text":"Datamodule compatible with the NEMO manifest data format. PARAMETER DESCRIPTION train_manifest Training manifest file TYPE: str val_manifest Validation manifest file TYPE: str test_manifest Test manifest file TYPE: str force_mono Check ManifestSpeechDataset TYPE: bool DEFAULT: True sample_rate Check ManifestSpeechDataset TYPE: int DEFAULT: 16000 batch_size Batch size used by dataloader TYPE: int DEFAULT: 10 num_workers Number of workers used by dataloader TYPE: int DEFAULT: 8 Source code in thunder/data/datamodule.py 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 def __init__ ( self , train_manifest : str , val_manifest : str , test_manifest : str , force_mono : bool = True , sample_rate : int = 16000 , batch_size : int = 10 , num_workers : int = 8 , ): \"\"\"Datamodule compatible with the NEMO manifest data format. Args: train_manifest: Training manifest file val_manifest: Validation manifest file test_manifest: Test manifest file force_mono: Check [`ManifestSpeechDataset`][thunder.data.dataset.ManifestSpeechDataset] sample_rate: Check [`ManifestSpeechDataset`][thunder.data.dataset.ManifestSpeechDataset] batch_size: Batch size used by dataloader num_workers: Number of workers used by dataloader \"\"\" super () . __init__ ( batch_size = batch_size , num_workers = num_workers , ) self . manifest_mapping = { \"train\" : train_manifest , \"valid\" : val_manifest , \"test\" : test_manifest , } self . force_mono = force_mono self . sample_rate = sample_rate","title":"__init__()"},{"location":"api/Data/dataset/","text":"Speech recognition datasets AudioFileLoader Bases: nn . Module Source code in thunder/data/dataset.py 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 class AudioFileLoader ( nn . Module ): def __init__ ( self , force_mono : bool = True , sample_rate : int = 16000 ): \"\"\"Module containing the data loading and basic preprocessing. It's used internally by the datasets, but can be exported so that during inference time there's no code dependency. Args: force_mono: If true, convert all the loaded samples to mono. sample_rate: Sample rate used by the dataset. All of the samples that have different rate will be resampled. \"\"\" super () . __init__ () self . force_mono = force_mono self . sample_rate = sample_rate @torch . jit . export def open_audio ( self , item : str ) -> Tuple [ Tensor , int ]: \"\"\"Uses the data returned by get_item to open the audio Args: item: Data returned by get_item(index) Returns: Tuple containing the audio tensor with shape (channels, time), and the corresponding sample rate. \"\"\" return torchaudio . load ( item ) @torch . jit . export def preprocess_audio ( self , audio : Tensor , sample_rate : int ) -> Tensor : \"\"\"Apply some base transforms to the audio, that fix silent problems. It transforms all the audios to mono (depending on class creation parameter), remove the possible DC bias present and then resamples the audios to a common sample rate. Args: audio: Audio tensor sample_rate: Sample rate Returns: Audio tensor after the transforms. \"\"\" if self . force_mono and ( audio . shape [ 0 ] > 1 ): audio = audio . mean ( 0 , keepdim = True ) # Removing the dc component from the audio # It happens when a faulty capture device introduce # an offset into the recorded waveform, and this can # cause problems with later transforms. # https://en.wikipedia.org/wiki/DC_bias audio = audio - audio . mean ( 1 ) if self . sample_rate != sample_rate : audio = resample ( audio , orig_freq = int ( sample_rate ), new_freq = int ( self . sample_rate ) ) return audio def forward ( self , item : str ) -> Tensor : \"\"\"Opens audio item and do basic preprocessing Args: item: Path to the audio to be opened Returns: Audio tensor after preprocessing \"\"\" audio , sample_rate = self . open_audio ( item ) return self . preprocess_audio ( audio , sample_rate ) __init__ ( force_mono = True , sample_rate = 16000 ) Module containing the data loading and basic preprocessing. It's used internally by the datasets, but can be exported so that during inference time there's no code dependency. PARAMETER DESCRIPTION force_mono If true, convert all the loaded samples to mono. TYPE: bool DEFAULT: True sample_rate Sample rate used by the dataset. All of the samples that have different rate will be resampled. TYPE: int DEFAULT: 16000 Source code in thunder/data/dataset.py 24 25 26 27 28 29 30 31 32 33 34 35 def __init__ ( self , force_mono : bool = True , sample_rate : int = 16000 ): \"\"\"Module containing the data loading and basic preprocessing. It's used internally by the datasets, but can be exported so that during inference time there's no code dependency. Args: force_mono: If true, convert all the loaded samples to mono. sample_rate: Sample rate used by the dataset. All of the samples that have different rate will be resampled. \"\"\" super () . __init__ () self . force_mono = force_mono self . sample_rate = sample_rate forward ( item ) Opens audio item and do basic preprocessing PARAMETER DESCRIPTION item Path to the audio to be opened TYPE: str RETURNS DESCRIPTION Tensor Audio tensor after preprocessing Source code in thunder/data/dataset.py 79 80 81 82 83 84 85 86 87 88 89 def forward ( self , item : str ) -> Tensor : \"\"\"Opens audio item and do basic preprocessing Args: item: Path to the audio to be opened Returns: Audio tensor after preprocessing \"\"\" audio , sample_rate = self . open_audio ( item ) return self . preprocess_audio ( audio , sample_rate ) open_audio ( item ) Uses the data returned by get_item to open the audio PARAMETER DESCRIPTION item Data returned by get_item(index) TYPE: str RETURNS DESCRIPTION Tuple [ Tensor , int ] Tuple containing the audio tensor with shape (channels, time), and the corresponding sample rate. Source code in thunder/data/dataset.py 37 38 39 40 41 42 43 44 45 46 47 @torch . jit . export def open_audio ( self , item : str ) -> Tuple [ Tensor , int ]: \"\"\"Uses the data returned by get_item to open the audio Args: item: Data returned by get_item(index) Returns: Tuple containing the audio tensor with shape (channels, time), and the corresponding sample rate. \"\"\" return torchaudio . load ( item ) preprocess_audio ( audio , sample_rate ) Apply some base transforms to the audio, that fix silent problems. It transforms all the audios to mono (depending on class creation parameter), remove the possible DC bias present and then resamples the audios to a common sample rate. PARAMETER DESCRIPTION audio Audio tensor TYPE: Tensor sample_rate Sample rate TYPE: int RETURNS DESCRIPTION Tensor Audio tensor after the transforms. Source code in thunder/data/dataset.py 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 @torch . jit . export def preprocess_audio ( self , audio : Tensor , sample_rate : int ) -> Tensor : \"\"\"Apply some base transforms to the audio, that fix silent problems. It transforms all the audios to mono (depending on class creation parameter), remove the possible DC bias present and then resamples the audios to a common sample rate. Args: audio: Audio tensor sample_rate: Sample rate Returns: Audio tensor after the transforms. \"\"\" if self . force_mono and ( audio . shape [ 0 ] > 1 ): audio = audio . mean ( 0 , keepdim = True ) # Removing the dc component from the audio # It happens when a faulty capture device introduce # an offset into the recorded waveform, and this can # cause problems with later transforms. # https://en.wikipedia.org/wiki/DC_bias audio = audio - audio . mean ( 1 ) if self . sample_rate != sample_rate : audio = resample ( audio , orig_freq = int ( sample_rate ), new_freq = int ( self . sample_rate ) ) return audio BaseSpeechDataset Bases: Dataset Source code in thunder/data/dataset.py 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 class BaseSpeechDataset ( Dataset ): def __init__ ( self , items : Sequence , force_mono : bool = True , sample_rate : int = 16000 ): \"\"\"This is the base class that implements the minimal functionality to have a compatible speech dataset, in a way that can be easily customized by subclassing. Args: items: Source of items in the dataset, sorted by audio duration. This can be a list of files, a pandas dataframe or any other iterable structure where you record your data. force_mono: If true, convert all the loaded samples to mono. sample_rate: Sample rate used by the dataset. All of the samples that have different rate will be resampled. \"\"\" super () . __init__ () self . items = items self . loader = AudioFileLoader ( force_mono , sample_rate ) def __len__ ( self ): return len ( self . items ) def __getitem__ ( self , index : int ) -> Tuple [ Tensor , str ]: item = self . get_item ( index ) # Dealing with input audio , sr = self . open_audio ( item ) audio = self . preprocess_audio ( audio , sr ) # Dealing with output text = self . open_text ( item ) text = self . preprocess_text ( text ) return audio , text def all_outputs ( self ) -> List [ str ]: \"\"\"Return a list with just the outputs for the whole dataset. Useful when creating the initial vocab tokens, or to train a language model. Returns: All of the outputs of the dataset, with the corresponding preprocessing applied. \"\"\" outputs = [] for index in range ( len ( self )): item = self . get_item ( index ) text = self . open_text ( item ) text = self . preprocess_text ( text ) outputs . append ( text ) return outputs def get_item ( self , index : int ) -> Any : \"\"\"Get the item source specified by the index. Args: index: Indicates what item it needs to return information about. Returns: Whatever data necessary to open the audio and text corresponding to this index. \"\"\" return self . items [ index ] def open_audio ( self , item : Any ) -> Tuple [ Tensor , int ]: \"\"\"Uses the data returned by get_item to open the audio Args: item: Data returned by get_item(index) Returns: Tuple containing the audio tensor with shape (channels, time), and the corresponding sample rate. \"\"\" return self . loader . open_audio ( item ) def preprocess_audio ( self , audio : Tensor , sample_rate : int ) -> Tensor : \"\"\"Apply some base transforms to the audio, that fix silent problems. It transforms all the audios to mono (depending on class creation parameter), remove the possible DC bias present and then resamples the audios to a common sample rate. Args: audio: Audio tensor sample_rate: Sample rate Returns: Audio tensor after the transforms. \"\"\" return self . loader . preprocess_audio ( audio , sample_rate ) def open_text ( self , item : Any ) -> str : \"\"\"Opens the transcription based on the data returned by get_item(index) Args: item: The data returned by get_item. Returns: The transcription corresponding to the item. \"\"\" raise NotImplementedError () def preprocess_text ( self , text : str ) -> str : \"\"\"Add here preprocessing steps to remove some common problems in the text. Args: text: Label text Returns: Label text after processing \"\"\" return text __init__ ( items , force_mono = True , sample_rate = 16000 ) This is the base class that implements the minimal functionality to have a compatible speech dataset, in a way that can be easily customized by subclassing. PARAMETER DESCRIPTION items Source of items in the dataset, sorted by audio duration. This can be a list of files, a pandas dataframe or any other iterable structure where you record your data. TYPE: Sequence force_mono If true, convert all the loaded samples to mono. TYPE: bool DEFAULT: True sample_rate Sample rate used by the dataset. All of the samples that have different rate will be resampled. TYPE: int DEFAULT: 16000 Source code in thunder/data/dataset.py 93 94 95 96 97 98 99 100 101 102 103 104 105 106 def __init__ ( self , items : Sequence , force_mono : bool = True , sample_rate : int = 16000 ): \"\"\"This is the base class that implements the minimal functionality to have a compatible speech dataset, in a way that can be easily customized by subclassing. Args: items: Source of items in the dataset, sorted by audio duration. This can be a list of files, a pandas dataframe or any other iterable structure where you record your data. force_mono: If true, convert all the loaded samples to mono. sample_rate: Sample rate used by the dataset. All of the samples that have different rate will be resampled. \"\"\" super () . __init__ () self . items = items self . loader = AudioFileLoader ( force_mono , sample_rate ) all_outputs () Return a list with just the outputs for the whole dataset. Useful when creating the initial vocab tokens, or to train a language model. RETURNS DESCRIPTION List [ str ] All of the outputs of the dataset, with the corresponding preprocessing applied. Source code in thunder/data/dataset.py 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 def all_outputs ( self ) -> List [ str ]: \"\"\"Return a list with just the outputs for the whole dataset. Useful when creating the initial vocab tokens, or to train a language model. Returns: All of the outputs of the dataset, with the corresponding preprocessing applied. \"\"\" outputs = [] for index in range ( len ( self )): item = self . get_item ( index ) text = self . open_text ( item ) text = self . preprocess_text ( text ) outputs . append ( text ) return outputs get_item ( index ) Get the item source specified by the index. PARAMETER DESCRIPTION index Indicates what item it needs to return information about. TYPE: int RETURNS DESCRIPTION Any Whatever data necessary to open the audio and text corresponding to this index. Source code in thunder/data/dataset.py 138 139 140 141 142 143 144 145 146 147 def get_item ( self , index : int ) -> Any : \"\"\"Get the item source specified by the index. Args: index: Indicates what item it needs to return information about. Returns: Whatever data necessary to open the audio and text corresponding to this index. \"\"\" return self . items [ index ] open_audio ( item ) Uses the data returned by get_item to open the audio PARAMETER DESCRIPTION item Data returned by get_item(index) TYPE: Any RETURNS DESCRIPTION Tuple [ Tensor , int ] Tuple containing the audio tensor with shape (channels, time), and the corresponding sample rate. Source code in thunder/data/dataset.py 149 150 151 152 153 154 155 156 157 158 def open_audio ( self , item : Any ) -> Tuple [ Tensor , int ]: \"\"\"Uses the data returned by get_item to open the audio Args: item: Data returned by get_item(index) Returns: Tuple containing the audio tensor with shape (channels, time), and the corresponding sample rate. \"\"\" return self . loader . open_audio ( item ) open_text ( item ) Opens the transcription based on the data returned by get_item(index) PARAMETER DESCRIPTION item The data returned by get_item. TYPE: Any RETURNS DESCRIPTION str The transcription corresponding to the item. Source code in thunder/data/dataset.py 175 176 177 178 179 180 181 182 183 184 def open_text ( self , item : Any ) -> str : \"\"\"Opens the transcription based on the data returned by get_item(index) Args: item: The data returned by get_item. Returns: The transcription corresponding to the item. \"\"\" raise NotImplementedError () preprocess_audio ( audio , sample_rate ) Apply some base transforms to the audio, that fix silent problems. It transforms all the audios to mono (depending on class creation parameter), remove the possible DC bias present and then resamples the audios to a common sample rate. PARAMETER DESCRIPTION audio Audio tensor TYPE: Tensor sample_rate Sample rate TYPE: int RETURNS DESCRIPTION Tensor Audio tensor after the transforms. Source code in thunder/data/dataset.py 160 161 162 163 164 165 166 167 168 169 170 171 172 173 def preprocess_audio ( self , audio : Tensor , sample_rate : int ) -> Tensor : \"\"\"Apply some base transforms to the audio, that fix silent problems. It transforms all the audios to mono (depending on class creation parameter), remove the possible DC bias present and then resamples the audios to a common sample rate. Args: audio: Audio tensor sample_rate: Sample rate Returns: Audio tensor after the transforms. \"\"\" return self . loader . preprocess_audio ( audio , sample_rate ) preprocess_text ( text ) Add here preprocessing steps to remove some common problems in the text. PARAMETER DESCRIPTION text Label text TYPE: str RETURNS DESCRIPTION str Label text after processing Source code in thunder/data/dataset.py 186 187 188 189 190 191 192 193 194 195 def preprocess_text ( self , text : str ) -> str : \"\"\"Add here preprocessing steps to remove some common problems in the text. Args: text: Label text Returns: Label text after processing \"\"\" return text ManifestSpeechDataset Bases: BaseSpeechDataset Source code in thunder/data/dataset.py 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 class ManifestSpeechDataset ( BaseSpeechDataset ): def __init__ ( self , file : Union [ str , Path ], force_mono : bool , sample_rate : int ): \"\"\"Dataset that loads from nemo manifest files. Args: file: Nemo manifest file. force_mono: If true, convert all the loaded samples to mono. sample_rate: Sample rate used by the dataset. All of the samples that have different rate will be resampled. \"\"\" file = Path ( file ) # Reading from the manifest file items = [ json . loads ( line ) for line in file . read_text () . strip () . splitlines ()] super () . __init__ ( items , force_mono = force_mono , sample_rate = sample_rate ) def open_audio ( self , item : dict ) -> Tuple [ Tensor , int ]: # path = \"/home/scart/datasets_raw/\" + item[\"audio_filepath\"] path = item [ \"audio_filepath\" ] return self . loader . open_audio ( path ) def open_text ( self , item : dict ) -> str : return item [ \"text\" ] __init__ ( file , force_mono , sample_rate ) Dataset that loads from nemo manifest files. PARAMETER DESCRIPTION file Nemo manifest file. TYPE: Union [ str , Path ] force_mono If true, convert all the loaded samples to mono. TYPE: bool sample_rate Sample rate used by the dataset. All of the samples that have different rate will be resampled. TYPE: int Source code in thunder/data/dataset.py 199 200 201 202 203 204 205 206 207 208 209 210 def __init__ ( self , file : Union [ str , Path ], force_mono : bool , sample_rate : int ): \"\"\"Dataset that loads from nemo manifest files. Args: file: Nemo manifest file. force_mono: If true, convert all the loaded samples to mono. sample_rate: Sample rate used by the dataset. All of the samples that have different rate will be resampled. \"\"\" file = Path ( file ) # Reading from the manifest file items = [ json . loads ( line ) for line in file . read_text () . strip () . splitlines ()] super () . __init__ ( items , force_mono = force_mono , sample_rate = sample_rate )","title":"Dataset"},{"location":"api/Data/dataset/#thunder.data.dataset.AudioFileLoader","text":"Bases: nn . Module Source code in thunder/data/dataset.py 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 class AudioFileLoader ( nn . Module ): def __init__ ( self , force_mono : bool = True , sample_rate : int = 16000 ): \"\"\"Module containing the data loading and basic preprocessing. It's used internally by the datasets, but can be exported so that during inference time there's no code dependency. Args: force_mono: If true, convert all the loaded samples to mono. sample_rate: Sample rate used by the dataset. All of the samples that have different rate will be resampled. \"\"\" super () . __init__ () self . force_mono = force_mono self . sample_rate = sample_rate @torch . jit . export def open_audio ( self , item : str ) -> Tuple [ Tensor , int ]: \"\"\"Uses the data returned by get_item to open the audio Args: item: Data returned by get_item(index) Returns: Tuple containing the audio tensor with shape (channels, time), and the corresponding sample rate. \"\"\" return torchaudio . load ( item ) @torch . jit . export def preprocess_audio ( self , audio : Tensor , sample_rate : int ) -> Tensor : \"\"\"Apply some base transforms to the audio, that fix silent problems. It transforms all the audios to mono (depending on class creation parameter), remove the possible DC bias present and then resamples the audios to a common sample rate. Args: audio: Audio tensor sample_rate: Sample rate Returns: Audio tensor after the transforms. \"\"\" if self . force_mono and ( audio . shape [ 0 ] > 1 ): audio = audio . mean ( 0 , keepdim = True ) # Removing the dc component from the audio # It happens when a faulty capture device introduce # an offset into the recorded waveform, and this can # cause problems with later transforms. # https://en.wikipedia.org/wiki/DC_bias audio = audio - audio . mean ( 1 ) if self . sample_rate != sample_rate : audio = resample ( audio , orig_freq = int ( sample_rate ), new_freq = int ( self . sample_rate ) ) return audio def forward ( self , item : str ) -> Tensor : \"\"\"Opens audio item and do basic preprocessing Args: item: Path to the audio to be opened Returns: Audio tensor after preprocessing \"\"\" audio , sample_rate = self . open_audio ( item ) return self . preprocess_audio ( audio , sample_rate )","title":"AudioFileLoader"},{"location":"api/Data/dataset/#thunder.data.dataset.AudioFileLoader.__init__","text":"Module containing the data loading and basic preprocessing. It's used internally by the datasets, but can be exported so that during inference time there's no code dependency. PARAMETER DESCRIPTION force_mono If true, convert all the loaded samples to mono. TYPE: bool DEFAULT: True sample_rate Sample rate used by the dataset. All of the samples that have different rate will be resampled. TYPE: int DEFAULT: 16000 Source code in thunder/data/dataset.py 24 25 26 27 28 29 30 31 32 33 34 35 def __init__ ( self , force_mono : bool = True , sample_rate : int = 16000 ): \"\"\"Module containing the data loading and basic preprocessing. It's used internally by the datasets, but can be exported so that during inference time there's no code dependency. Args: force_mono: If true, convert all the loaded samples to mono. sample_rate: Sample rate used by the dataset. All of the samples that have different rate will be resampled. \"\"\" super () . __init__ () self . force_mono = force_mono self . sample_rate = sample_rate","title":"__init__()"},{"location":"api/Data/dataset/#thunder.data.dataset.AudioFileLoader.forward","text":"Opens audio item and do basic preprocessing PARAMETER DESCRIPTION item Path to the audio to be opened TYPE: str RETURNS DESCRIPTION Tensor Audio tensor after preprocessing Source code in thunder/data/dataset.py 79 80 81 82 83 84 85 86 87 88 89 def forward ( self , item : str ) -> Tensor : \"\"\"Opens audio item and do basic preprocessing Args: item: Path to the audio to be opened Returns: Audio tensor after preprocessing \"\"\" audio , sample_rate = self . open_audio ( item ) return self . preprocess_audio ( audio , sample_rate )","title":"forward()"},{"location":"api/Data/dataset/#thunder.data.dataset.AudioFileLoader.open_audio","text":"Uses the data returned by get_item to open the audio PARAMETER DESCRIPTION item Data returned by get_item(index) TYPE: str RETURNS DESCRIPTION Tuple [ Tensor , int ] Tuple containing the audio tensor with shape (channels, time), and the corresponding sample rate. Source code in thunder/data/dataset.py 37 38 39 40 41 42 43 44 45 46 47 @torch . jit . export def open_audio ( self , item : str ) -> Tuple [ Tensor , int ]: \"\"\"Uses the data returned by get_item to open the audio Args: item: Data returned by get_item(index) Returns: Tuple containing the audio tensor with shape (channels, time), and the corresponding sample rate. \"\"\" return torchaudio . load ( item )","title":"open_audio()"},{"location":"api/Data/dataset/#thunder.data.dataset.AudioFileLoader.preprocess_audio","text":"Apply some base transforms to the audio, that fix silent problems. It transforms all the audios to mono (depending on class creation parameter), remove the possible DC bias present and then resamples the audios to a common sample rate. PARAMETER DESCRIPTION audio Audio tensor TYPE: Tensor sample_rate Sample rate TYPE: int RETURNS DESCRIPTION Tensor Audio tensor after the transforms. Source code in thunder/data/dataset.py 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 @torch . jit . export def preprocess_audio ( self , audio : Tensor , sample_rate : int ) -> Tensor : \"\"\"Apply some base transforms to the audio, that fix silent problems. It transforms all the audios to mono (depending on class creation parameter), remove the possible DC bias present and then resamples the audios to a common sample rate. Args: audio: Audio tensor sample_rate: Sample rate Returns: Audio tensor after the transforms. \"\"\" if self . force_mono and ( audio . shape [ 0 ] > 1 ): audio = audio . mean ( 0 , keepdim = True ) # Removing the dc component from the audio # It happens when a faulty capture device introduce # an offset into the recorded waveform, and this can # cause problems with later transforms. # https://en.wikipedia.org/wiki/DC_bias audio = audio - audio . mean ( 1 ) if self . sample_rate != sample_rate : audio = resample ( audio , orig_freq = int ( sample_rate ), new_freq = int ( self . sample_rate ) ) return audio","title":"preprocess_audio()"},{"location":"api/Data/dataset/#thunder.data.dataset.BaseSpeechDataset","text":"Bases: Dataset Source code in thunder/data/dataset.py 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 class BaseSpeechDataset ( Dataset ): def __init__ ( self , items : Sequence , force_mono : bool = True , sample_rate : int = 16000 ): \"\"\"This is the base class that implements the minimal functionality to have a compatible speech dataset, in a way that can be easily customized by subclassing. Args: items: Source of items in the dataset, sorted by audio duration. This can be a list of files, a pandas dataframe or any other iterable structure where you record your data. force_mono: If true, convert all the loaded samples to mono. sample_rate: Sample rate used by the dataset. All of the samples that have different rate will be resampled. \"\"\" super () . __init__ () self . items = items self . loader = AudioFileLoader ( force_mono , sample_rate ) def __len__ ( self ): return len ( self . items ) def __getitem__ ( self , index : int ) -> Tuple [ Tensor , str ]: item = self . get_item ( index ) # Dealing with input audio , sr = self . open_audio ( item ) audio = self . preprocess_audio ( audio , sr ) # Dealing with output text = self . open_text ( item ) text = self . preprocess_text ( text ) return audio , text def all_outputs ( self ) -> List [ str ]: \"\"\"Return a list with just the outputs for the whole dataset. Useful when creating the initial vocab tokens, or to train a language model. Returns: All of the outputs of the dataset, with the corresponding preprocessing applied. \"\"\" outputs = [] for index in range ( len ( self )): item = self . get_item ( index ) text = self . open_text ( item ) text = self . preprocess_text ( text ) outputs . append ( text ) return outputs def get_item ( self , index : int ) -> Any : \"\"\"Get the item source specified by the index. Args: index: Indicates what item it needs to return information about. Returns: Whatever data necessary to open the audio and text corresponding to this index. \"\"\" return self . items [ index ] def open_audio ( self , item : Any ) -> Tuple [ Tensor , int ]: \"\"\"Uses the data returned by get_item to open the audio Args: item: Data returned by get_item(index) Returns: Tuple containing the audio tensor with shape (channels, time), and the corresponding sample rate. \"\"\" return self . loader . open_audio ( item ) def preprocess_audio ( self , audio : Tensor , sample_rate : int ) -> Tensor : \"\"\"Apply some base transforms to the audio, that fix silent problems. It transforms all the audios to mono (depending on class creation parameter), remove the possible DC bias present and then resamples the audios to a common sample rate. Args: audio: Audio tensor sample_rate: Sample rate Returns: Audio tensor after the transforms. \"\"\" return self . loader . preprocess_audio ( audio , sample_rate ) def open_text ( self , item : Any ) -> str : \"\"\"Opens the transcription based on the data returned by get_item(index) Args: item: The data returned by get_item. Returns: The transcription corresponding to the item. \"\"\" raise NotImplementedError () def preprocess_text ( self , text : str ) -> str : \"\"\"Add here preprocessing steps to remove some common problems in the text. Args: text: Label text Returns: Label text after processing \"\"\" return text","title":"BaseSpeechDataset"},{"location":"api/Data/dataset/#thunder.data.dataset.BaseSpeechDataset.__init__","text":"This is the base class that implements the minimal functionality to have a compatible speech dataset, in a way that can be easily customized by subclassing. PARAMETER DESCRIPTION items Source of items in the dataset, sorted by audio duration. This can be a list of files, a pandas dataframe or any other iterable structure where you record your data. TYPE: Sequence force_mono If true, convert all the loaded samples to mono. TYPE: bool DEFAULT: True sample_rate Sample rate used by the dataset. All of the samples that have different rate will be resampled. TYPE: int DEFAULT: 16000 Source code in thunder/data/dataset.py 93 94 95 96 97 98 99 100 101 102 103 104 105 106 def __init__ ( self , items : Sequence , force_mono : bool = True , sample_rate : int = 16000 ): \"\"\"This is the base class that implements the minimal functionality to have a compatible speech dataset, in a way that can be easily customized by subclassing. Args: items: Source of items in the dataset, sorted by audio duration. This can be a list of files, a pandas dataframe or any other iterable structure where you record your data. force_mono: If true, convert all the loaded samples to mono. sample_rate: Sample rate used by the dataset. All of the samples that have different rate will be resampled. \"\"\" super () . __init__ () self . items = items self . loader = AudioFileLoader ( force_mono , sample_rate )","title":"__init__()"},{"location":"api/Data/dataset/#thunder.data.dataset.BaseSpeechDataset.all_outputs","text":"Return a list with just the outputs for the whole dataset. Useful when creating the initial vocab tokens, or to train a language model. RETURNS DESCRIPTION List [ str ] All of the outputs of the dataset, with the corresponding preprocessing applied. Source code in thunder/data/dataset.py 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 def all_outputs ( self ) -> List [ str ]: \"\"\"Return a list with just the outputs for the whole dataset. Useful when creating the initial vocab tokens, or to train a language model. Returns: All of the outputs of the dataset, with the corresponding preprocessing applied. \"\"\" outputs = [] for index in range ( len ( self )): item = self . get_item ( index ) text = self . open_text ( item ) text = self . preprocess_text ( text ) outputs . append ( text ) return outputs","title":"all_outputs()"},{"location":"api/Data/dataset/#thunder.data.dataset.BaseSpeechDataset.get_item","text":"Get the item source specified by the index. PARAMETER DESCRIPTION index Indicates what item it needs to return information about. TYPE: int RETURNS DESCRIPTION Any Whatever data necessary to open the audio and text corresponding to this index. Source code in thunder/data/dataset.py 138 139 140 141 142 143 144 145 146 147 def get_item ( self , index : int ) -> Any : \"\"\"Get the item source specified by the index. Args: index: Indicates what item it needs to return information about. Returns: Whatever data necessary to open the audio and text corresponding to this index. \"\"\" return self . items [ index ]","title":"get_item()"},{"location":"api/Data/dataset/#thunder.data.dataset.BaseSpeechDataset.open_audio","text":"Uses the data returned by get_item to open the audio PARAMETER DESCRIPTION item Data returned by get_item(index) TYPE: Any RETURNS DESCRIPTION Tuple [ Tensor , int ] Tuple containing the audio tensor with shape (channels, time), and the corresponding sample rate. Source code in thunder/data/dataset.py 149 150 151 152 153 154 155 156 157 158 def open_audio ( self , item : Any ) -> Tuple [ Tensor , int ]: \"\"\"Uses the data returned by get_item to open the audio Args: item: Data returned by get_item(index) Returns: Tuple containing the audio tensor with shape (channels, time), and the corresponding sample rate. \"\"\" return self . loader . open_audio ( item )","title":"open_audio()"},{"location":"api/Data/dataset/#thunder.data.dataset.BaseSpeechDataset.open_text","text":"Opens the transcription based on the data returned by get_item(index) PARAMETER DESCRIPTION item The data returned by get_item. TYPE: Any RETURNS DESCRIPTION str The transcription corresponding to the item. Source code in thunder/data/dataset.py 175 176 177 178 179 180 181 182 183 184 def open_text ( self , item : Any ) -> str : \"\"\"Opens the transcription based on the data returned by get_item(index) Args: item: The data returned by get_item. Returns: The transcription corresponding to the item. \"\"\" raise NotImplementedError ()","title":"open_text()"},{"location":"api/Data/dataset/#thunder.data.dataset.BaseSpeechDataset.preprocess_audio","text":"Apply some base transforms to the audio, that fix silent problems. It transforms all the audios to mono (depending on class creation parameter), remove the possible DC bias present and then resamples the audios to a common sample rate. PARAMETER DESCRIPTION audio Audio tensor TYPE: Tensor sample_rate Sample rate TYPE: int RETURNS DESCRIPTION Tensor Audio tensor after the transforms. Source code in thunder/data/dataset.py 160 161 162 163 164 165 166 167 168 169 170 171 172 173 def preprocess_audio ( self , audio : Tensor , sample_rate : int ) -> Tensor : \"\"\"Apply some base transforms to the audio, that fix silent problems. It transforms all the audios to mono (depending on class creation parameter), remove the possible DC bias present and then resamples the audios to a common sample rate. Args: audio: Audio tensor sample_rate: Sample rate Returns: Audio tensor after the transforms. \"\"\" return self . loader . preprocess_audio ( audio , sample_rate )","title":"preprocess_audio()"},{"location":"api/Data/dataset/#thunder.data.dataset.BaseSpeechDataset.preprocess_text","text":"Add here preprocessing steps to remove some common problems in the text. PARAMETER DESCRIPTION text Label text TYPE: str RETURNS DESCRIPTION str Label text after processing Source code in thunder/data/dataset.py 186 187 188 189 190 191 192 193 194 195 def preprocess_text ( self , text : str ) -> str : \"\"\"Add here preprocessing steps to remove some common problems in the text. Args: text: Label text Returns: Label text after processing \"\"\" return text","title":"preprocess_text()"},{"location":"api/Data/dataset/#thunder.data.dataset.ManifestSpeechDataset","text":"Bases: BaseSpeechDataset Source code in thunder/data/dataset.py 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 class ManifestSpeechDataset ( BaseSpeechDataset ): def __init__ ( self , file : Union [ str , Path ], force_mono : bool , sample_rate : int ): \"\"\"Dataset that loads from nemo manifest files. Args: file: Nemo manifest file. force_mono: If true, convert all the loaded samples to mono. sample_rate: Sample rate used by the dataset. All of the samples that have different rate will be resampled. \"\"\" file = Path ( file ) # Reading from the manifest file items = [ json . loads ( line ) for line in file . read_text () . strip () . splitlines ()] super () . __init__ ( items , force_mono = force_mono , sample_rate = sample_rate ) def open_audio ( self , item : dict ) -> Tuple [ Tensor , int ]: # path = \"/home/scart/datasets_raw/\" + item[\"audio_filepath\"] path = item [ \"audio_filepath\" ] return self . loader . open_audio ( path ) def open_text ( self , item : dict ) -> str : return item [ \"text\" ]","title":"ManifestSpeechDataset"},{"location":"api/Data/dataset/#thunder.data.dataset.ManifestSpeechDataset.__init__","text":"Dataset that loads from nemo manifest files. PARAMETER DESCRIPTION file Nemo manifest file. TYPE: Union [ str , Path ] force_mono If true, convert all the loaded samples to mono. TYPE: bool sample_rate Sample rate used by the dataset. All of the samples that have different rate will be resampled. TYPE: int Source code in thunder/data/dataset.py 199 200 201 202 203 204 205 206 207 208 209 210 def __init__ ( self , file : Union [ str , Path ], force_mono : bool , sample_rate : int ): \"\"\"Dataset that loads from nemo manifest files. Args: file: Nemo manifest file. force_mono: If true, convert all the loaded samples to mono. sample_rate: Sample rate used by the dataset. All of the samples that have different rate will be resampled. \"\"\" file = Path ( file ) # Reading from the manifest file items = [ json . loads ( line ) for line in file . read_text () . strip () . splitlines ()] super () . __init__ ( items , force_mono = force_mono , sample_rate = sample_rate )","title":"__init__()"},{"location":"api/Huggingface/compatibility/","text":"Helper functions to load huggingface speech recognition models. load_huggingface_checkpoint ( model_name , ** model_kwargs ) Load huggingface model and convert to thunder BaseCTCModule PARAMETER DESCRIPTION model_name huggingface identifier of the model, like \"facebook/wav2vec2-large-960h\" TYPE: str model_kwargs extra keyword arguments to be passed to AutoModelForCTC.from_pretrained RETURNS DESCRIPTION BaseCTCModule Thunder module containing the huggingface model. Source code in thunder/huggingface/compatibility.py 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 def load_huggingface_checkpoint ( model_name : str , ** model_kwargs : Dict [ str , Any ] ) -> BaseCTCModule : \"\"\"Load huggingface model and convert to thunder [`BaseCTCModule`][thunder.module.BaseCTCModule] Args: model_name: huggingface identifier of the model, like \"facebook/wav2vec2-large-960h\" model_kwargs: extra keyword arguments to be passed to `AutoModelForCTC.from_pretrained` Returns: Thunder module containing the huggingface model. \"\"\" model = AutoModelForCTC . from_pretrained ( model_name , ** model_kwargs ) processor = Wav2Vec2Processor . from_pretrained ( model_name ) vocab = list ( processor . tokenizer . get_vocab () . keys ()) text_transform = BatchTextTransformer ( tokens = vocab , blank_token = processor . tokenizer . pad_token , pad_token = processor . tokenizer . pad_token , unknown_token = processor . tokenizer . unk_token , start_token = processor . tokenizer . bos_token , end_token = processor . tokenizer . eos_token , ) decoder = linear_decoder ( model . base_model . config . hidden_size , len ( vocab ), decoder_dropout = 0.0 ) if hasattr ( model , \"lm_head\" ): decoder [ 1 ] . load_state_dict ( model . lm_head . state_dict ()) module = BaseCTCModule ( encoder = _HuggingFaceEncoderAdapt ( model . base_model , mask_input = processor . feature_extractor . return_attention_mask , ), decoder = decoder , text_transform = text_transform , audio_transform = Wav2Vec2Preprocess ( mask_input = processor . feature_extractor . return_attention_mask , ), encoder_final_dimension = model . base_model . config . hidden_size , ) return module . eval () prepare_scriptable_wav2vec ( module , quantized = False ) Converts thunder module containing a wav2vec2 model to be scriptable. PARAMETER DESCRIPTION module Module containing wav2vec2 TYPE: BaseCTCModule quantized If true, also performs quantization of the model TYPE: bool DEFAULT: False RETURNS DESCRIPTION BaseCTCModule Modified module ready to call torch.jit.script(module) or module.to_torchscript() Source code in thunder/huggingface/compatibility.py 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 def prepare_scriptable_wav2vec ( module : BaseCTCModule , quantized : bool = False ) -> BaseCTCModule : \"\"\"Converts thunder module containing a wav2vec2 model to be scriptable. Args: module: Module containing wav2vec2 quantized: If true, also performs quantization of the model Returns: Modified module ready to call torch.jit.script(module) or module.to_torchscript() \"\"\" imported = import_huggingface_model ( module . encoder . original_encoder ) if quantized : imported . encoder . transformer . pos_conv_embed . __prepare_scriptable__ () imported = torch . quantization . quantize_dynamic ( imported , qconfig_spec = { torch . nn . Linear }, dtype = torch . qint8 ) module . encoder = imported return module","title":"Compatibility"},{"location":"api/Huggingface/compatibility/#thunder.huggingface.compatibility.load_huggingface_checkpoint","text":"Load huggingface model and convert to thunder BaseCTCModule PARAMETER DESCRIPTION model_name huggingface identifier of the model, like \"facebook/wav2vec2-large-960h\" TYPE: str model_kwargs extra keyword arguments to be passed to AutoModelForCTC.from_pretrained RETURNS DESCRIPTION BaseCTCModule Thunder module containing the huggingface model. Source code in thunder/huggingface/compatibility.py 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 def load_huggingface_checkpoint ( model_name : str , ** model_kwargs : Dict [ str , Any ] ) -> BaseCTCModule : \"\"\"Load huggingface model and convert to thunder [`BaseCTCModule`][thunder.module.BaseCTCModule] Args: model_name: huggingface identifier of the model, like \"facebook/wav2vec2-large-960h\" model_kwargs: extra keyword arguments to be passed to `AutoModelForCTC.from_pretrained` Returns: Thunder module containing the huggingface model. \"\"\" model = AutoModelForCTC . from_pretrained ( model_name , ** model_kwargs ) processor = Wav2Vec2Processor . from_pretrained ( model_name ) vocab = list ( processor . tokenizer . get_vocab () . keys ()) text_transform = BatchTextTransformer ( tokens = vocab , blank_token = processor . tokenizer . pad_token , pad_token = processor . tokenizer . pad_token , unknown_token = processor . tokenizer . unk_token , start_token = processor . tokenizer . bos_token , end_token = processor . tokenizer . eos_token , ) decoder = linear_decoder ( model . base_model . config . hidden_size , len ( vocab ), decoder_dropout = 0.0 ) if hasattr ( model , \"lm_head\" ): decoder [ 1 ] . load_state_dict ( model . lm_head . state_dict ()) module = BaseCTCModule ( encoder = _HuggingFaceEncoderAdapt ( model . base_model , mask_input = processor . feature_extractor . return_attention_mask , ), decoder = decoder , text_transform = text_transform , audio_transform = Wav2Vec2Preprocess ( mask_input = processor . feature_extractor . return_attention_mask , ), encoder_final_dimension = model . base_model . config . hidden_size , ) return module . eval ()","title":"load_huggingface_checkpoint()"},{"location":"api/Huggingface/compatibility/#thunder.huggingface.compatibility.prepare_scriptable_wav2vec","text":"Converts thunder module containing a wav2vec2 model to be scriptable. PARAMETER DESCRIPTION module Module containing wav2vec2 TYPE: BaseCTCModule quantized If true, also performs quantization of the model TYPE: bool DEFAULT: False RETURNS DESCRIPTION BaseCTCModule Modified module ready to call torch.jit.script(module) or module.to_torchscript() Source code in thunder/huggingface/compatibility.py 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 def prepare_scriptable_wav2vec ( module : BaseCTCModule , quantized : bool = False ) -> BaseCTCModule : \"\"\"Converts thunder module containing a wav2vec2 model to be scriptable. Args: module: Module containing wav2vec2 quantized: If true, also performs quantization of the model Returns: Modified module ready to call torch.jit.script(module) or module.to_torchscript() \"\"\" imported = import_huggingface_model ( module . encoder . original_encoder ) if quantized : imported . encoder . transformer . pos_conv_embed . __prepare_scriptable__ () imported = torch . quantization . quantize_dynamic ( imported , qconfig_spec = { torch . nn . Linear }, dtype = torch . qint8 ) module . encoder = imported return module","title":"prepare_scriptable_wav2vec()"},{"location":"api/Huggingface/transform/","text":"Implementation of data preprocessing transform compatible with the huggingface wav2vec2 one Wav2Vec2Preprocess Bases: nn . Module Source code in thunder/huggingface/transform.py 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 class Wav2Vec2Preprocess ( nn . Module ): def __init__ ( self , div_guard : float = 1e-7 , mask_input : bool = False , ): \"\"\"Wav2Vec model preprocessing. It consists of normalizing the audio and optional mask. Args: div_guard: Guard value to prevent division by zero. \"\"\" super () . __init__ () self . div_guard = div_guard self . mask_input = mask_input def forward ( self , audio : torch . Tensor , audio_lengths : torch . Tensor ) -> Tuple [ torch . Tensor , Optional [ torch . Tensor ]]: \"\"\"Applies the normalization Args: audio: Audio tensor of shape [batch_size, time] Returns: Normalized audio tensor with same shape as input. Optionally the valid mask \"\"\" attention_mask : Optional [ torch . Tensor ] = None if self . mask_input : attention_mask = lengths_to_mask ( audio_lengths , max_length = audio . size ( - 1 ) ) . int () return ( normalize_tensor ( audio , attention_mask , div_guard = self . div_guard ), audio_lengths , ) __init__ ( div_guard = 1e-07 , mask_input = False ) Wav2Vec model preprocessing. It consists of normalizing the audio and optional mask. PARAMETER DESCRIPTION div_guard Guard value to prevent division by zero. TYPE: float DEFAULT: 1e-07 Source code in thunder/huggingface/transform.py 19 20 21 22 23 24 25 26 27 28 29 30 31 def __init__ ( self , div_guard : float = 1e-7 , mask_input : bool = False , ): \"\"\"Wav2Vec model preprocessing. It consists of normalizing the audio and optional mask. Args: div_guard: Guard value to prevent division by zero. \"\"\" super () . __init__ () self . div_guard = div_guard self . mask_input = mask_input forward ( audio , audio_lengths ) Applies the normalization PARAMETER DESCRIPTION audio Audio tensor of shape [batch_size, time] TYPE: torch . Tensor RETURNS DESCRIPTION Tuple [ torch . Tensor , Optional [ torch . Tensor ]] Normalized audio tensor with same shape as input. Optionally the valid mask Source code in thunder/huggingface/transform.py 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 def forward ( self , audio : torch . Tensor , audio_lengths : torch . Tensor ) -> Tuple [ torch . Tensor , Optional [ torch . Tensor ]]: \"\"\"Applies the normalization Args: audio: Audio tensor of shape [batch_size, time] Returns: Normalized audio tensor with same shape as input. Optionally the valid mask \"\"\" attention_mask : Optional [ torch . Tensor ] = None if self . mask_input : attention_mask = lengths_to_mask ( audio_lengths , max_length = audio . size ( - 1 ) ) . int () return ( normalize_tensor ( audio , attention_mask , div_guard = self . div_guard ), audio_lengths , )","title":"Transform"},{"location":"api/Huggingface/transform/#thunder.huggingface.transform.Wav2Vec2Preprocess","text":"Bases: nn . Module Source code in thunder/huggingface/transform.py 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 class Wav2Vec2Preprocess ( nn . Module ): def __init__ ( self , div_guard : float = 1e-7 , mask_input : bool = False , ): \"\"\"Wav2Vec model preprocessing. It consists of normalizing the audio and optional mask. Args: div_guard: Guard value to prevent division by zero. \"\"\" super () . __init__ () self . div_guard = div_guard self . mask_input = mask_input def forward ( self , audio : torch . Tensor , audio_lengths : torch . Tensor ) -> Tuple [ torch . Tensor , Optional [ torch . Tensor ]]: \"\"\"Applies the normalization Args: audio: Audio tensor of shape [batch_size, time] Returns: Normalized audio tensor with same shape as input. Optionally the valid mask \"\"\" attention_mask : Optional [ torch . Tensor ] = None if self . mask_input : attention_mask = lengths_to_mask ( audio_lengths , max_length = audio . size ( - 1 ) ) . int () return ( normalize_tensor ( audio , attention_mask , div_guard = self . div_guard ), audio_lengths , )","title":"Wav2Vec2Preprocess"},{"location":"api/Huggingface/transform/#thunder.huggingface.transform.Wav2Vec2Preprocess.__init__","text":"Wav2Vec model preprocessing. It consists of normalizing the audio and optional mask. PARAMETER DESCRIPTION div_guard Guard value to prevent division by zero. TYPE: float DEFAULT: 1e-07 Source code in thunder/huggingface/transform.py 19 20 21 22 23 24 25 26 27 28 29 30 31 def __init__ ( self , div_guard : float = 1e-7 , mask_input : bool = False , ): \"\"\"Wav2Vec model preprocessing. It consists of normalizing the audio and optional mask. Args: div_guard: Guard value to prevent division by zero. \"\"\" super () . __init__ () self . div_guard = div_guard self . mask_input = mask_input","title":"__init__()"},{"location":"api/Huggingface/transform/#thunder.huggingface.transform.Wav2Vec2Preprocess.forward","text":"Applies the normalization PARAMETER DESCRIPTION audio Audio tensor of shape [batch_size, time] TYPE: torch . Tensor RETURNS DESCRIPTION Tuple [ torch . Tensor , Optional [ torch . Tensor ]] Normalized audio tensor with same shape as input. Optionally the valid mask Source code in thunder/huggingface/transform.py 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 def forward ( self , audio : torch . Tensor , audio_lengths : torch . Tensor ) -> Tuple [ torch . Tensor , Optional [ torch . Tensor ]]: \"\"\"Applies the normalization Args: audio: Audio tensor of shape [batch_size, time] Returns: Normalized audio tensor with same shape as input. Optionally the valid mask \"\"\" attention_mask : Optional [ torch . Tensor ] = None if self . mask_input : attention_mask = lengths_to_mask ( audio_lengths , max_length = audio . size ( - 1 ) ) . int () return ( normalize_tensor ( audio , attention_mask , div_guard = self . div_guard ), audio_lengths , )","title":"forward()"},{"location":"api/Quartznet/blocks/","text":"Basic building blocks to create the Quartznet model InitMode Bases: str , Enum Note Possible values are xavier_uniform , xavier_normal , kaiming_uniform and kaiming_normal Weight init methods. Used by init_weights . Source code in thunder/quartznet/blocks.py 46 47 48 49 50 51 52 53 54 55 56 class InitMode ( str , Enum ): \"\"\"Weight init methods. Used by [`init_weights`][thunder.quartznet.blocks.init_weights]. Note: Possible values are `xavier_uniform`,`xavier_normal`,`kaiming_uniform` and `kaiming_normal` \"\"\" xavier_uniform = \"xavier_uniform\" xavier_normal = \"xavier_normal\" kaiming_uniform = \"kaiming_uniform\" kaiming_normal = \"kaiming_normal\" MaskedConv1d Bases: nn . Module Source code in thunder/quartznet/blocks.py 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 class MaskedConv1d ( nn . Module ): __constants__ = [ \"use_mask\" , \"padding\" , \"dilation\" , \"kernel_size\" , \"stride\" ] def __init__ ( self , in_channels : int , out_channels : int , kernel_size : _size_1_t , stride : _size_1_t = 1 , padding : _size_1_t = 0 , dilation : _size_1_t = 1 , groups : int = 1 , bias : bool = False , use_mask : bool = True , ): \"\"\"Masked Convolution. This module correspond to a 1d convolution with input masking. Arguments to create are the same as nn.Conv1d, but with the addition of use_mask for special behaviour. Args: in_channels: Same as nn.Conv1d out_channels: Same as nn.Conv1d kernel_size: Same as nn.Conv1d stride: Same as nn.Conv1d padding: Same as nn.Conv1d dilation: Same as nn.Conv1d groups: Same as nn.Conv1d bias: Same as nn.Conv1d use_mask: Controls the masking of input before the convolution during the forward. \"\"\" super () . __init__ () self . use_mask = use_mask self . conv = nn . Conv1d ( in_channels , out_channels , kernel_size , stride = stride , padding = padding , dilation = dilation , groups = groups , bias = bias , ) self . padding = self . conv . padding [ 0 ] self . dilation = self . conv . dilation [ 0 ] self . kernel_size = self . conv . kernel_size [ 0 ] self . stride = self . conv . stride [ 0 ] def get_seq_len ( self , lengths : torch . Tensor ) -> torch . Tensor : \"\"\"Get the lengths of the inputs after the convolution operation is applied. Args: lengths: Original lengths of the inputs Returns: Resulting lengths after the convolution \"\"\" return ( torch . div ( lengths + 2 * self . padding - self . dilation * ( self . kernel_size - 1 ) - 1 , self . stride , rounding_mode = \"floor\" , ) + 1 ) def mask_fill ( self , x : torch . Tensor , lengths : torch . Tensor ) -> torch . Tensor : \"\"\"Mask the input based on it's respective lengths. Args: x: Signal to be processed, of shape (batch, features, time) lengths: Lenghts of each element in the batch of x, with shape (batch) Returns: The masked signal \"\"\" mask = lengths_to_mask ( lengths , x . shape [ - 1 ]) return x . masked_fill ( ~ mask . unsqueeze ( 1 ), 0 ) def forward ( self , x : torch . Tensor , lengths : torch . Tensor ) -> Tuple [ torch . Tensor , torch . Tensor ]: \"\"\"Forward method Args: x: Signal to be processed, of shape (batch, features, time) lengths: Lenghts of each element in the batch of x, with shape (batch) Returns: Both the signal processed by the convolution and the resulting lengths \"\"\" if self . use_mask : x = self . mask_fill ( x , lengths ) out = self . conv ( x ) return out , self . get_seq_len ( lengths ) __init__ ( in_channels , out_channels , kernel_size , stride = 1 , padding = 0 , dilation = 1 , groups = 1 , bias = False , use_mask = True ) Masked Convolution. This module correspond to a 1d convolution with input masking. Arguments to create are the same as nn.Conv1d, but with the addition of use_mask for special behaviour. PARAMETER DESCRIPTION in_channels Same as nn.Conv1d TYPE: int out_channels Same as nn.Conv1d TYPE: int kernel_size Same as nn.Conv1d TYPE: _size_1_t stride Same as nn.Conv1d TYPE: _size_1_t DEFAULT: 1 padding Same as nn.Conv1d TYPE: _size_1_t DEFAULT: 0 dilation Same as nn.Conv1d TYPE: _size_1_t DEFAULT: 1 groups Same as nn.Conv1d TYPE: int DEFAULT: 1 bias Same as nn.Conv1d TYPE: bool DEFAULT: False use_mask Controls the masking of input before the convolution during the forward. TYPE: bool DEFAULT: True Source code in thunder/quartznet/blocks.py 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 def __init__ ( self , in_channels : int , out_channels : int , kernel_size : _size_1_t , stride : _size_1_t = 1 , padding : _size_1_t = 0 , dilation : _size_1_t = 1 , groups : int = 1 , bias : bool = False , use_mask : bool = True , ): \"\"\"Masked Convolution. This module correspond to a 1d convolution with input masking. Arguments to create are the same as nn.Conv1d, but with the addition of use_mask for special behaviour. Args: in_channels: Same as nn.Conv1d out_channels: Same as nn.Conv1d kernel_size: Same as nn.Conv1d stride: Same as nn.Conv1d padding: Same as nn.Conv1d dilation: Same as nn.Conv1d groups: Same as nn.Conv1d bias: Same as nn.Conv1d use_mask: Controls the masking of input before the convolution during the forward. \"\"\" super () . __init__ () self . use_mask = use_mask self . conv = nn . Conv1d ( in_channels , out_channels , kernel_size , stride = stride , padding = padding , dilation = dilation , groups = groups , bias = bias , ) self . padding = self . conv . padding [ 0 ] self . dilation = self . conv . dilation [ 0 ] self . kernel_size = self . conv . kernel_size [ 0 ] self . stride = self . conv . stride [ 0 ] forward ( x , lengths ) Forward method PARAMETER DESCRIPTION x Signal to be processed, of shape (batch, features, time) TYPE: torch . Tensor lengths Lenghts of each element in the batch of x, with shape (batch) TYPE: torch . Tensor RETURNS DESCRIPTION Tuple [ torch . Tensor , torch . Tensor ] Both the signal processed by the convolution and the resulting lengths Source code in thunder/quartznet/blocks.py 169 170 171 172 173 174 175 176 177 178 179 180 181 182 def forward ( self , x : torch . Tensor , lengths : torch . Tensor ) -> Tuple [ torch . Tensor , torch . Tensor ]: \"\"\"Forward method Args: x: Signal to be processed, of shape (batch, features, time) lengths: Lenghts of each element in the batch of x, with shape (batch) Returns: Both the signal processed by the convolution and the resulting lengths \"\"\" if self . use_mask : x = self . mask_fill ( x , lengths ) out = self . conv ( x ) return out , self . get_seq_len ( lengths ) get_seq_len ( lengths ) Get the lengths of the inputs after the convolution operation is applied. PARAMETER DESCRIPTION lengths Original lengths of the inputs TYPE: torch . Tensor RETURNS DESCRIPTION torch . Tensor Resulting lengths after the convolution Source code in thunder/quartznet/blocks.py 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 def get_seq_len ( self , lengths : torch . Tensor ) -> torch . Tensor : \"\"\"Get the lengths of the inputs after the convolution operation is applied. Args: lengths: Original lengths of the inputs Returns: Resulting lengths after the convolution \"\"\" return ( torch . div ( lengths + 2 * self . padding - self . dilation * ( self . kernel_size - 1 ) - 1 , self . stride , rounding_mode = \"floor\" , ) + 1 ) mask_fill ( x , lengths ) Mask the input based on it's respective lengths. PARAMETER DESCRIPTION x Signal to be processed, of shape (batch, features, time) TYPE: torch . Tensor lengths Lenghts of each element in the batch of x, with shape (batch) TYPE: torch . Tensor RETURNS DESCRIPTION torch . Tensor The masked signal Source code in thunder/quartznet/blocks.py 158 159 160 161 162 163 164 165 166 167 def mask_fill ( self , x : torch . Tensor , lengths : torch . Tensor ) -> torch . Tensor : \"\"\"Mask the input based on it's respective lengths. Args: x: Signal to be processed, of shape (batch, features, time) lengths: Lenghts of each element in the batch of x, with shape (batch) Returns: The masked signal \"\"\" mask = lengths_to_mask ( lengths , x . shape [ - 1 ]) return x . masked_fill ( ~ mask . unsqueeze ( 1 ), 0 ) QuartznetBlock Bases: nn . Module Source code in thunder/quartznet/blocks.py 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 class QuartznetBlock ( nn . Module ): def __init__ ( self , in_channels : int , out_channels : int , repeat : int = 5 , kernel_size : _size_1_t = ( 11 ,), stride : _size_1_t = ( 1 ,), dilation : _size_1_t = ( 1 ,), dropout : float = 0.0 , residual : bool = True , separable : bool = False , ): \"\"\"Quartznet block. This is a refactoring of the Jasperblock present on the NeMo toolkit, but simplified to only support the new quartznet model. Biggest change is that dense residual used on Jasper is not supported here, and masked convolutions were also removed. Args: in_channels: Number of input channels out_channels: Number of output channels repeat: Repetitions inside block. kernel_size: Kernel size. stride: Stride of each repetition. dilation: Dilation of each repetition. dropout: Dropout used before each activation. residual: Controls the use of residual connection. separable: Controls the use of separable convolutions. \"\"\" super () . __init__ () padding_val = get_same_padding ( kernel_size [ 0 ], stride [ 0 ], dilation [ 0 ]) inplanes_loop = in_channels conv = [] for _ in range ( repeat - 1 ): conv . extend ( _get_conv_bn_layer ( inplanes_loop , out_channels , kernel_size = kernel_size , stride = stride , dilation = dilation , padding = padding_val , separable = separable , bias = False , ) ) conv . extend ( _get_act_dropout_layer ( drop_prob = dropout )) inplanes_loop = out_channels conv . extend ( _get_conv_bn_layer ( inplanes_loop , out_channels , kernel_size = kernel_size , stride = stride , dilation = dilation , padding = padding_val , separable = separable , bias = False , ) ) self . mconv = MultiSequential ( * conv ) if residual : stride_residual = stride if stride [ 0 ] == 1 else stride [ 0 ] ** repeat self . res = MultiSequential ( * _get_conv_bn_layer ( in_channels , out_channels , kernel_size = 1 , stride = stride_residual , bias = False , ) ) else : self . res = None self . mout = MultiSequential ( * _get_act_dropout_layer ( drop_prob = dropout )) def forward ( self , x : torch . Tensor , lengths : torch . Tensor ) -> Tuple [ torch . Tensor , torch . Tensor ]: \"\"\" Args: x: Tensor of shape (batch, features, time) where #features == inplanes Returns: Result of applying the block on the input, and corresponding output lengths \"\"\" # compute forward convolutions out , lengths_out = self . mconv ( x , lengths ) # compute the residuals if self . res is not None : res_out , _ = self . res ( x , lengths ) out = out + res_out # compute the output out , lengths_out = self . mout ( out , lengths_out ) return out , lengths_out __init__ ( in_channels , out_channels , repeat = 5 , kernel_size = ( 11 ,), stride = ( 1 ,), dilation = ( 1 ,), dropout = 0.0 , residual = True , separable = False ) Quartznet block. This is a refactoring of the Jasperblock present on the NeMo toolkit, but simplified to only support the new quartznet model. Biggest change is that dense residual used on Jasper is not supported here, and masked convolutions were also removed. PARAMETER DESCRIPTION in_channels Number of input channels TYPE: int out_channels Number of output channels TYPE: int repeat Repetitions inside block. TYPE: int DEFAULT: 5 kernel_size Kernel size. TYPE: _size_1_t DEFAULT: (11,) stride Stride of each repetition. TYPE: _size_1_t DEFAULT: (1,) dilation Dilation of each repetition. TYPE: _size_1_t DEFAULT: (1,) dropout Dropout used before each activation. TYPE: float DEFAULT: 0.0 residual Controls the use of residual connection. TYPE: bool DEFAULT: True separable Controls the use of separable convolutions. TYPE: bool DEFAULT: False Source code in thunder/quartznet/blocks.py 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 def __init__ ( self , in_channels : int , out_channels : int , repeat : int = 5 , kernel_size : _size_1_t = ( 11 ,), stride : _size_1_t = ( 1 ,), dilation : _size_1_t = ( 1 ,), dropout : float = 0.0 , residual : bool = True , separable : bool = False , ): \"\"\"Quartznet block. This is a refactoring of the Jasperblock present on the NeMo toolkit, but simplified to only support the new quartznet model. Biggest change is that dense residual used on Jasper is not supported here, and masked convolutions were also removed. Args: in_channels: Number of input channels out_channels: Number of output channels repeat: Repetitions inside block. kernel_size: Kernel size. stride: Stride of each repetition. dilation: Dilation of each repetition. dropout: Dropout used before each activation. residual: Controls the use of residual connection. separable: Controls the use of separable convolutions. \"\"\" super () . __init__ () padding_val = get_same_padding ( kernel_size [ 0 ], stride [ 0 ], dilation [ 0 ]) inplanes_loop = in_channels conv = [] for _ in range ( repeat - 1 ): conv . extend ( _get_conv_bn_layer ( inplanes_loop , out_channels , kernel_size = kernel_size , stride = stride , dilation = dilation , padding = padding_val , separable = separable , bias = False , ) ) conv . extend ( _get_act_dropout_layer ( drop_prob = dropout )) inplanes_loop = out_channels conv . extend ( _get_conv_bn_layer ( inplanes_loop , out_channels , kernel_size = kernel_size , stride = stride , dilation = dilation , padding = padding_val , separable = separable , bias = False , ) ) self . mconv = MultiSequential ( * conv ) if residual : stride_residual = stride if stride [ 0 ] == 1 else stride [ 0 ] ** repeat self . res = MultiSequential ( * _get_conv_bn_layer ( in_channels , out_channels , kernel_size = 1 , stride = stride_residual , bias = False , ) ) else : self . res = None self . mout = MultiSequential ( * _get_act_dropout_layer ( drop_prob = dropout )) forward ( x , lengths ) PARAMETER DESCRIPTION x Tensor of shape (batch, features, time) where #features == inplanes TYPE: torch . Tensor RETURNS DESCRIPTION Tuple [ torch . Tensor , torch . Tensor ] Result of applying the block on the input, and corresponding output lengths Source code in thunder/quartznet/blocks.py 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 def forward ( self , x : torch . Tensor , lengths : torch . Tensor ) -> Tuple [ torch . Tensor , torch . Tensor ]: \"\"\" Args: x: Tensor of shape (batch, features, time) where #features == inplanes Returns: Result of applying the block on the input, and corresponding output lengths \"\"\" # compute forward convolutions out , lengths_out = self . mconv ( x , lengths ) # compute the residuals if self . res is not None : res_out , _ = self . res ( x , lengths ) out = out + res_out # compute the output out , lengths_out = self . mout ( out , lengths_out ) return out , lengths_out QuartznetEncoder ( feat_in = 64 , filters = [ 256 , 256 , 512 , 512 , 512 ], kernel_sizes = [ 33 , 39 , 51 , 63 , 75 ], repeat_blocks = 1 ) Basic Quartznet encoder setup. Can be used to build either Quartznet5x5 (repeat_blocks=1) or Quartznet15x5 (repeat_blocks=3) PARAMETER DESCRIPTION feat_in Number of input features to the model. TYPE: int DEFAULT: 64 filters List of filter sizes used to create the encoder blocks. TYPE: List [ int ] DEFAULT: [256, 256, 512, 512, 512] kernel_sizes List of kernel sizes corresponding to each filter size. TYPE: List [ int ] DEFAULT: [33, 39, 51, 63, 75] repeat_blocks Number of repetitions of each block. TYPE: int DEFAULT: 1 RETURNS DESCRIPTION nn . Module Pytorch model corresponding to the encoder. Source code in thunder/quartznet/blocks.py 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 def QuartznetEncoder ( feat_in : int = 64 , filters : List [ int ] = [ 256 , 256 , 512 , 512 , 512 ], kernel_sizes : List [ int ] = [ 33 , 39 , 51 , 63 , 75 ], repeat_blocks : int = 1 , ) -> nn . Module : \"\"\"Basic Quartznet encoder setup. Can be used to build either Quartznet5x5 (repeat_blocks=1) or Quartznet15x5 (repeat_blocks=3) Args: feat_in: Number of input features to the model. filters: List of filter sizes used to create the encoder blocks. kernel_sizes: List of kernel sizes corresponding to each filter size. repeat_blocks: Number of repetitions of each block. Returns: Pytorch model corresponding to the encoder. \"\"\" return MultiSequential ( stem ( feat_in ), * body ( filters , kernel_sizes , repeat_blocks ), ) body ( filters , kernel_size , repeat_blocks = 1 ) Creates the body of the Quartznet model. That is the middle part. PARAMETER DESCRIPTION filters List of filters inside each block in the body. TYPE: List [ int ] kernel_size Corresponding list of kernel sizes for each block. Should have the same length as the first argument. TYPE: List [ int ] repeat_blocks Number of repetitions of each block inside the body. TYPE: int DEFAULT: 1 RETURNS DESCRIPTION List [ QuartznetBlock ] List of layers that form the body of the network. Source code in thunder/quartznet/blocks.py 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 def body ( filters : List [ int ], kernel_size : List [ int ], repeat_blocks : int = 1 ) -> List [ QuartznetBlock ]: \"\"\"Creates the body of the Quartznet model. That is the middle part. Args: filters: List of filters inside each block in the body. kernel_size: Corresponding list of kernel sizes for each block. Should have the same length as the first argument. repeat_blocks: Number of repetitions of each block inside the body. Returns: List of layers that form the body of the network. \"\"\" layers = [] f_in = 256 for f , k in zip ( filters , kernel_size ): for _ in range ( repeat_blocks ): layers . append ( QuartznetBlock ( f_in , f , kernel_size = ( k ,), separable = True )) f_in = f layers . extend ( [ QuartznetBlock ( f_in , 512 , repeat = 1 , dilation = ( 2 ,), kernel_size = ( 87 ,), residual = False , separable = True , ), QuartznetBlock ( 512 , 1024 , repeat = 1 , kernel_size = ( 1 ,), residual = False , separable = False ), ] ) return layers init_weights ( m , mode = InitMode . xavier_uniform ) Initialize Linear, Conv1d or BatchNorm1d weights. There's no return, the operation occurs inplace. PARAMETER DESCRIPTION m The layer to be initialized TYPE: nn . Module mode Weight initialization mode. Only applicable to linear and conv layers. TYPE: InitMode DEFAULT: InitMode.xavier_uniform RAISES DESCRIPTION ValueError Raised when the initial mode is not one of the possible options. Source code in thunder/quartznet/blocks.py 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 def init_weights ( m : nn . Module , mode : InitMode = InitMode . xavier_uniform ): \"\"\"Initialize Linear, Conv1d or BatchNorm1d weights. There's no return, the operation occurs inplace. Args: m: The layer to be initialized mode: Weight initialization mode. Only applicable to linear and conv layers. Raises: ValueError: Raised when the initial mode is not one of the possible options. \"\"\" if isinstance ( m , MaskedConv1d ): init_weights ( m . conv , mode ) if isinstance ( m , ( nn . Conv1d , nn . Linear )): if mode == InitMode . xavier_uniform : nn . init . xavier_uniform_ ( m . weight , gain = 1.0 ) elif mode == InitMode . xavier_normal : nn . init . xavier_normal_ ( m . weight , gain = 1.0 ) elif mode == InitMode . kaiming_uniform : nn . init . kaiming_uniform_ ( m . weight , nonlinearity = \"relu\" ) elif mode == InitMode . kaiming_normal : nn . init . kaiming_normal_ ( m . weight , nonlinearity = \"relu\" ) else : raise ValueError ( \"Unknown Initialization mode: {0} \" . format ( mode )) elif isinstance ( m , nn . BatchNorm1d ): if m . track_running_stats : m . running_mean . zero_ () m . running_var . fill_ ( 1 ) m . num_batches_tracked . zero_ () if m . affine : nn . init . ones_ ( m . weight ) nn . init . zeros_ ( m . bias ) stem ( feat_in ) Creates the Quartznet stem. That is the first block of the model, that process the input directly. PARAMETER DESCRIPTION feat_in Number of input features TYPE: int RETURNS DESCRIPTION QuartznetBlock Quartznet stem block Source code in thunder/quartznet/blocks.py 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 def stem ( feat_in : int ) -> QuartznetBlock : \"\"\"Creates the Quartznet stem. That is the first block of the model, that process the input directly. Args: feat_in: Number of input features Returns: Quartznet stem block \"\"\" return QuartznetBlock ( feat_in , 256 , repeat = 1 , stride = ( 2 ,), kernel_size = ( 33 ,), residual = False , separable = True , )","title":"Blocks"},{"location":"api/Quartznet/blocks/#thunder.quartznet.blocks.InitMode","text":"Bases: str , Enum Note Possible values are xavier_uniform , xavier_normal , kaiming_uniform and kaiming_normal Weight init methods. Used by init_weights . Source code in thunder/quartznet/blocks.py 46 47 48 49 50 51 52 53 54 55 56 class InitMode ( str , Enum ): \"\"\"Weight init methods. Used by [`init_weights`][thunder.quartznet.blocks.init_weights]. Note: Possible values are `xavier_uniform`,`xavier_normal`,`kaiming_uniform` and `kaiming_normal` \"\"\" xavier_uniform = \"xavier_uniform\" xavier_normal = \"xavier_normal\" kaiming_uniform = \"kaiming_uniform\" kaiming_normal = \"kaiming_normal\"","title":"InitMode"},{"location":"api/Quartznet/blocks/#thunder.quartznet.blocks.MaskedConv1d","text":"Bases: nn . Module Source code in thunder/quartznet/blocks.py 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 class MaskedConv1d ( nn . Module ): __constants__ = [ \"use_mask\" , \"padding\" , \"dilation\" , \"kernel_size\" , \"stride\" ] def __init__ ( self , in_channels : int , out_channels : int , kernel_size : _size_1_t , stride : _size_1_t = 1 , padding : _size_1_t = 0 , dilation : _size_1_t = 1 , groups : int = 1 , bias : bool = False , use_mask : bool = True , ): \"\"\"Masked Convolution. This module correspond to a 1d convolution with input masking. Arguments to create are the same as nn.Conv1d, but with the addition of use_mask for special behaviour. Args: in_channels: Same as nn.Conv1d out_channels: Same as nn.Conv1d kernel_size: Same as nn.Conv1d stride: Same as nn.Conv1d padding: Same as nn.Conv1d dilation: Same as nn.Conv1d groups: Same as nn.Conv1d bias: Same as nn.Conv1d use_mask: Controls the masking of input before the convolution during the forward. \"\"\" super () . __init__ () self . use_mask = use_mask self . conv = nn . Conv1d ( in_channels , out_channels , kernel_size , stride = stride , padding = padding , dilation = dilation , groups = groups , bias = bias , ) self . padding = self . conv . padding [ 0 ] self . dilation = self . conv . dilation [ 0 ] self . kernel_size = self . conv . kernel_size [ 0 ] self . stride = self . conv . stride [ 0 ] def get_seq_len ( self , lengths : torch . Tensor ) -> torch . Tensor : \"\"\"Get the lengths of the inputs after the convolution operation is applied. Args: lengths: Original lengths of the inputs Returns: Resulting lengths after the convolution \"\"\" return ( torch . div ( lengths + 2 * self . padding - self . dilation * ( self . kernel_size - 1 ) - 1 , self . stride , rounding_mode = \"floor\" , ) + 1 ) def mask_fill ( self , x : torch . Tensor , lengths : torch . Tensor ) -> torch . Tensor : \"\"\"Mask the input based on it's respective lengths. Args: x: Signal to be processed, of shape (batch, features, time) lengths: Lenghts of each element in the batch of x, with shape (batch) Returns: The masked signal \"\"\" mask = lengths_to_mask ( lengths , x . shape [ - 1 ]) return x . masked_fill ( ~ mask . unsqueeze ( 1 ), 0 ) def forward ( self , x : torch . Tensor , lengths : torch . Tensor ) -> Tuple [ torch . Tensor , torch . Tensor ]: \"\"\"Forward method Args: x: Signal to be processed, of shape (batch, features, time) lengths: Lenghts of each element in the batch of x, with shape (batch) Returns: Both the signal processed by the convolution and the resulting lengths \"\"\" if self . use_mask : x = self . mask_fill ( x , lengths ) out = self . conv ( x ) return out , self . get_seq_len ( lengths )","title":"MaskedConv1d"},{"location":"api/Quartznet/blocks/#thunder.quartznet.blocks.MaskedConv1d.__init__","text":"Masked Convolution. This module correspond to a 1d convolution with input masking. Arguments to create are the same as nn.Conv1d, but with the addition of use_mask for special behaviour. PARAMETER DESCRIPTION in_channels Same as nn.Conv1d TYPE: int out_channels Same as nn.Conv1d TYPE: int kernel_size Same as nn.Conv1d TYPE: _size_1_t stride Same as nn.Conv1d TYPE: _size_1_t DEFAULT: 1 padding Same as nn.Conv1d TYPE: _size_1_t DEFAULT: 0 dilation Same as nn.Conv1d TYPE: _size_1_t DEFAULT: 1 groups Same as nn.Conv1d TYPE: int DEFAULT: 1 bias Same as nn.Conv1d TYPE: bool DEFAULT: False use_mask Controls the masking of input before the convolution during the forward. TYPE: bool DEFAULT: True Source code in thunder/quartznet/blocks.py 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 def __init__ ( self , in_channels : int , out_channels : int , kernel_size : _size_1_t , stride : _size_1_t = 1 , padding : _size_1_t = 0 , dilation : _size_1_t = 1 , groups : int = 1 , bias : bool = False , use_mask : bool = True , ): \"\"\"Masked Convolution. This module correspond to a 1d convolution with input masking. Arguments to create are the same as nn.Conv1d, but with the addition of use_mask for special behaviour. Args: in_channels: Same as nn.Conv1d out_channels: Same as nn.Conv1d kernel_size: Same as nn.Conv1d stride: Same as nn.Conv1d padding: Same as nn.Conv1d dilation: Same as nn.Conv1d groups: Same as nn.Conv1d bias: Same as nn.Conv1d use_mask: Controls the masking of input before the convolution during the forward. \"\"\" super () . __init__ () self . use_mask = use_mask self . conv = nn . Conv1d ( in_channels , out_channels , kernel_size , stride = stride , padding = padding , dilation = dilation , groups = groups , bias = bias , ) self . padding = self . conv . padding [ 0 ] self . dilation = self . conv . dilation [ 0 ] self . kernel_size = self . conv . kernel_size [ 0 ] self . stride = self . conv . stride [ 0 ]","title":"__init__()"},{"location":"api/Quartznet/blocks/#thunder.quartznet.blocks.MaskedConv1d.forward","text":"Forward method PARAMETER DESCRIPTION x Signal to be processed, of shape (batch, features, time) TYPE: torch . Tensor lengths Lenghts of each element in the batch of x, with shape (batch) TYPE: torch . Tensor RETURNS DESCRIPTION Tuple [ torch . Tensor , torch . Tensor ] Both the signal processed by the convolution and the resulting lengths Source code in thunder/quartznet/blocks.py 169 170 171 172 173 174 175 176 177 178 179 180 181 182 def forward ( self , x : torch . Tensor , lengths : torch . Tensor ) -> Tuple [ torch . Tensor , torch . Tensor ]: \"\"\"Forward method Args: x: Signal to be processed, of shape (batch, features, time) lengths: Lenghts of each element in the batch of x, with shape (batch) Returns: Both the signal processed by the convolution and the resulting lengths \"\"\" if self . use_mask : x = self . mask_fill ( x , lengths ) out = self . conv ( x ) return out , self . get_seq_len ( lengths )","title":"forward()"},{"location":"api/Quartznet/blocks/#thunder.quartznet.blocks.MaskedConv1d.get_seq_len","text":"Get the lengths of the inputs after the convolution operation is applied. PARAMETER DESCRIPTION lengths Original lengths of the inputs TYPE: torch . Tensor RETURNS DESCRIPTION torch . Tensor Resulting lengths after the convolution Source code in thunder/quartznet/blocks.py 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 def get_seq_len ( self , lengths : torch . Tensor ) -> torch . Tensor : \"\"\"Get the lengths of the inputs after the convolution operation is applied. Args: lengths: Original lengths of the inputs Returns: Resulting lengths after the convolution \"\"\" return ( torch . div ( lengths + 2 * self . padding - self . dilation * ( self . kernel_size - 1 ) - 1 , self . stride , rounding_mode = \"floor\" , ) + 1 )","title":"get_seq_len()"},{"location":"api/Quartznet/blocks/#thunder.quartznet.blocks.MaskedConv1d.mask_fill","text":"Mask the input based on it's respective lengths. PARAMETER DESCRIPTION x Signal to be processed, of shape (batch, features, time) TYPE: torch . Tensor lengths Lenghts of each element in the batch of x, with shape (batch) TYPE: torch . Tensor RETURNS DESCRIPTION torch . Tensor The masked signal Source code in thunder/quartznet/blocks.py 158 159 160 161 162 163 164 165 166 167 def mask_fill ( self , x : torch . Tensor , lengths : torch . Tensor ) -> torch . Tensor : \"\"\"Mask the input based on it's respective lengths. Args: x: Signal to be processed, of shape (batch, features, time) lengths: Lenghts of each element in the batch of x, with shape (batch) Returns: The masked signal \"\"\" mask = lengths_to_mask ( lengths , x . shape [ - 1 ]) return x . masked_fill ( ~ mask . unsqueeze ( 1 ), 0 )","title":"mask_fill()"},{"location":"api/Quartznet/blocks/#thunder.quartznet.blocks.QuartznetBlock","text":"Bases: nn . Module Source code in thunder/quartznet/blocks.py 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 class QuartznetBlock ( nn . Module ): def __init__ ( self , in_channels : int , out_channels : int , repeat : int = 5 , kernel_size : _size_1_t = ( 11 ,), stride : _size_1_t = ( 1 ,), dilation : _size_1_t = ( 1 ,), dropout : float = 0.0 , residual : bool = True , separable : bool = False , ): \"\"\"Quartznet block. This is a refactoring of the Jasperblock present on the NeMo toolkit, but simplified to only support the new quartznet model. Biggest change is that dense residual used on Jasper is not supported here, and masked convolutions were also removed. Args: in_channels: Number of input channels out_channels: Number of output channels repeat: Repetitions inside block. kernel_size: Kernel size. stride: Stride of each repetition. dilation: Dilation of each repetition. dropout: Dropout used before each activation. residual: Controls the use of residual connection. separable: Controls the use of separable convolutions. \"\"\" super () . __init__ () padding_val = get_same_padding ( kernel_size [ 0 ], stride [ 0 ], dilation [ 0 ]) inplanes_loop = in_channels conv = [] for _ in range ( repeat - 1 ): conv . extend ( _get_conv_bn_layer ( inplanes_loop , out_channels , kernel_size = kernel_size , stride = stride , dilation = dilation , padding = padding_val , separable = separable , bias = False , ) ) conv . extend ( _get_act_dropout_layer ( drop_prob = dropout )) inplanes_loop = out_channels conv . extend ( _get_conv_bn_layer ( inplanes_loop , out_channels , kernel_size = kernel_size , stride = stride , dilation = dilation , padding = padding_val , separable = separable , bias = False , ) ) self . mconv = MultiSequential ( * conv ) if residual : stride_residual = stride if stride [ 0 ] == 1 else stride [ 0 ] ** repeat self . res = MultiSequential ( * _get_conv_bn_layer ( in_channels , out_channels , kernel_size = 1 , stride = stride_residual , bias = False , ) ) else : self . res = None self . mout = MultiSequential ( * _get_act_dropout_layer ( drop_prob = dropout )) def forward ( self , x : torch . Tensor , lengths : torch . Tensor ) -> Tuple [ torch . Tensor , torch . Tensor ]: \"\"\" Args: x: Tensor of shape (batch, features, time) where #features == inplanes Returns: Result of applying the block on the input, and corresponding output lengths \"\"\" # compute forward convolutions out , lengths_out = self . mconv ( x , lengths ) # compute the residuals if self . res is not None : res_out , _ = self . res ( x , lengths ) out = out + res_out # compute the output out , lengths_out = self . mout ( out , lengths_out ) return out , lengths_out","title":"QuartznetBlock"},{"location":"api/Quartznet/blocks/#thunder.quartznet.blocks.QuartznetBlock.__init__","text":"Quartznet block. This is a refactoring of the Jasperblock present on the NeMo toolkit, but simplified to only support the new quartznet model. Biggest change is that dense residual used on Jasper is not supported here, and masked convolutions were also removed. PARAMETER DESCRIPTION in_channels Number of input channels TYPE: int out_channels Number of output channels TYPE: int repeat Repetitions inside block. TYPE: int DEFAULT: 5 kernel_size Kernel size. TYPE: _size_1_t DEFAULT: (11,) stride Stride of each repetition. TYPE: _size_1_t DEFAULT: (1,) dilation Dilation of each repetition. TYPE: _size_1_t DEFAULT: (1,) dropout Dropout used before each activation. TYPE: float DEFAULT: 0.0 residual Controls the use of residual connection. TYPE: bool DEFAULT: True separable Controls the use of separable convolutions. TYPE: bool DEFAULT: False Source code in thunder/quartznet/blocks.py 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 def __init__ ( self , in_channels : int , out_channels : int , repeat : int = 5 , kernel_size : _size_1_t = ( 11 ,), stride : _size_1_t = ( 1 ,), dilation : _size_1_t = ( 1 ,), dropout : float = 0.0 , residual : bool = True , separable : bool = False , ): \"\"\"Quartznet block. This is a refactoring of the Jasperblock present on the NeMo toolkit, but simplified to only support the new quartznet model. Biggest change is that dense residual used on Jasper is not supported here, and masked convolutions were also removed. Args: in_channels: Number of input channels out_channels: Number of output channels repeat: Repetitions inside block. kernel_size: Kernel size. stride: Stride of each repetition. dilation: Dilation of each repetition. dropout: Dropout used before each activation. residual: Controls the use of residual connection. separable: Controls the use of separable convolutions. \"\"\" super () . __init__ () padding_val = get_same_padding ( kernel_size [ 0 ], stride [ 0 ], dilation [ 0 ]) inplanes_loop = in_channels conv = [] for _ in range ( repeat - 1 ): conv . extend ( _get_conv_bn_layer ( inplanes_loop , out_channels , kernel_size = kernel_size , stride = stride , dilation = dilation , padding = padding_val , separable = separable , bias = False , ) ) conv . extend ( _get_act_dropout_layer ( drop_prob = dropout )) inplanes_loop = out_channels conv . extend ( _get_conv_bn_layer ( inplanes_loop , out_channels , kernel_size = kernel_size , stride = stride , dilation = dilation , padding = padding_val , separable = separable , bias = False , ) ) self . mconv = MultiSequential ( * conv ) if residual : stride_residual = stride if stride [ 0 ] == 1 else stride [ 0 ] ** repeat self . res = MultiSequential ( * _get_conv_bn_layer ( in_channels , out_channels , kernel_size = 1 , stride = stride_residual , bias = False , ) ) else : self . res = None self . mout = MultiSequential ( * _get_act_dropout_layer ( drop_prob = dropout ))","title":"__init__()"},{"location":"api/Quartznet/blocks/#thunder.quartznet.blocks.QuartznetBlock.forward","text":"PARAMETER DESCRIPTION x Tensor of shape (batch, features, time) where #features == inplanes TYPE: torch . Tensor RETURNS DESCRIPTION Tuple [ torch . Tensor , torch . Tensor ] Result of applying the block on the input, and corresponding output lengths Source code in thunder/quartznet/blocks.py 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 def forward ( self , x : torch . Tensor , lengths : torch . Tensor ) -> Tuple [ torch . Tensor , torch . Tensor ]: \"\"\" Args: x: Tensor of shape (batch, features, time) where #features == inplanes Returns: Result of applying the block on the input, and corresponding output lengths \"\"\" # compute forward convolutions out , lengths_out = self . mconv ( x , lengths ) # compute the residuals if self . res is not None : res_out , _ = self . res ( x , lengths ) out = out + res_out # compute the output out , lengths_out = self . mout ( out , lengths_out ) return out , lengths_out","title":"forward()"},{"location":"api/Quartznet/blocks/#thunder.quartznet.blocks.QuartznetEncoder","text":"Basic Quartznet encoder setup. Can be used to build either Quartznet5x5 (repeat_blocks=1) or Quartznet15x5 (repeat_blocks=3) PARAMETER DESCRIPTION feat_in Number of input features to the model. TYPE: int DEFAULT: 64 filters List of filter sizes used to create the encoder blocks. TYPE: List [ int ] DEFAULT: [256, 256, 512, 512, 512] kernel_sizes List of kernel sizes corresponding to each filter size. TYPE: List [ int ] DEFAULT: [33, 39, 51, 63, 75] repeat_blocks Number of repetitions of each block. TYPE: int DEFAULT: 1 RETURNS DESCRIPTION nn . Module Pytorch model corresponding to the encoder. Source code in thunder/quartznet/blocks.py 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 def QuartznetEncoder ( feat_in : int = 64 , filters : List [ int ] = [ 256 , 256 , 512 , 512 , 512 ], kernel_sizes : List [ int ] = [ 33 , 39 , 51 , 63 , 75 ], repeat_blocks : int = 1 , ) -> nn . Module : \"\"\"Basic Quartznet encoder setup. Can be used to build either Quartznet5x5 (repeat_blocks=1) or Quartznet15x5 (repeat_blocks=3) Args: feat_in: Number of input features to the model. filters: List of filter sizes used to create the encoder blocks. kernel_sizes: List of kernel sizes corresponding to each filter size. repeat_blocks: Number of repetitions of each block. Returns: Pytorch model corresponding to the encoder. \"\"\" return MultiSequential ( stem ( feat_in ), * body ( filters , kernel_sizes , repeat_blocks ), )","title":"QuartznetEncoder()"},{"location":"api/Quartznet/blocks/#thunder.quartznet.blocks.body","text":"Creates the body of the Quartznet model. That is the middle part. PARAMETER DESCRIPTION filters List of filters inside each block in the body. TYPE: List [ int ] kernel_size Corresponding list of kernel sizes for each block. Should have the same length as the first argument. TYPE: List [ int ] repeat_blocks Number of repetitions of each block inside the body. TYPE: int DEFAULT: 1 RETURNS DESCRIPTION List [ QuartznetBlock ] List of layers that form the body of the network. Source code in thunder/quartznet/blocks.py 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 def body ( filters : List [ int ], kernel_size : List [ int ], repeat_blocks : int = 1 ) -> List [ QuartznetBlock ]: \"\"\"Creates the body of the Quartznet model. That is the middle part. Args: filters: List of filters inside each block in the body. kernel_size: Corresponding list of kernel sizes for each block. Should have the same length as the first argument. repeat_blocks: Number of repetitions of each block inside the body. Returns: List of layers that form the body of the network. \"\"\" layers = [] f_in = 256 for f , k in zip ( filters , kernel_size ): for _ in range ( repeat_blocks ): layers . append ( QuartznetBlock ( f_in , f , kernel_size = ( k ,), separable = True )) f_in = f layers . extend ( [ QuartznetBlock ( f_in , 512 , repeat = 1 , dilation = ( 2 ,), kernel_size = ( 87 ,), residual = False , separable = True , ), QuartznetBlock ( 512 , 1024 , repeat = 1 , kernel_size = ( 1 ,), residual = False , separable = False ), ] ) return layers","title":"body()"},{"location":"api/Quartznet/blocks/#thunder.quartznet.blocks.init_weights","text":"Initialize Linear, Conv1d or BatchNorm1d weights. There's no return, the operation occurs inplace. PARAMETER DESCRIPTION m The layer to be initialized TYPE: nn . Module mode Weight initialization mode. Only applicable to linear and conv layers. TYPE: InitMode DEFAULT: InitMode.xavier_uniform RAISES DESCRIPTION ValueError Raised when the initial mode is not one of the possible options. Source code in thunder/quartznet/blocks.py 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 def init_weights ( m : nn . Module , mode : InitMode = InitMode . xavier_uniform ): \"\"\"Initialize Linear, Conv1d or BatchNorm1d weights. There's no return, the operation occurs inplace. Args: m: The layer to be initialized mode: Weight initialization mode. Only applicable to linear and conv layers. Raises: ValueError: Raised when the initial mode is not one of the possible options. \"\"\" if isinstance ( m , MaskedConv1d ): init_weights ( m . conv , mode ) if isinstance ( m , ( nn . Conv1d , nn . Linear )): if mode == InitMode . xavier_uniform : nn . init . xavier_uniform_ ( m . weight , gain = 1.0 ) elif mode == InitMode . xavier_normal : nn . init . xavier_normal_ ( m . weight , gain = 1.0 ) elif mode == InitMode . kaiming_uniform : nn . init . kaiming_uniform_ ( m . weight , nonlinearity = \"relu\" ) elif mode == InitMode . kaiming_normal : nn . init . kaiming_normal_ ( m . weight , nonlinearity = \"relu\" ) else : raise ValueError ( \"Unknown Initialization mode: {0} \" . format ( mode )) elif isinstance ( m , nn . BatchNorm1d ): if m . track_running_stats : m . running_mean . zero_ () m . running_var . fill_ ( 1 ) m . num_batches_tracked . zero_ () if m . affine : nn . init . ones_ ( m . weight ) nn . init . zeros_ ( m . bias )","title":"init_weights()"},{"location":"api/Quartznet/blocks/#thunder.quartznet.blocks.stem","text":"Creates the Quartznet stem. That is the first block of the model, that process the input directly. PARAMETER DESCRIPTION feat_in Number of input features TYPE: int RETURNS DESCRIPTION QuartznetBlock Quartznet stem block Source code in thunder/quartznet/blocks.py 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 def stem ( feat_in : int ) -> QuartznetBlock : \"\"\"Creates the Quartznet stem. That is the first block of the model, that process the input directly. Args: feat_in: Number of input features Returns: Quartznet stem block \"\"\" return QuartznetBlock ( feat_in , 256 , repeat = 1 , stride = ( 2 ,), kernel_size = ( 33 ,), residual = False , separable = True , )","title":"stem()"},{"location":"api/Quartznet/compatibility/","text":"Helper functions to load the Quartznet model from original Nemo released checkpoint files. QuartznetCheckpoint Bases: BaseCheckpoint Note Possible values are QuartzNet15x5Base_En , QuartzNet15x5Base_Zh , QuartzNet5x5LS_En , QuartzNet15x5NR_En , stt_ca_quartznet15x5 , stt_it_quartznet15x5 , stt_fr_quartznet15x5 , stt_es_quartznet15x5 , stt_de_quartznet15x5 , stt_pl_quartznet15x5 , stt_ru_quartznet15x5 , stt_en_quartznet15x5 , stt_zh_quartznet15x5 Trained model weight checkpoints. Used by download_checkpoint and load_quartznet_checkpoint . Source code in thunder/quartznet/compatibility.py 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 class QuartznetCheckpoint ( BaseCheckpoint ): \"\"\"Trained model weight checkpoints. Used by [`download_checkpoint`][thunder.utils.download_checkpoint] and [`load_quartznet_checkpoint`][thunder.quartznet.compatibility.load_quartznet_checkpoint]. Note: Possible values are `QuartzNet15x5Base_En`,`QuartzNet15x5Base_Zh`,`QuartzNet5x5LS_En`, `QuartzNet15x5NR_En`, `stt_ca_quartznet15x5`,`stt_it_quartznet15x5`,`stt_fr_quartznet15x5`,`stt_es_quartznet15x5`, `stt_de_quartznet15x5`,`stt_pl_quartznet15x5`,`stt_ru_quartznet15x5`,`stt_en_quartznet15x5`, `stt_zh_quartznet15x5` \"\"\" QuartzNet15x5Base_En = \"https://api.ngc.nvidia.com/v2/models/nvidia/nemospeechmodels/versions/1.0.0a5/files/QuartzNet15x5Base-En.nemo\" QuartzNet15x5Base_Zh = \"https://api.ngc.nvidia.com/v2/models/nvidia/nemospeechmodels/versions/1.0.0a5/files/QuartzNet15x5Base-Zh.nemo\" QuartzNet5x5LS_En = \"https://api.ngc.nvidia.com/v2/models/nvidia/nemospeechmodels/versions/1.0.0a5/files/QuartzNet5x5LS-En.nemo\" QuartzNet15x5NR_En = \"https://api.ngc.nvidia.com/v2/models/nvidia/nemospeechmodels/versions/1.0.0a5/files/QuartzNet15x5NR-En.nemo\" stt_ca_quartznet15x5 = \"https://api.ngc.nvidia.com/v2/models/nvidia/nemo/stt_ca_quartznet15x5/versions/1.0.0rc1/files/stt_ca_quartznet15x5.nemo\" stt_it_quartznet15x5 = \"https://api.ngc.nvidia.com/v2/models/nvidia/nemo/stt_it_quartznet15x5/versions/1.0.0rc1/files/stt_it_quartznet15x5.nemo\" stt_fr_quartznet15x5 = \"https://api.ngc.nvidia.com/v2/models/nvidia/nemo/stt_fr_quartznet15x5/versions/1.0.0rc1/files/stt_fr_quartznet15x5.nemo\" stt_es_quartznet15x5 = \"https://api.ngc.nvidia.com/v2/models/nvidia/nemo/stt_es_quartznet15x5/versions/1.0.0rc1/files/stt_es_quartznet15x5.nemo\" stt_de_quartznet15x5 = \"https://api.ngc.nvidia.com/v2/models/nvidia/nemo/stt_de_quartznet15x5/versions/1.0.0rc1/files/stt_de_quartznet15x5.nemo\" stt_pl_quartznet15x5 = \"https://api.ngc.nvidia.com/v2/models/nvidia/nemo/stt_pl_quartznet15x5/versions/1.0.0rc1/files/stt_pl_quartznet15x5.nemo\" stt_ru_quartznet15x5 = \"https://api.ngc.nvidia.com/v2/models/nvidia/nemo/stt_ru_quartznet15x5/versions/1.0.0rc1/files/stt_ru_quartznet15x5.nemo\" stt_en_quartznet15x5 = \"https://api.ngc.nvidia.com/v2/models/nvidia/nemo/stt_en_quartznet15x5/versions/1.0.0rc1/files/stt_en_quartznet15x5.nemo\" stt_zh_quartznet15x5 = \"https://api.ngc.nvidia.com/v2/models/nvidia/nemo/stt_zh_quartznet15x5/versions/1.0.0rc1/files/stt_zh_quartznet15x5.nemo\" load_components_from_quartznet_config ( config_path ) Read the important parameters from the config stored inside the .nemo checkpoint. PARAMETER DESCRIPTION config_path Path to the .yaml file, usually called model_config.yaml TYPE: Union [ str , Path ] RETURNS DESCRIPTION Tuple [ nn . Module , nn . Module , BatchTextTransformer ] A tuple containing, in this order, the encoder, the audio transform and the text transform Source code in thunder/quartznet/compatibility.py 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 def load_components_from_quartznet_config ( config_path : Union [ str , Path ] ) -> Tuple [ nn . Module , nn . Module , BatchTextTransformer ]: \"\"\"Read the important parameters from the config stored inside the .nemo checkpoint. Args: config_path: Path to the .yaml file, usually called model_config.yaml Returns: A tuple containing, in this order, the encoder, the audio transform and the text transform \"\"\" conf = OmegaConf . load ( config_path ) encoder_params = conf [ \"encoder\" ][ \"params\" ] quartznet_conf = OmegaConf . to_container ( encoder_params [ \"jasper\" ]) body_config = quartznet_conf [ 1 : - 2 ] filters = [ cfg [ \"filters\" ] for cfg in body_config ] kernel_sizes = [ cfg [ \"kernel\" ][ 0 ] for cfg in body_config ] encoder_cfg = { \"filters\" : filters , \"kernel_sizes\" : kernel_sizes , } preprocess = conf [ \"preprocessor\" ][ \"params\" ] preprocess_cfg = { \"sample_rate\" : preprocess [ \"sample_rate\" ], \"n_window_size\" : int ( preprocess [ \"window_size\" ] * preprocess [ \"sample_rate\" ]), \"n_window_stride\" : int ( preprocess [ \"window_stride\" ] * preprocess [ \"sample_rate\" ]), \"n_fft\" : preprocess [ \"n_fft\" ], \"nfilt\" : preprocess [ \"features\" ], \"dither\" : preprocess [ \"dither\" ], } labels = ( conf [ \"labels\" ] if \"labels\" in conf else conf [ \"decoder\" ][ \"params\" ][ \"vocabulary\" ] ) audio_transform = FilterbankFeatures ( ** preprocess_cfg ) encoder = QuartznetEncoder ( ** encoder_cfg ) text_transform = BatchTextTransformer ( tokens = OmegaConf . to_container ( labels ), ) return ( encoder , audio_transform , text_transform , ) load_quartznet_checkpoint ( checkpoint , save_folder = None ) Load from the original nemo checkpoint. PARAMETER DESCRIPTION checkpoint Path to local .nemo file or checkpoint to be downloaded locally and lodaded. TYPE: Union [ str , QuartznetCheckpoint ] save_folder Path to save the checkpoint when downloading it. Ignored if you pass a .nemo file as the first argument. TYPE: str DEFAULT: None RETURNS DESCRIPTION BaseCTCModule The model loaded from the checkpoint Source code in thunder/quartznet/compatibility.py 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 def load_quartznet_checkpoint ( checkpoint : Union [ str , QuartznetCheckpoint ], save_folder : str = None ) -> BaseCTCModule : \"\"\"Load from the original nemo checkpoint. Args: checkpoint: Path to local .nemo file or checkpoint to be downloaded locally and lodaded. save_folder: Path to save the checkpoint when downloading it. Ignored if you pass a .nemo file as the first argument. Returns: The model loaded from the checkpoint \"\"\" if isinstance ( checkpoint , QuartznetCheckpoint ): nemo_filepath = download_checkpoint ( checkpoint , save_folder ) else : nemo_filepath = Path ( checkpoint ) with TemporaryDirectory () as extract_folder : extract_archive ( str ( nemo_filepath ), extract_folder ) extract_path = Path ( extract_folder ) config_path = extract_path / \"model_config.yaml\" ( encoder , audio_transform , text_transform , ) = load_components_from_quartznet_config ( config_path ) decoder = conv1d_decoder ( 1024 , text_transform . num_tokens ) weights_path = extract_path / \"model_weights.ckpt\" load_quartznet_weights ( encoder , decoder , str ( weights_path )) module = BaseCTCModule ( encoder , decoder , audio_transform , text_transform , encoder_final_dimension = 1024 , ) return module . eval () load_quartznet_weights ( encoder , decoder , weights_path ) Load Quartznet model weights from data present inside .nemo file PARAMETER DESCRIPTION encoder Encoder module to load the weights into TYPE: nn . Module decoder Decoder module to load the weights into TYPE: nn . Module weights_path Path to the pytorch weights checkpoint TYPE: str Source code in thunder/quartznet/compatibility.py 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 def load_quartznet_weights ( encoder : nn . Module , decoder : nn . Module , weights_path : str ): \"\"\"Load Quartznet model weights from data present inside .nemo file Args: encoder: Encoder module to load the weights into decoder: Decoder module to load the weights into weights_path: Path to the pytorch weights checkpoint \"\"\" weights = torch . load ( weights_path ) def fix_encoder_name ( x : str ) -> str : x = x . replace ( \"encoder.\" , \"\" ) . replace ( \".res.0\" , \".res\" ) # Add another abstraction layer if it's not a masked conv # This is caused by the new Masked wrapper if \".conv\" not in x : parts = x . split ( \".\" ) x = \".\" . join ( parts [: 3 ] + [ \"layer\" , \"0\" ] + parts [ 3 :]) return x # We remove the 'encoder.' and 'decoder.' prefix from the weights to enable # compatibility to load with plain nn.Modules created by reading the config encoder_weights = { fix_encoder_name ( k ): v for k , v in weights . items () if \"encoder\" in k } encoder . load_state_dict ( encoder_weights , strict = True ) decoder_weights = { k . replace ( \"decoder.decoder_layers.0.\" , \"\" ): v for k , v in weights . items () if \"decoder\" in k } decoder . load_state_dict ( decoder_weights , strict = True )","title":"Compatibility"},{"location":"api/Quartznet/compatibility/#thunder.quartznet.compatibility.QuartznetCheckpoint","text":"Bases: BaseCheckpoint Note Possible values are QuartzNet15x5Base_En , QuartzNet15x5Base_Zh , QuartzNet5x5LS_En , QuartzNet15x5NR_En , stt_ca_quartznet15x5 , stt_it_quartznet15x5 , stt_fr_quartznet15x5 , stt_es_quartznet15x5 , stt_de_quartznet15x5 , stt_pl_quartznet15x5 , stt_ru_quartznet15x5 , stt_en_quartznet15x5 , stt_zh_quartznet15x5 Trained model weight checkpoints. Used by download_checkpoint and load_quartznet_checkpoint . Source code in thunder/quartznet/compatibility.py 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 class QuartznetCheckpoint ( BaseCheckpoint ): \"\"\"Trained model weight checkpoints. Used by [`download_checkpoint`][thunder.utils.download_checkpoint] and [`load_quartznet_checkpoint`][thunder.quartznet.compatibility.load_quartznet_checkpoint]. Note: Possible values are `QuartzNet15x5Base_En`,`QuartzNet15x5Base_Zh`,`QuartzNet5x5LS_En`, `QuartzNet15x5NR_En`, `stt_ca_quartznet15x5`,`stt_it_quartznet15x5`,`stt_fr_quartznet15x5`,`stt_es_quartznet15x5`, `stt_de_quartznet15x5`,`stt_pl_quartznet15x5`,`stt_ru_quartznet15x5`,`stt_en_quartznet15x5`, `stt_zh_quartznet15x5` \"\"\" QuartzNet15x5Base_En = \"https://api.ngc.nvidia.com/v2/models/nvidia/nemospeechmodels/versions/1.0.0a5/files/QuartzNet15x5Base-En.nemo\" QuartzNet15x5Base_Zh = \"https://api.ngc.nvidia.com/v2/models/nvidia/nemospeechmodels/versions/1.0.0a5/files/QuartzNet15x5Base-Zh.nemo\" QuartzNet5x5LS_En = \"https://api.ngc.nvidia.com/v2/models/nvidia/nemospeechmodels/versions/1.0.0a5/files/QuartzNet5x5LS-En.nemo\" QuartzNet15x5NR_En = \"https://api.ngc.nvidia.com/v2/models/nvidia/nemospeechmodels/versions/1.0.0a5/files/QuartzNet15x5NR-En.nemo\" stt_ca_quartznet15x5 = \"https://api.ngc.nvidia.com/v2/models/nvidia/nemo/stt_ca_quartznet15x5/versions/1.0.0rc1/files/stt_ca_quartznet15x5.nemo\" stt_it_quartznet15x5 = \"https://api.ngc.nvidia.com/v2/models/nvidia/nemo/stt_it_quartznet15x5/versions/1.0.0rc1/files/stt_it_quartznet15x5.nemo\" stt_fr_quartznet15x5 = \"https://api.ngc.nvidia.com/v2/models/nvidia/nemo/stt_fr_quartznet15x5/versions/1.0.0rc1/files/stt_fr_quartznet15x5.nemo\" stt_es_quartznet15x5 = \"https://api.ngc.nvidia.com/v2/models/nvidia/nemo/stt_es_quartznet15x5/versions/1.0.0rc1/files/stt_es_quartznet15x5.nemo\" stt_de_quartznet15x5 = \"https://api.ngc.nvidia.com/v2/models/nvidia/nemo/stt_de_quartznet15x5/versions/1.0.0rc1/files/stt_de_quartznet15x5.nemo\" stt_pl_quartznet15x5 = \"https://api.ngc.nvidia.com/v2/models/nvidia/nemo/stt_pl_quartznet15x5/versions/1.0.0rc1/files/stt_pl_quartznet15x5.nemo\" stt_ru_quartznet15x5 = \"https://api.ngc.nvidia.com/v2/models/nvidia/nemo/stt_ru_quartznet15x5/versions/1.0.0rc1/files/stt_ru_quartznet15x5.nemo\" stt_en_quartznet15x5 = \"https://api.ngc.nvidia.com/v2/models/nvidia/nemo/stt_en_quartznet15x5/versions/1.0.0rc1/files/stt_en_quartznet15x5.nemo\" stt_zh_quartznet15x5 = \"https://api.ngc.nvidia.com/v2/models/nvidia/nemo/stt_zh_quartznet15x5/versions/1.0.0rc1/files/stt_zh_quartznet15x5.nemo\"","title":"QuartznetCheckpoint"},{"location":"api/Quartznet/compatibility/#thunder.quartznet.compatibility.load_components_from_quartznet_config","text":"Read the important parameters from the config stored inside the .nemo checkpoint. PARAMETER DESCRIPTION config_path Path to the .yaml file, usually called model_config.yaml TYPE: Union [ str , Path ] RETURNS DESCRIPTION Tuple [ nn . Module , nn . Module , BatchTextTransformer ] A tuple containing, in this order, the encoder, the audio transform and the text transform Source code in thunder/quartznet/compatibility.py 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 def load_components_from_quartznet_config ( config_path : Union [ str , Path ] ) -> Tuple [ nn . Module , nn . Module , BatchTextTransformer ]: \"\"\"Read the important parameters from the config stored inside the .nemo checkpoint. Args: config_path: Path to the .yaml file, usually called model_config.yaml Returns: A tuple containing, in this order, the encoder, the audio transform and the text transform \"\"\" conf = OmegaConf . load ( config_path ) encoder_params = conf [ \"encoder\" ][ \"params\" ] quartznet_conf = OmegaConf . to_container ( encoder_params [ \"jasper\" ]) body_config = quartznet_conf [ 1 : - 2 ] filters = [ cfg [ \"filters\" ] for cfg in body_config ] kernel_sizes = [ cfg [ \"kernel\" ][ 0 ] for cfg in body_config ] encoder_cfg = { \"filters\" : filters , \"kernel_sizes\" : kernel_sizes , } preprocess = conf [ \"preprocessor\" ][ \"params\" ] preprocess_cfg = { \"sample_rate\" : preprocess [ \"sample_rate\" ], \"n_window_size\" : int ( preprocess [ \"window_size\" ] * preprocess [ \"sample_rate\" ]), \"n_window_stride\" : int ( preprocess [ \"window_stride\" ] * preprocess [ \"sample_rate\" ]), \"n_fft\" : preprocess [ \"n_fft\" ], \"nfilt\" : preprocess [ \"features\" ], \"dither\" : preprocess [ \"dither\" ], } labels = ( conf [ \"labels\" ] if \"labels\" in conf else conf [ \"decoder\" ][ \"params\" ][ \"vocabulary\" ] ) audio_transform = FilterbankFeatures ( ** preprocess_cfg ) encoder = QuartznetEncoder ( ** encoder_cfg ) text_transform = BatchTextTransformer ( tokens = OmegaConf . to_container ( labels ), ) return ( encoder , audio_transform , text_transform , )","title":"load_components_from_quartznet_config()"},{"location":"api/Quartznet/compatibility/#thunder.quartznet.compatibility.load_quartznet_checkpoint","text":"Load from the original nemo checkpoint. PARAMETER DESCRIPTION checkpoint Path to local .nemo file or checkpoint to be downloaded locally and lodaded. TYPE: Union [ str , QuartznetCheckpoint ] save_folder Path to save the checkpoint when downloading it. Ignored if you pass a .nemo file as the first argument. TYPE: str DEFAULT: None RETURNS DESCRIPTION BaseCTCModule The model loaded from the checkpoint Source code in thunder/quartznet/compatibility.py 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 def load_quartznet_checkpoint ( checkpoint : Union [ str , QuartznetCheckpoint ], save_folder : str = None ) -> BaseCTCModule : \"\"\"Load from the original nemo checkpoint. Args: checkpoint: Path to local .nemo file or checkpoint to be downloaded locally and lodaded. save_folder: Path to save the checkpoint when downloading it. Ignored if you pass a .nemo file as the first argument. Returns: The model loaded from the checkpoint \"\"\" if isinstance ( checkpoint , QuartznetCheckpoint ): nemo_filepath = download_checkpoint ( checkpoint , save_folder ) else : nemo_filepath = Path ( checkpoint ) with TemporaryDirectory () as extract_folder : extract_archive ( str ( nemo_filepath ), extract_folder ) extract_path = Path ( extract_folder ) config_path = extract_path / \"model_config.yaml\" ( encoder , audio_transform , text_transform , ) = load_components_from_quartznet_config ( config_path ) decoder = conv1d_decoder ( 1024 , text_transform . num_tokens ) weights_path = extract_path / \"model_weights.ckpt\" load_quartznet_weights ( encoder , decoder , str ( weights_path )) module = BaseCTCModule ( encoder , decoder , audio_transform , text_transform , encoder_final_dimension = 1024 , ) return module . eval ()","title":"load_quartznet_checkpoint()"},{"location":"api/Quartznet/compatibility/#thunder.quartznet.compatibility.load_quartznet_weights","text":"Load Quartznet model weights from data present inside .nemo file PARAMETER DESCRIPTION encoder Encoder module to load the weights into TYPE: nn . Module decoder Decoder module to load the weights into TYPE: nn . Module weights_path Path to the pytorch weights checkpoint TYPE: str Source code in thunder/quartznet/compatibility.py 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 def load_quartznet_weights ( encoder : nn . Module , decoder : nn . Module , weights_path : str ): \"\"\"Load Quartznet model weights from data present inside .nemo file Args: encoder: Encoder module to load the weights into decoder: Decoder module to load the weights into weights_path: Path to the pytorch weights checkpoint \"\"\" weights = torch . load ( weights_path ) def fix_encoder_name ( x : str ) -> str : x = x . replace ( \"encoder.\" , \"\" ) . replace ( \".res.0\" , \".res\" ) # Add another abstraction layer if it's not a masked conv # This is caused by the new Masked wrapper if \".conv\" not in x : parts = x . split ( \".\" ) x = \".\" . join ( parts [: 3 ] + [ \"layer\" , \"0\" ] + parts [ 3 :]) return x # We remove the 'encoder.' and 'decoder.' prefix from the weights to enable # compatibility to load with plain nn.Modules created by reading the config encoder_weights = { fix_encoder_name ( k ): v for k , v in weights . items () if \"encoder\" in k } encoder . load_state_dict ( encoder_weights , strict = True ) decoder_weights = { k . replace ( \"decoder.decoder_layers.0.\" , \"\" ): v for k , v in weights . items () if \"decoder\" in k } decoder . load_state_dict ( decoder_weights , strict = True )","title":"load_quartznet_weights()"},{"location":"api/Quartznet/transform/","text":"Functionality to transform the audio input in the same way that the Quartznet model expects it. DitherAudio Bases: nn . Module Source code in thunder/quartznet/transform.py 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 class DitherAudio ( nn . Module ): def __init__ ( self , dither : float = 1e-5 ): \"\"\"Add some dithering to the audio tensor. Note: From wikipedia: Dither is an intentionally applied form of noise used to randomize quantization error. Args: dither: Amount of dither to add. \"\"\" super () . __init__ () self . dither = dither @torch . no_grad () def forward ( self , x : torch . Tensor ) -> torch . Tensor : \"\"\" Args: x: Tensor of shape (batch, time) \"\"\" if self . training : return x + ( self . dither * torch . randn_like ( x )) else : return x __init__ ( dither = 1e-05 ) Note From wikipedia: Dither is an intentionally applied form of noise used to randomize quantization error. Add some dithering to the audio tensor. PARAMETER DESCRIPTION dither Amount of dither to add. TYPE: float DEFAULT: 1e-05 Source code in thunder/quartznet/transform.py 94 95 96 97 98 99 100 101 102 103 104 105 def __init__ ( self , dither : float = 1e-5 ): \"\"\"Add some dithering to the audio tensor. Note: From wikipedia: Dither is an intentionally applied form of noise used to randomize quantization error. Args: dither: Amount of dither to add. \"\"\" super () . __init__ () self . dither = dither forward ( x ) PARAMETER DESCRIPTION x Tensor of shape (batch, time) TYPE: torch . Tensor Source code in thunder/quartznet/transform.py 107 108 109 110 111 112 113 114 115 116 @torch . no_grad () def forward ( self , x : torch . Tensor ) -> torch . Tensor : \"\"\" Args: x: Tensor of shape (batch, time) \"\"\" if self . training : return x + ( self . dither * torch . randn_like ( x )) else : return x FeatureBatchNormalizer Bases: nn . Module Source code in thunder/quartznet/transform.py 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 class FeatureBatchNormalizer ( nn . Module ): def __init__ ( self ): \"\"\"Normalize batch at the feature dimension.\"\"\" super () . __init__ () self . div_guard = 1e-5 def forward ( self , x : torch . Tensor , lengths : torch . Tensor ) -> Tuple [ torch . Tensor , torch . Tensor ]: \"\"\" Args: x: Tensor of shape (batch, features, time) \"\"\" # https://github.com/pytorch/pytorch/issues/45208 # https://github.com/pytorch/pytorch/issues/44768 with torch . no_grad (): mask = lengths_to_mask ( lengths , x . shape [ - 1 ]) return ( normalize_tensor ( x , mask . unsqueeze ( 1 ), div_guard = self . div_guard ), lengths , ) __init__ () Normalize batch at the feature dimension. Source code in thunder/quartznet/transform.py 71 72 73 74 def __init__ ( self ): \"\"\"Normalize batch at the feature dimension.\"\"\" super () . __init__ () self . div_guard = 1e-5 forward ( x , lengths ) PARAMETER DESCRIPTION x Tensor of shape (batch, features, time) TYPE: torch . Tensor Source code in thunder/quartznet/transform.py 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 def forward ( self , x : torch . Tensor , lengths : torch . Tensor ) -> Tuple [ torch . Tensor , torch . Tensor ]: \"\"\" Args: x: Tensor of shape (batch, features, time) \"\"\" # https://github.com/pytorch/pytorch/issues/45208 # https://github.com/pytorch/pytorch/issues/44768 with torch . no_grad (): mask = lengths_to_mask ( lengths , x . shape [ - 1 ]) return ( normalize_tensor ( x , mask . unsqueeze ( 1 ), div_guard = self . div_guard ), lengths , ) MelScale Bases: nn . Module Source code in thunder/quartznet/transform.py 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 class MelScale ( nn . Module ): def __init__ ( self , sample_rate : int , n_fft : int , nfilt : int , log_scale : bool = True ): \"\"\"Convert a spectrogram to Mel scale, following the default formula of librosa instead of the one used by torchaudio. Also converts to log scale. Args: sample_rate: Sampling rate of the signal n_fft: Number of fourier features nfilt: Number of output mel filters to use log_scale: Controls if the output should also be applied a log scale. \"\"\" super () . __init__ () filterbanks = ( melscale_fbanks ( int ( 1 + n_fft // 2 ), n_mels = nfilt , sample_rate = sample_rate , f_min = 0 , f_max = sample_rate / 2 , norm = \"slaney\" , mel_scale = \"slaney\" , ) . transpose ( 0 , 1 ) . unsqueeze ( 0 ) ) self . register_buffer ( \"fb\" , filterbanks ) self . log_scale = log_scale @torch . no_grad () def forward ( self , x : torch . Tensor ) -> torch . Tensor : \"\"\" Args: x: Tensor of shape (batch, features, time) \"\"\" # dot with filterbank energies x = torch . matmul ( self . fb . to ( x . dtype ), x ) # log features # We want to avoid taking the log of zero if self . log_scale : x = torch . log ( x + 2 ** - 24 ) return x __init__ ( sample_rate , n_fft , nfilt , log_scale = True ) Convert a spectrogram to Mel scale, following the default formula of librosa instead of the one used by torchaudio. Also converts to log scale. PARAMETER DESCRIPTION sample_rate Sampling rate of the signal TYPE: int n_fft Number of fourier features TYPE: int nfilt Number of output mel filters to use TYPE: int log_scale Controls if the output should also be applied a log scale. TYPE: bool DEFAULT: True Source code in thunder/quartznet/transform.py 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 def __init__ ( self , sample_rate : int , n_fft : int , nfilt : int , log_scale : bool = True ): \"\"\"Convert a spectrogram to Mel scale, following the default formula of librosa instead of the one used by torchaudio. Also converts to log scale. Args: sample_rate: Sampling rate of the signal n_fft: Number of fourier features nfilt: Number of output mel filters to use log_scale: Controls if the output should also be applied a log scale. \"\"\" super () . __init__ () filterbanks = ( melscale_fbanks ( int ( 1 + n_fft // 2 ), n_mels = nfilt , sample_rate = sample_rate , f_min = 0 , f_max = sample_rate / 2 , norm = \"slaney\" , mel_scale = \"slaney\" , ) . transpose ( 0 , 1 ) . unsqueeze ( 0 ) ) self . register_buffer ( \"fb\" , filterbanks ) self . log_scale = log_scale forward ( x ) PARAMETER DESCRIPTION x Tensor of shape (batch, features, time) TYPE: torch . Tensor Source code in thunder/quartznet/transform.py 241 242 243 244 245 246 247 248 249 250 251 252 253 @torch . no_grad () def forward ( self , x : torch . Tensor ) -> torch . Tensor : \"\"\" Args: x: Tensor of shape (batch, features, time) \"\"\" # dot with filterbank energies x = torch . matmul ( self . fb . to ( x . dtype ), x ) # log features # We want to avoid taking the log of zero if self . log_scale : x = torch . log ( x + 2 ** - 24 ) return x PowerSpectrum Bases: nn . Module Source code in thunder/quartznet/transform.py 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 class PowerSpectrum ( nn . Module ): def __init__ ( self , n_window_size : int = 320 , n_window_stride : int = 160 , n_fft : Optional [ int ] = None , ): \"\"\"Calculates the power spectrum of the audio signal, following the same method as used in NEMO. Args: n_window_size: Number of elements in the window size. n_window_stride: Number of elements in the window stride. n_fft: Number of fourier features. Raises: ValueError: Raised when incompatible parameters are passed. \"\"\" super () . __init__ () if n_window_size <= 0 or n_window_stride <= 0 : raise ValueError ( f \" { self } got an invalid value for either n_window_size or \" f \"n_window_stride. Both must be positive ints.\" ) self . win_length = n_window_size self . hop_length = n_window_stride self . n_fft = n_fft or 2 ** math . ceil ( math . log2 ( self . win_length )) window_tensor = torch . hann_window ( self . win_length , periodic = False ) self . register_buffer ( \"window\" , window_tensor ) # This way so that the torch.stft can be changed to the patched version # before scripting. That way it works correctly when the export option # doesnt support fft, like mobile or onnx. self . stft_func = torch . stft def get_sequence_length ( self , lengths : torch . Tensor ) -> torch . Tensor : seq_len = torch . floor ( lengths / self . hop_length ) + 1 return seq_len . to ( dtype = torch . long ) @torch . no_grad () def forward ( self , x : torch . Tensor , lengths : torch . Tensor ) -> Tuple [ torch . Tensor , torch . Tensor ]: \"\"\" Args: x: Tensor of shape (batch, time) \"\"\" x = self . stft_func ( x , n_fft = self . n_fft , hop_length = self . hop_length , win_length = self . win_length , center = True , window = self . window . to ( dtype = torch . float ), return_complex = False , ) # torch returns real, imag; so convert to magnitude x = torch . sqrt ( x . pow ( 2 ) . sum ( - 1 )) # get power spectrum x = x . pow ( 2.0 ) return x , self . get_sequence_length ( lengths ) __init__ ( n_window_size = 320 , n_window_stride = 160 , n_fft = None ) Calculates the power spectrum of the audio signal, following the same method as used in NEMO. PARAMETER DESCRIPTION n_window_size Number of elements in the window size. TYPE: int DEFAULT: 320 n_window_stride Number of elements in the window stride. TYPE: int DEFAULT: 160 n_fft Number of fourier features. TYPE: Optional [ int ] DEFAULT: None RAISES DESCRIPTION ValueError Raised when incompatible parameters are passed. Source code in thunder/quartznet/transform.py 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 def __init__ ( self , n_window_size : int = 320 , n_window_stride : int = 160 , n_fft : Optional [ int ] = None , ): \"\"\"Calculates the power spectrum of the audio signal, following the same method as used in NEMO. Args: n_window_size: Number of elements in the window size. n_window_stride: Number of elements in the window stride. n_fft: Number of fourier features. Raises: ValueError: Raised when incompatible parameters are passed. \"\"\" super () . __init__ () if n_window_size <= 0 or n_window_stride <= 0 : raise ValueError ( f \" { self } got an invalid value for either n_window_size or \" f \"n_window_stride. Both must be positive ints.\" ) self . win_length = n_window_size self . hop_length = n_window_stride self . n_fft = n_fft or 2 ** math . ceil ( math . log2 ( self . win_length )) window_tensor = torch . hann_window ( self . win_length , periodic = False ) self . register_buffer ( \"window\" , window_tensor ) # This way so that the torch.stft can be changed to the patched version # before scripting. That way it works correctly when the export option # doesnt support fft, like mobile or onnx. self . stft_func = torch . stft forward ( x , lengths ) PARAMETER DESCRIPTION x Tensor of shape (batch, time) TYPE: torch . Tensor Source code in thunder/quartznet/transform.py 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 @torch . no_grad () def forward ( self , x : torch . Tensor , lengths : torch . Tensor ) -> Tuple [ torch . Tensor , torch . Tensor ]: \"\"\" Args: x: Tensor of shape (batch, time) \"\"\" x = self . stft_func ( x , n_fft = self . n_fft , hop_length = self . hop_length , win_length = self . win_length , center = True , window = self . window . to ( dtype = torch . float ), return_complex = False , ) # torch returns real, imag; so convert to magnitude x = torch . sqrt ( x . pow ( 2 ) . sum ( - 1 )) # get power spectrum x = x . pow ( 2.0 ) return x , self . get_sequence_length ( lengths ) PreEmphasisFilter Bases: nn . Module Source code in thunder/quartznet/transform.py 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 class PreEmphasisFilter ( nn . Module ): def __init__ ( self , preemph : float = 0.97 ): \"\"\"Applies preemphasis filtering to the audio signal. This is a classic signal processing function to emphasise the high frequency portion of the content compared to the low frequency. It applies a FIR filter of the form: `y[n] = y[n] - preemph * y[n-1]` Args: preemph: Filter control factor. \"\"\" super () . __init__ () self . preemph = preemph @torch . no_grad () def forward ( self , x : torch . Tensor ) -> torch . Tensor : \"\"\" Args: x: Tensor of shape (batch, time) \"\"\" return torch . cat ( ( x [:, 0 ] . unsqueeze ( 1 ), x [:, 1 :] - self . preemph * x [:, : - 1 ]), dim = 1 ) __init__ ( preemph = 0.97 ) Applies preemphasis filtering to the audio signal. This is a classic signal processing function to emphasise the high frequency portion of the content compared to the low frequency. It applies a FIR filter of the form: y[n] = y[n] - preemph * y[n-1] PARAMETER DESCRIPTION preemph Filter control factor. TYPE: float DEFAULT: 0.97 Source code in thunder/quartznet/transform.py 120 121 122 123 124 125 126 127 128 129 130 131 132 def __init__ ( self , preemph : float = 0.97 ): \"\"\"Applies preemphasis filtering to the audio signal. This is a classic signal processing function to emphasise the high frequency portion of the content compared to the low frequency. It applies a FIR filter of the form: `y[n] = y[n] - preemph * y[n-1]` Args: preemph: Filter control factor. \"\"\" super () . __init__ () self . preemph = preemph forward ( x ) PARAMETER DESCRIPTION x Tensor of shape (batch, time) TYPE: torch . Tensor Source code in thunder/quartznet/transform.py 134 135 136 137 138 139 140 141 142 @torch . no_grad () def forward ( self , x : torch . Tensor ) -> torch . Tensor : \"\"\" Args: x: Tensor of shape (batch, time) \"\"\" return torch . cat ( ( x [:, 0 ] . unsqueeze ( 1 ), x [:, 1 :] - self . preemph * x [:, : - 1 ]), dim = 1 ) FilterbankFeatures ( sample_rate = 16000 , n_window_size = 320 , n_window_stride = 160 , n_fft = 512 , preemph = 0.97 , nfilt = 64 , dither = 1e-05 ) Creates the Filterbank features used in the Quartznet model. PARAMETER DESCRIPTION sample_rate Sampling rate of the signal. TYPE: int DEFAULT: 16000 n_window_size Number of elements in the window size. TYPE: int DEFAULT: 320 n_window_stride Number of elements in the window stride. TYPE: int DEFAULT: 160 n_fft Number of fourier features. TYPE: int DEFAULT: 512 preemph Preemphasis filtering control factor. TYPE: float DEFAULT: 0.97 nfilt Number of output mel filters to use. TYPE: int DEFAULT: 64 dither Amount of dither to add. TYPE: float DEFAULT: 1e-05 RETURNS DESCRIPTION nn . Module Module that computes the features based on raw audio tensor. Source code in thunder/quartznet/transform.py 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 def FilterbankFeatures ( sample_rate : int = 16000 , n_window_size : int = 320 , n_window_stride : int = 160 , n_fft : int = 512 , preemph : float = 0.97 , nfilt : int = 64 , dither : float = 1e-5 , ) -> nn . Module : \"\"\"Creates the Filterbank features used in the Quartznet model. Args: sample_rate: Sampling rate of the signal. n_window_size: Number of elements in the window size. n_window_stride: Number of elements in the window stride. n_fft: Number of fourier features. preemph: Preemphasis filtering control factor. nfilt: Number of output mel filters to use. dither: Amount of dither to add. Returns: Module that computes the features based on raw audio tensor. \"\"\" return MultiSequential ( Masked ( DitherAudio ( dither = dither ), PreEmphasisFilter ( preemph = preemph )), PowerSpectrum ( n_window_size = n_window_size , n_window_stride = n_window_stride , n_fft = n_fft , ), Masked ( MelScale ( sample_rate = sample_rate , n_fft = n_fft , nfilt = nfilt )), FeatureBatchNormalizer (), ) patch_stft ( filterbank ) This function applies a patch to the FilterbankFeatures to use instead a convolution layer based stft. That makes possible to export to onnx and use the scripted model directly on arm cpu's, inside mobile applications. PARAMETER DESCRIPTION filterbank the FilterbankFeatures layer to be patched TYPE: nn . Module RETURNS DESCRIPTION nn . Module Layer with the stft operation patched. Source code in thunder/quartznet/transform.py 290 291 292 293 294 295 296 297 298 299 300 301 302 def patch_stft ( filterbank : nn . Module ) -> nn . Module : \"\"\"This function applies a patch to the FilterbankFeatures to use instead a convolution layer based stft. That makes possible to export to onnx and use the scripted model directly on arm cpu's, inside mobile applications. Args: filterbank: the FilterbankFeatures layer to be patched Returns: Layer with the stft operation patched. \"\"\" filterbank [ 1 ] . stft_func = convolution_stft return filterbank","title":"Transform"},{"location":"api/Quartznet/transform/#thunder.quartznet.transform.DitherAudio","text":"Bases: nn . Module Source code in thunder/quartznet/transform.py 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 class DitherAudio ( nn . Module ): def __init__ ( self , dither : float = 1e-5 ): \"\"\"Add some dithering to the audio tensor. Note: From wikipedia: Dither is an intentionally applied form of noise used to randomize quantization error. Args: dither: Amount of dither to add. \"\"\" super () . __init__ () self . dither = dither @torch . no_grad () def forward ( self , x : torch . Tensor ) -> torch . Tensor : \"\"\" Args: x: Tensor of shape (batch, time) \"\"\" if self . training : return x + ( self . dither * torch . randn_like ( x )) else : return x","title":"DitherAudio"},{"location":"api/Quartznet/transform/#thunder.quartznet.transform.DitherAudio.__init__","text":"Note From wikipedia: Dither is an intentionally applied form of noise used to randomize quantization error. Add some dithering to the audio tensor. PARAMETER DESCRIPTION dither Amount of dither to add. TYPE: float DEFAULT: 1e-05 Source code in thunder/quartznet/transform.py 94 95 96 97 98 99 100 101 102 103 104 105 def __init__ ( self , dither : float = 1e-5 ): \"\"\"Add some dithering to the audio tensor. Note: From wikipedia: Dither is an intentionally applied form of noise used to randomize quantization error. Args: dither: Amount of dither to add. \"\"\" super () . __init__ () self . dither = dither","title":"__init__()"},{"location":"api/Quartznet/transform/#thunder.quartznet.transform.DitherAudio.forward","text":"PARAMETER DESCRIPTION x Tensor of shape (batch, time) TYPE: torch . Tensor Source code in thunder/quartznet/transform.py 107 108 109 110 111 112 113 114 115 116 @torch . no_grad () def forward ( self , x : torch . Tensor ) -> torch . Tensor : \"\"\" Args: x: Tensor of shape (batch, time) \"\"\" if self . training : return x + ( self . dither * torch . randn_like ( x )) else : return x","title":"forward()"},{"location":"api/Quartznet/transform/#thunder.quartznet.transform.FeatureBatchNormalizer","text":"Bases: nn . Module Source code in thunder/quartznet/transform.py 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 class FeatureBatchNormalizer ( nn . Module ): def __init__ ( self ): \"\"\"Normalize batch at the feature dimension.\"\"\" super () . __init__ () self . div_guard = 1e-5 def forward ( self , x : torch . Tensor , lengths : torch . Tensor ) -> Tuple [ torch . Tensor , torch . Tensor ]: \"\"\" Args: x: Tensor of shape (batch, features, time) \"\"\" # https://github.com/pytorch/pytorch/issues/45208 # https://github.com/pytorch/pytorch/issues/44768 with torch . no_grad (): mask = lengths_to_mask ( lengths , x . shape [ - 1 ]) return ( normalize_tensor ( x , mask . unsqueeze ( 1 ), div_guard = self . div_guard ), lengths , )","title":"FeatureBatchNormalizer"},{"location":"api/Quartznet/transform/#thunder.quartznet.transform.FeatureBatchNormalizer.__init__","text":"Normalize batch at the feature dimension. Source code in thunder/quartznet/transform.py 71 72 73 74 def __init__ ( self ): \"\"\"Normalize batch at the feature dimension.\"\"\" super () . __init__ () self . div_guard = 1e-5","title":"__init__()"},{"location":"api/Quartznet/transform/#thunder.quartznet.transform.FeatureBatchNormalizer.forward","text":"PARAMETER DESCRIPTION x Tensor of shape (batch, features, time) TYPE: torch . Tensor Source code in thunder/quartznet/transform.py 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 def forward ( self , x : torch . Tensor , lengths : torch . Tensor ) -> Tuple [ torch . Tensor , torch . Tensor ]: \"\"\" Args: x: Tensor of shape (batch, features, time) \"\"\" # https://github.com/pytorch/pytorch/issues/45208 # https://github.com/pytorch/pytorch/issues/44768 with torch . no_grad (): mask = lengths_to_mask ( lengths , x . shape [ - 1 ]) return ( normalize_tensor ( x , mask . unsqueeze ( 1 ), div_guard = self . div_guard ), lengths , )","title":"forward()"},{"location":"api/Quartznet/transform/#thunder.quartznet.transform.MelScale","text":"Bases: nn . Module Source code in thunder/quartznet/transform.py 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 class MelScale ( nn . Module ): def __init__ ( self , sample_rate : int , n_fft : int , nfilt : int , log_scale : bool = True ): \"\"\"Convert a spectrogram to Mel scale, following the default formula of librosa instead of the one used by torchaudio. Also converts to log scale. Args: sample_rate: Sampling rate of the signal n_fft: Number of fourier features nfilt: Number of output mel filters to use log_scale: Controls if the output should also be applied a log scale. \"\"\" super () . __init__ () filterbanks = ( melscale_fbanks ( int ( 1 + n_fft // 2 ), n_mels = nfilt , sample_rate = sample_rate , f_min = 0 , f_max = sample_rate / 2 , norm = \"slaney\" , mel_scale = \"slaney\" , ) . transpose ( 0 , 1 ) . unsqueeze ( 0 ) ) self . register_buffer ( \"fb\" , filterbanks ) self . log_scale = log_scale @torch . no_grad () def forward ( self , x : torch . Tensor ) -> torch . Tensor : \"\"\" Args: x: Tensor of shape (batch, features, time) \"\"\" # dot with filterbank energies x = torch . matmul ( self . fb . to ( x . dtype ), x ) # log features # We want to avoid taking the log of zero if self . log_scale : x = torch . log ( x + 2 ** - 24 ) return x","title":"MelScale"},{"location":"api/Quartznet/transform/#thunder.quartznet.transform.MelScale.__init__","text":"Convert a spectrogram to Mel scale, following the default formula of librosa instead of the one used by torchaudio. Also converts to log scale. PARAMETER DESCRIPTION sample_rate Sampling rate of the signal TYPE: int n_fft Number of fourier features TYPE: int nfilt Number of output mel filters to use TYPE: int log_scale Controls if the output should also be applied a log scale. TYPE: bool DEFAULT: True Source code in thunder/quartznet/transform.py 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 def __init__ ( self , sample_rate : int , n_fft : int , nfilt : int , log_scale : bool = True ): \"\"\"Convert a spectrogram to Mel scale, following the default formula of librosa instead of the one used by torchaudio. Also converts to log scale. Args: sample_rate: Sampling rate of the signal n_fft: Number of fourier features nfilt: Number of output mel filters to use log_scale: Controls if the output should also be applied a log scale. \"\"\" super () . __init__ () filterbanks = ( melscale_fbanks ( int ( 1 + n_fft // 2 ), n_mels = nfilt , sample_rate = sample_rate , f_min = 0 , f_max = sample_rate / 2 , norm = \"slaney\" , mel_scale = \"slaney\" , ) . transpose ( 0 , 1 ) . unsqueeze ( 0 ) ) self . register_buffer ( \"fb\" , filterbanks ) self . log_scale = log_scale","title":"__init__()"},{"location":"api/Quartznet/transform/#thunder.quartznet.transform.MelScale.forward","text":"PARAMETER DESCRIPTION x Tensor of shape (batch, features, time) TYPE: torch . Tensor Source code in thunder/quartznet/transform.py 241 242 243 244 245 246 247 248 249 250 251 252 253 @torch . no_grad () def forward ( self , x : torch . Tensor ) -> torch . Tensor : \"\"\" Args: x: Tensor of shape (batch, features, time) \"\"\" # dot with filterbank energies x = torch . matmul ( self . fb . to ( x . dtype ), x ) # log features # We want to avoid taking the log of zero if self . log_scale : x = torch . log ( x + 2 ** - 24 ) return x","title":"forward()"},{"location":"api/Quartznet/transform/#thunder.quartznet.transform.PowerSpectrum","text":"Bases: nn . Module Source code in thunder/quartznet/transform.py 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 class PowerSpectrum ( nn . Module ): def __init__ ( self , n_window_size : int = 320 , n_window_stride : int = 160 , n_fft : Optional [ int ] = None , ): \"\"\"Calculates the power spectrum of the audio signal, following the same method as used in NEMO. Args: n_window_size: Number of elements in the window size. n_window_stride: Number of elements in the window stride. n_fft: Number of fourier features. Raises: ValueError: Raised when incompatible parameters are passed. \"\"\" super () . __init__ () if n_window_size <= 0 or n_window_stride <= 0 : raise ValueError ( f \" { self } got an invalid value for either n_window_size or \" f \"n_window_stride. Both must be positive ints.\" ) self . win_length = n_window_size self . hop_length = n_window_stride self . n_fft = n_fft or 2 ** math . ceil ( math . log2 ( self . win_length )) window_tensor = torch . hann_window ( self . win_length , periodic = False ) self . register_buffer ( \"window\" , window_tensor ) # This way so that the torch.stft can be changed to the patched version # before scripting. That way it works correctly when the export option # doesnt support fft, like mobile or onnx. self . stft_func = torch . stft def get_sequence_length ( self , lengths : torch . Tensor ) -> torch . Tensor : seq_len = torch . floor ( lengths / self . hop_length ) + 1 return seq_len . to ( dtype = torch . long ) @torch . no_grad () def forward ( self , x : torch . Tensor , lengths : torch . Tensor ) -> Tuple [ torch . Tensor , torch . Tensor ]: \"\"\" Args: x: Tensor of shape (batch, time) \"\"\" x = self . stft_func ( x , n_fft = self . n_fft , hop_length = self . hop_length , win_length = self . win_length , center = True , window = self . window . to ( dtype = torch . float ), return_complex = False , ) # torch returns real, imag; so convert to magnitude x = torch . sqrt ( x . pow ( 2 ) . sum ( - 1 )) # get power spectrum x = x . pow ( 2.0 ) return x , self . get_sequence_length ( lengths )","title":"PowerSpectrum"},{"location":"api/Quartznet/transform/#thunder.quartznet.transform.PowerSpectrum.__init__","text":"Calculates the power spectrum of the audio signal, following the same method as used in NEMO. PARAMETER DESCRIPTION n_window_size Number of elements in the window size. TYPE: int DEFAULT: 320 n_window_stride Number of elements in the window stride. TYPE: int DEFAULT: 160 n_fft Number of fourier features. TYPE: Optional [ int ] DEFAULT: None RAISES DESCRIPTION ValueError Raised when incompatible parameters are passed. Source code in thunder/quartznet/transform.py 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 def __init__ ( self , n_window_size : int = 320 , n_window_stride : int = 160 , n_fft : Optional [ int ] = None , ): \"\"\"Calculates the power spectrum of the audio signal, following the same method as used in NEMO. Args: n_window_size: Number of elements in the window size. n_window_stride: Number of elements in the window stride. n_fft: Number of fourier features. Raises: ValueError: Raised when incompatible parameters are passed. \"\"\" super () . __init__ () if n_window_size <= 0 or n_window_stride <= 0 : raise ValueError ( f \" { self } got an invalid value for either n_window_size or \" f \"n_window_stride. Both must be positive ints.\" ) self . win_length = n_window_size self . hop_length = n_window_stride self . n_fft = n_fft or 2 ** math . ceil ( math . log2 ( self . win_length )) window_tensor = torch . hann_window ( self . win_length , periodic = False ) self . register_buffer ( \"window\" , window_tensor ) # This way so that the torch.stft can be changed to the patched version # before scripting. That way it works correctly when the export option # doesnt support fft, like mobile or onnx. self . stft_func = torch . stft","title":"__init__()"},{"location":"api/Quartznet/transform/#thunder.quartznet.transform.PowerSpectrum.forward","text":"PARAMETER DESCRIPTION x Tensor of shape (batch, time) TYPE: torch . Tensor Source code in thunder/quartznet/transform.py 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 @torch . no_grad () def forward ( self , x : torch . Tensor , lengths : torch . Tensor ) -> Tuple [ torch . Tensor , torch . Tensor ]: \"\"\" Args: x: Tensor of shape (batch, time) \"\"\" x = self . stft_func ( x , n_fft = self . n_fft , hop_length = self . hop_length , win_length = self . win_length , center = True , window = self . window . to ( dtype = torch . float ), return_complex = False , ) # torch returns real, imag; so convert to magnitude x = torch . sqrt ( x . pow ( 2 ) . sum ( - 1 )) # get power spectrum x = x . pow ( 2.0 ) return x , self . get_sequence_length ( lengths )","title":"forward()"},{"location":"api/Quartznet/transform/#thunder.quartznet.transform.PreEmphasisFilter","text":"Bases: nn . Module Source code in thunder/quartznet/transform.py 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 class PreEmphasisFilter ( nn . Module ): def __init__ ( self , preemph : float = 0.97 ): \"\"\"Applies preemphasis filtering to the audio signal. This is a classic signal processing function to emphasise the high frequency portion of the content compared to the low frequency. It applies a FIR filter of the form: `y[n] = y[n] - preemph * y[n-1]` Args: preemph: Filter control factor. \"\"\" super () . __init__ () self . preemph = preemph @torch . no_grad () def forward ( self , x : torch . Tensor ) -> torch . Tensor : \"\"\" Args: x: Tensor of shape (batch, time) \"\"\" return torch . cat ( ( x [:, 0 ] . unsqueeze ( 1 ), x [:, 1 :] - self . preemph * x [:, : - 1 ]), dim = 1 )","title":"PreEmphasisFilter"},{"location":"api/Quartznet/transform/#thunder.quartznet.transform.PreEmphasisFilter.__init__","text":"Applies preemphasis filtering to the audio signal. This is a classic signal processing function to emphasise the high frequency portion of the content compared to the low frequency. It applies a FIR filter of the form: y[n] = y[n] - preemph * y[n-1] PARAMETER DESCRIPTION preemph Filter control factor. TYPE: float DEFAULT: 0.97 Source code in thunder/quartznet/transform.py 120 121 122 123 124 125 126 127 128 129 130 131 132 def __init__ ( self , preemph : float = 0.97 ): \"\"\"Applies preemphasis filtering to the audio signal. This is a classic signal processing function to emphasise the high frequency portion of the content compared to the low frequency. It applies a FIR filter of the form: `y[n] = y[n] - preemph * y[n-1]` Args: preemph: Filter control factor. \"\"\" super () . __init__ () self . preemph = preemph","title":"__init__()"},{"location":"api/Quartznet/transform/#thunder.quartznet.transform.PreEmphasisFilter.forward","text":"PARAMETER DESCRIPTION x Tensor of shape (batch, time) TYPE: torch . Tensor Source code in thunder/quartznet/transform.py 134 135 136 137 138 139 140 141 142 @torch . no_grad () def forward ( self , x : torch . Tensor ) -> torch . Tensor : \"\"\" Args: x: Tensor of shape (batch, time) \"\"\" return torch . cat ( ( x [:, 0 ] . unsqueeze ( 1 ), x [:, 1 :] - self . preemph * x [:, : - 1 ]), dim = 1 )","title":"forward()"},{"location":"api/Quartznet/transform/#thunder.quartznet.transform.FilterbankFeatures","text":"Creates the Filterbank features used in the Quartznet model. PARAMETER DESCRIPTION sample_rate Sampling rate of the signal. TYPE: int DEFAULT: 16000 n_window_size Number of elements in the window size. TYPE: int DEFAULT: 320 n_window_stride Number of elements in the window stride. TYPE: int DEFAULT: 160 n_fft Number of fourier features. TYPE: int DEFAULT: 512 preemph Preemphasis filtering control factor. TYPE: float DEFAULT: 0.97 nfilt Number of output mel filters to use. TYPE: int DEFAULT: 64 dither Amount of dither to add. TYPE: float DEFAULT: 1e-05 RETURNS DESCRIPTION nn . Module Module that computes the features based on raw audio tensor. Source code in thunder/quartznet/transform.py 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 def FilterbankFeatures ( sample_rate : int = 16000 , n_window_size : int = 320 , n_window_stride : int = 160 , n_fft : int = 512 , preemph : float = 0.97 , nfilt : int = 64 , dither : float = 1e-5 , ) -> nn . Module : \"\"\"Creates the Filterbank features used in the Quartznet model. Args: sample_rate: Sampling rate of the signal. n_window_size: Number of elements in the window size. n_window_stride: Number of elements in the window stride. n_fft: Number of fourier features. preemph: Preemphasis filtering control factor. nfilt: Number of output mel filters to use. dither: Amount of dither to add. Returns: Module that computes the features based on raw audio tensor. \"\"\" return MultiSequential ( Masked ( DitherAudio ( dither = dither ), PreEmphasisFilter ( preemph = preemph )), PowerSpectrum ( n_window_size = n_window_size , n_window_stride = n_window_stride , n_fft = n_fft , ), Masked ( MelScale ( sample_rate = sample_rate , n_fft = n_fft , nfilt = nfilt )), FeatureBatchNormalizer (), )","title":"FilterbankFeatures()"},{"location":"api/Quartznet/transform/#thunder.quartznet.transform.patch_stft","text":"This function applies a patch to the FilterbankFeatures to use instead a convolution layer based stft. That makes possible to export to onnx and use the scripted model directly on arm cpu's, inside mobile applications. PARAMETER DESCRIPTION filterbank the FilterbankFeatures layer to be patched TYPE: nn . Module RETURNS DESCRIPTION nn . Module Layer with the stft operation patched. Source code in thunder/quartznet/transform.py 290 291 292 293 294 295 296 297 298 299 300 301 302 def patch_stft ( filterbank : nn . Module ) -> nn . Module : \"\"\"This function applies a patch to the FilterbankFeatures to use instead a convolution layer based stft. That makes possible to export to onnx and use the scripted model directly on arm cpu's, inside mobile applications. Args: filterbank: the FilterbankFeatures layer to be patched Returns: Layer with the stft operation patched. \"\"\" filterbank [ 1 ] . stft_func = convolution_stft return filterbank","title":"patch_stft()"},{"location":"api/Text%20Processing/preprocess/","text":"Text preprocessing functionality expand_numbers ( text , language = 'en' ) Expand the numbers present inside the text. That means converting \"42\" into \"forty two\". It also detects if the number is ordinal automatically. PARAMETER DESCRIPTION text Input text TYPE: str language Language used to expand the numbers. Defaults to \"en\". TYPE: str DEFAULT: 'en' RETURNS DESCRIPTION str Output text Source code in thunder/text_processing/preprocess.py 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 def expand_numbers ( text : str , language : str = \"en\" ) -> str : \"\"\"Expand the numbers present inside the text. That means converting \"42\" into \"forty two\". It also detects if the number is ordinal automatically. Args: text: Input text language: Language used to expand the numbers. Defaults to \"en\". Returns: Output text \"\"\" number_regex = re . compile ( r \"\\d+\u00ba*\" ) all_numbers = number_regex . findall ( text ) for num in all_numbers : if \"\u00ba\" in num : pure_number = num . replace ( \"\u00ba\" , \"\" ) . strip () expanded = num2words ( int ( pure_number ), lang = language , to = \"ordinal\" ) else : expanded = num2words ( int ( num ), lang = language ) text = text . replace ( num , expanded ) return text lower_text ( text ) Transform all the text to lowercase. PARAMETER DESCRIPTION text Input text TYPE: str RETURNS DESCRIPTION str Output text Source code in thunder/text_processing/preprocess.py 18 19 20 21 22 23 24 25 26 27 def lower_text ( text : str ) -> str : \"\"\"Transform all the text to lowercase. Args: text: Input text Returns: Output text \"\"\" return text . lower () normalize_text ( text ) Normalize the text to remove accents and ensure all the characters are valid ascii symbols. PARAMETER DESCRIPTION text Input text TYPE: str RETURNS DESCRIPTION str Output text Source code in thunder/text_processing/preprocess.py 30 31 32 33 34 35 36 37 38 39 40 41 42 43 def normalize_text ( text : str ) -> str : \"\"\"Normalize the text to remove accents and ensure all the characters are valid ascii symbols. Args: text: Input text Returns: Output text \"\"\" nfkd_form = unicodedata . normalize ( \"NFKD\" , text ) only_ascii = nfkd_form . encode ( \"ASCII\" , \"ignore\" ) return only_ascii . decode ()","title":"Preprocess"},{"location":"api/Text%20Processing/preprocess/#thunder.text_processing.preprocess.expand_numbers","text":"Expand the numbers present inside the text. That means converting \"42\" into \"forty two\". It also detects if the number is ordinal automatically. PARAMETER DESCRIPTION text Input text TYPE: str language Language used to expand the numbers. Defaults to \"en\". TYPE: str DEFAULT: 'en' RETURNS DESCRIPTION str Output text Source code in thunder/text_processing/preprocess.py 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 def expand_numbers ( text : str , language : str = \"en\" ) -> str : \"\"\"Expand the numbers present inside the text. That means converting \"42\" into \"forty two\". It also detects if the number is ordinal automatically. Args: text: Input text language: Language used to expand the numbers. Defaults to \"en\". Returns: Output text \"\"\" number_regex = re . compile ( r \"\\d+\u00ba*\" ) all_numbers = number_regex . findall ( text ) for num in all_numbers : if \"\u00ba\" in num : pure_number = num . replace ( \"\u00ba\" , \"\" ) . strip () expanded = num2words ( int ( pure_number ), lang = language , to = \"ordinal\" ) else : expanded = num2words ( int ( num ), lang = language ) text = text . replace ( num , expanded ) return text","title":"expand_numbers()"},{"location":"api/Text%20Processing/preprocess/#thunder.text_processing.preprocess.lower_text","text":"Transform all the text to lowercase. PARAMETER DESCRIPTION text Input text TYPE: str RETURNS DESCRIPTION str Output text Source code in thunder/text_processing/preprocess.py 18 19 20 21 22 23 24 25 26 27 def lower_text ( text : str ) -> str : \"\"\"Transform all the text to lowercase. Args: text: Input text Returns: Output text \"\"\" return text . lower ()","title":"lower_text()"},{"location":"api/Text%20Processing/preprocess/#thunder.text_processing.preprocess.normalize_text","text":"Normalize the text to remove accents and ensure all the characters are valid ascii symbols. PARAMETER DESCRIPTION text Input text TYPE: str RETURNS DESCRIPTION str Output text Source code in thunder/text_processing/preprocess.py 30 31 32 33 34 35 36 37 38 39 40 41 42 43 def normalize_text ( text : str ) -> str : \"\"\"Normalize the text to remove accents and ensure all the characters are valid ascii symbols. Args: text: Input text Returns: Output text \"\"\" nfkd_form = unicodedata . normalize ( \"NFKD\" , text ) only_ascii = nfkd_form . encode ( \"ASCII\" , \"ignore\" ) return only_ascii . decode ()","title":"normalize_text()"},{"location":"api/Text%20Processing/tokenize/","text":"Text tokenization including character, word or sentencepiece char_tokenizer ( text ) Tokenize input text splitting into characters PARAMETER DESCRIPTION text Input text TYPE: str RETURNS DESCRIPTION List [ str ] Tokenized text Source code in thunder/text_processing/tokenizer.py 114 115 116 117 118 119 120 121 122 123 def char_tokenizer ( text : str ) -> List [ str ]: \"\"\"Tokenize input text splitting into characters Args: text: Input text Returns: Tokenized text \"\"\" return list ( text ) get_most_frequent_tokens ( corpus , tokenize_function , minimum_frequency = 1 , max_number_of_tokens = None ) Helper function to get the most frequent tokens from a text corpus. PARAMETER DESCRIPTION corpus Text corpus to be used, this is a long string containing all of your text TYPE: str tokenize_function Same tokenizer function that will be used during training TYPE: Callable minimum_frequency Remove any token with frequency less than that. Defaults to 1. TYPE: int DEFAULT: 1 max_number_of_tokens Optionally limit to the K most frequent tokens. Defaults to None. TYPE: Optional [ int ] DEFAULT: None RETURNS DESCRIPTION List [ str ] All of the unique, most frequent tokens, ordered by frequency. Source code in thunder/text_processing/tokenizer.py 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 def get_most_frequent_tokens ( corpus : str , tokenize_function : Callable , minimum_frequency : int = 1 , max_number_of_tokens : Optional [ int ] = None , ) -> List [ str ]: \"\"\"Helper function to get the most frequent tokens from a text corpus. Args: corpus: Text corpus to be used, this is a long string containing all of your text tokenize_function: Same tokenizer function that will be used during training minimum_frequency: Remove any token with frequency less than that. Defaults to 1. max_number_of_tokens: Optionally limit to the K most frequent tokens. Defaults to None. Returns: All of the unique, most frequent tokens, ordered by frequency. \"\"\" tokenized = tokenize_function ( corpus ) token_counter = Counter ( tokenized ) output_tokens = [] for token , count in token_counter . most_common ( max_number_of_tokens ): if count >= minimum_frequency : output_tokens . append ( token ) return output_tokens train_sentencepiece_model ( data_file , vocab_size , output_dir , sample_size =- 1 , do_lower_case = True , tokenizer_type = 'unigram' , character_coverage = 1.0 , train_extremely_large_corpus = False , max_sentencepiece_length =- 1 ) Creates sentence piece tokenizer model from data file. This is a direct port of create_spt_model present on the NEMO toolkit (nemo/collections/common/tokenizers/sentencepiece_tokenizer.py) PARAMETER DESCRIPTION data_file text file containing the sentences that will be used to train the model TYPE: str vocab_size maximum vocabulary size TYPE: int output_dir folder to save created tokenizer model and vocab TYPE: str sample_size maximum number of sentences the trainer loads. -1 means to use all the data. TYPE: int DEFAULT: -1 do_lower_case if text should be lower cased before tokenizer model is created TYPE: bool DEFAULT: True tokenizer_type controls the sentencepiece model type. TYPE: str DEFAULT: 'unigram' character_coverage float value between 0 and 1 (as a percentage). For languages with a vast charset, can be < 1.0, but for all other languages, it should be set as 1.0 TYPE: float DEFAULT: 1.0 train_extremely_large_corpus If training on huge datasets, pass this flag to allow SentencePiece to build the tokenizer. TYPE: bool DEFAULT: False max_sentencepiece_length Limits the maximum length of the SentencePiece subword that can be constructed. By default, no limit is placed. TYPE: int DEFAULT: -1 Source code in thunder/text_processing/tokenizer.py 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 def train_sentencepiece_model ( data_file : str , vocab_size : int , output_dir : str , sample_size : int = - 1 , do_lower_case : bool = True , tokenizer_type : str = \"unigram\" , character_coverage : float = 1.0 , train_extremely_large_corpus : bool = False , max_sentencepiece_length : int = - 1 , ) -> str : \"\"\" Creates sentence piece tokenizer model from data file. This is a direct port of `create_spt_model` present on the NEMO toolkit (nemo/collections/common/tokenizers/sentencepiece_tokenizer.py) Args: data_file: text file containing the sentences that will be used to train the model vocab_size: maximum vocabulary size output_dir: folder to save created tokenizer model and vocab sample_size: maximum number of sentences the trainer loads. -1 means to use all the data. do_lower_case: if text should be lower cased before tokenizer model is created tokenizer_type: controls the sentencepiece model type. character_coverage: float value between 0 and 1 (as a percentage). For languages with a vast charset, can be < 1.0, but for all other languages, it should be set as 1.0 train_extremely_large_corpus: If training on huge datasets, pass this flag to allow SentencePiece to build the tokenizer. max_sentencepiece_length: Limits the maximum length of the SentencePiece subword that can be constructed. By default, no limit is placed. \"\"\" data_file = Path ( data_file ) if not data_file or not data_file . exists (): raise ValueError ( f \"data_file must be valid file path, but got { data_file } \" ) output_dir = Path ( output_dir ) if ( output_dir / \"tokenizer.model\" ) . exists (): warn ( \"There's already a trained sentencepiece model at the output directory. Skipping train.\" ) return str ( output_dir ) output_dir . mkdir ( exist_ok = True ) cmd = ( f \"--input= { data_file } --model_prefix= { output_dir } /tokenizer \" f \"--vocab_size= { vocab_size } \" f \"--shuffle_input_sentence=true --hard_vocab_limit=false \" f \"--model_type= { tokenizer_type } \" f \"--character_coverage= { character_coverage } \" ) if do_lower_case : cmd += \" --normalization_rule_name=nmt_nfkc_cf\" if sample_size > 0 : cmd += f \" --input_sentence_size= { sample_size } \" if train_extremely_large_corpus : cmd += \" --train_extremely_large_corpus=true\" if max_sentencepiece_length >= 0 : cmd += f \" --max_sentencepiece_length= { max_sentencepiece_length } \" sentencepiece . SentencePieceTrainer . Train ( cmd ) return str ( output_dir ) word_tokenizer ( text ) Tokenize input text splitting into words PARAMETER DESCRIPTION text Input text TYPE: str RETURNS DESCRIPTION List [ str ] Tokenized text Source code in thunder/text_processing/tokenizer.py 102 103 104 105 106 107 108 109 110 111 def word_tokenizer ( text : str ) -> List [ str ]: \"\"\"Tokenize input text splitting into words Args: text: Input text Returns: Tokenized text \"\"\" return text . split ()","title":"Tokenize"},{"location":"api/Text%20Processing/tokenize/#thunder.text_processing.tokenizer.char_tokenizer","text":"Tokenize input text splitting into characters PARAMETER DESCRIPTION text Input text TYPE: str RETURNS DESCRIPTION List [ str ] Tokenized text Source code in thunder/text_processing/tokenizer.py 114 115 116 117 118 119 120 121 122 123 def char_tokenizer ( text : str ) -> List [ str ]: \"\"\"Tokenize input text splitting into characters Args: text: Input text Returns: Tokenized text \"\"\" return list ( text )","title":"char_tokenizer()"},{"location":"api/Text%20Processing/tokenize/#thunder.text_processing.tokenizer.get_most_frequent_tokens","text":"Helper function to get the most frequent tokens from a text corpus. PARAMETER DESCRIPTION corpus Text corpus to be used, this is a long string containing all of your text TYPE: str tokenize_function Same tokenizer function that will be used during training TYPE: Callable minimum_frequency Remove any token with frequency less than that. Defaults to 1. TYPE: int DEFAULT: 1 max_number_of_tokens Optionally limit to the K most frequent tokens. Defaults to None. TYPE: Optional [ int ] DEFAULT: None RETURNS DESCRIPTION List [ str ] All of the unique, most frequent tokens, ordered by frequency. Source code in thunder/text_processing/tokenizer.py 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 def get_most_frequent_tokens ( corpus : str , tokenize_function : Callable , minimum_frequency : int = 1 , max_number_of_tokens : Optional [ int ] = None , ) -> List [ str ]: \"\"\"Helper function to get the most frequent tokens from a text corpus. Args: corpus: Text corpus to be used, this is a long string containing all of your text tokenize_function: Same tokenizer function that will be used during training minimum_frequency: Remove any token with frequency less than that. Defaults to 1. max_number_of_tokens: Optionally limit to the K most frequent tokens. Defaults to None. Returns: All of the unique, most frequent tokens, ordered by frequency. \"\"\" tokenized = tokenize_function ( corpus ) token_counter = Counter ( tokenized ) output_tokens = [] for token , count in token_counter . most_common ( max_number_of_tokens ): if count >= minimum_frequency : output_tokens . append ( token ) return output_tokens","title":"get_most_frequent_tokens()"},{"location":"api/Text%20Processing/tokenize/#thunder.text_processing.tokenizer.train_sentencepiece_model","text":"Creates sentence piece tokenizer model from data file. This is a direct port of create_spt_model present on the NEMO toolkit (nemo/collections/common/tokenizers/sentencepiece_tokenizer.py) PARAMETER DESCRIPTION data_file text file containing the sentences that will be used to train the model TYPE: str vocab_size maximum vocabulary size TYPE: int output_dir folder to save created tokenizer model and vocab TYPE: str sample_size maximum number of sentences the trainer loads. -1 means to use all the data. TYPE: int DEFAULT: -1 do_lower_case if text should be lower cased before tokenizer model is created TYPE: bool DEFAULT: True tokenizer_type controls the sentencepiece model type. TYPE: str DEFAULT: 'unigram' character_coverage float value between 0 and 1 (as a percentage). For languages with a vast charset, can be < 1.0, but for all other languages, it should be set as 1.0 TYPE: float DEFAULT: 1.0 train_extremely_large_corpus If training on huge datasets, pass this flag to allow SentencePiece to build the tokenizer. TYPE: bool DEFAULT: False max_sentencepiece_length Limits the maximum length of the SentencePiece subword that can be constructed. By default, no limit is placed. TYPE: int DEFAULT: -1 Source code in thunder/text_processing/tokenizer.py 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 def train_sentencepiece_model ( data_file : str , vocab_size : int , output_dir : str , sample_size : int = - 1 , do_lower_case : bool = True , tokenizer_type : str = \"unigram\" , character_coverage : float = 1.0 , train_extremely_large_corpus : bool = False , max_sentencepiece_length : int = - 1 , ) -> str : \"\"\" Creates sentence piece tokenizer model from data file. This is a direct port of `create_spt_model` present on the NEMO toolkit (nemo/collections/common/tokenizers/sentencepiece_tokenizer.py) Args: data_file: text file containing the sentences that will be used to train the model vocab_size: maximum vocabulary size output_dir: folder to save created tokenizer model and vocab sample_size: maximum number of sentences the trainer loads. -1 means to use all the data. do_lower_case: if text should be lower cased before tokenizer model is created tokenizer_type: controls the sentencepiece model type. character_coverage: float value between 0 and 1 (as a percentage). For languages with a vast charset, can be < 1.0, but for all other languages, it should be set as 1.0 train_extremely_large_corpus: If training on huge datasets, pass this flag to allow SentencePiece to build the tokenizer. max_sentencepiece_length: Limits the maximum length of the SentencePiece subword that can be constructed. By default, no limit is placed. \"\"\" data_file = Path ( data_file ) if not data_file or not data_file . exists (): raise ValueError ( f \"data_file must be valid file path, but got { data_file } \" ) output_dir = Path ( output_dir ) if ( output_dir / \"tokenizer.model\" ) . exists (): warn ( \"There's already a trained sentencepiece model at the output directory. Skipping train.\" ) return str ( output_dir ) output_dir . mkdir ( exist_ok = True ) cmd = ( f \"--input= { data_file } --model_prefix= { output_dir } /tokenizer \" f \"--vocab_size= { vocab_size } \" f \"--shuffle_input_sentence=true --hard_vocab_limit=false \" f \"--model_type= { tokenizer_type } \" f \"--character_coverage= { character_coverage } \" ) if do_lower_case : cmd += \" --normalization_rule_name=nmt_nfkc_cf\" if sample_size > 0 : cmd += f \" --input_sentence_size= { sample_size } \" if train_extremely_large_corpus : cmd += \" --train_extremely_large_corpus=true\" if max_sentencepiece_length >= 0 : cmd += f \" --max_sentencepiece_length= { max_sentencepiece_length } \" sentencepiece . SentencePieceTrainer . Train ( cmd ) return str ( output_dir )","title":"train_sentencepiece_model()"},{"location":"api/Text%20Processing/tokenize/#thunder.text_processing.tokenizer.word_tokenizer","text":"Tokenize input text splitting into words PARAMETER DESCRIPTION text Input text TYPE: str RETURNS DESCRIPTION List [ str ] Tokenized text Source code in thunder/text_processing/tokenizer.py 102 103 104 105 106 107 108 109 110 111 def word_tokenizer ( text : str ) -> List [ str ]: \"\"\"Tokenize input text splitting into words Args: text: Input text Returns: Tokenized text \"\"\" return text . split ()","title":"word_tokenizer()"},{"location":"api/Text%20Processing/transform/","text":"Process batched text BatchTextTransformer Bases: nn . Module Source code in thunder/text_processing/transform.py 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 class BatchTextTransformer ( nn . Module ): def __init__ ( self , tokens : List [ str ], blank_token : str = \"<blank>\" , pad_token : str = None , unknown_token : str = None , start_token : str = None , end_token : str = None , sentencepiece_model : Optional [ str ] = None , ): \"\"\"That class is the glue code that uses all of the text processing functions to encode/decode an entire batch of text at once. Args: tokens: Basic list of tokens that will be part of the vocabulary. blank_token: Check [`Vocabulary`][thunder.text_processing.vocab.Vocabulary] pad_token: Check [`Vocabulary`][thunder.text_processing.vocab.Vocabulary] unknown_token: Check [`Vocabulary`][thunder.text_processing.vocab.Vocabulary] start_token: Check [`Vocabulary`][thunder.text_processing.vocab.Vocabulary] end_token: Check [`Vocabulary`][thunder.text_processing.vocab.Vocabulary] sentencepiece_model: Path to sentencepiece .model file, if applicable. \"\"\" super () . __init__ () self . vocab = Vocabulary ( tokens , blank_token , pad_token , unknown_token , start_token , end_token , ) self . tokenizer = ( BPETokenizer ( sentencepiece_model ) if sentencepiece_model else char_tokenizer ) def encode ( self , items : List [ str ], return_length : bool = True , device = None ): tokenized = [ self . tokenizer ( x ) for x in items ] expanded_tokenized = [ self . vocab . add_special_tokens ( x ) for x in tokenized ] encoded = [ self . vocab . numericalize ( x ) . to ( device = device ) for x in expanded_tokenized ] encoded_batched = pad_sequence ( encoded , batch_first = True , padding_value = self . vocab . pad_idx ) if return_length : lengths = torch . LongTensor ([ len ( it ) for it in encoded ]) . to ( device = device ) return encoded_batched , lengths else : return encoded_batched @torch . jit . export def decode_prediction ( self , predictions : torch . Tensor , remove_repeated : bool = True ) -> List [ str ]: \"\"\" Args: predictions: Tensor of shape (batch, time) remove_repeated: controls if repeated elements without a blank between them will be removed while decoding Returns: A list of decoded strings, one for each element in the batch. \"\"\" out_list : List [ str ] = [] for element in predictions : # Remove consecutive repeated elements if remove_repeated : element = torch . unique_consecutive ( element ) # Map back to string out = self . vocab . decode_into_text ( element ) # Join prediction into one string out = \"\" . join ( out ) # _ is a special char only present on sentencepiece out = out . replace ( \"\u2581\" , \" \" ) # | is a special char used by huggingface as space out = out . replace ( \"|\" , \" \" ) out = self . vocab . remove_special_tokens ( out ) out_list . append ( out ) return out_list @classmethod def from_sentencepiece ( cls , output_dir : str ) -> \"BatchTextTransformer\" : \"\"\"Load the data from a folder that contains the `tokenizer.vocab` and `tokenizer.model` outputs from sentencepiece. Args: output_dir: Output directory of the sentencepiece training, that contains the required files. Returns: Instance of `BatchTextTransformer` with the corresponding data loaded. \"\"\" special_tokens = [ \"<s>\" , \"</s>\" , \"<pad>\" , \"<unk>\" ] vocab = [] with open ( f \" { output_dir } /tokenizer.vocab\" , \"r\" ) as f : # Read tokens from each line and parse for vocab for line in f : piece = line . split ( \" \\t \" )[ 0 ] if piece in special_tokens : # skip special tokens continue vocab . append ( piece ) return cls ( tokens = vocab , sentencepiece_model = f \" { output_dir } /tokenizer.model\" , ) @property def num_tokens ( self ): return len ( self . vocab . itos ) __init__ ( tokens , blank_token = '<blank>' , pad_token = None , unknown_token = None , start_token = None , end_token = None , sentencepiece_model = None ) That class is the glue code that uses all of the text processing functions to encode/decode an entire batch of text at once. PARAMETER DESCRIPTION tokens Basic list of tokens that will be part of the vocabulary. TYPE: List [ str ] blank_token Check Vocabulary TYPE: str DEFAULT: '<blank>' pad_token Check Vocabulary TYPE: str DEFAULT: None unknown_token Check Vocabulary TYPE: str DEFAULT: None start_token Check Vocabulary TYPE: str DEFAULT: None end_token Check Vocabulary TYPE: str DEFAULT: None sentencepiece_model Path to sentencepiece .model file, if applicable. TYPE: Optional [ str ] DEFAULT: None Source code in thunder/text_processing/transform.py 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 def __init__ ( self , tokens : List [ str ], blank_token : str = \"<blank>\" , pad_token : str = None , unknown_token : str = None , start_token : str = None , end_token : str = None , sentencepiece_model : Optional [ str ] = None , ): \"\"\"That class is the glue code that uses all of the text processing functions to encode/decode an entire batch of text at once. Args: tokens: Basic list of tokens that will be part of the vocabulary. blank_token: Check [`Vocabulary`][thunder.text_processing.vocab.Vocabulary] pad_token: Check [`Vocabulary`][thunder.text_processing.vocab.Vocabulary] unknown_token: Check [`Vocabulary`][thunder.text_processing.vocab.Vocabulary] start_token: Check [`Vocabulary`][thunder.text_processing.vocab.Vocabulary] end_token: Check [`Vocabulary`][thunder.text_processing.vocab.Vocabulary] sentencepiece_model: Path to sentencepiece .model file, if applicable. \"\"\" super () . __init__ () self . vocab = Vocabulary ( tokens , blank_token , pad_token , unknown_token , start_token , end_token , ) self . tokenizer = ( BPETokenizer ( sentencepiece_model ) if sentencepiece_model else char_tokenizer ) decode_prediction ( predictions , remove_repeated = True ) PARAMETER DESCRIPTION predictions Tensor of shape (batch, time) TYPE: torch . Tensor remove_repeated controls if repeated elements without a blank between them will be removed while decoding TYPE: bool DEFAULT: True RETURNS DESCRIPTION List [ str ] A list of decoded strings, one for each element in the batch. Source code in thunder/text_processing/transform.py 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 @torch . jit . export def decode_prediction ( self , predictions : torch . Tensor , remove_repeated : bool = True ) -> List [ str ]: \"\"\" Args: predictions: Tensor of shape (batch, time) remove_repeated: controls if repeated elements without a blank between them will be removed while decoding Returns: A list of decoded strings, one for each element in the batch. \"\"\" out_list : List [ str ] = [] for element in predictions : # Remove consecutive repeated elements if remove_repeated : element = torch . unique_consecutive ( element ) # Map back to string out = self . vocab . decode_into_text ( element ) # Join prediction into one string out = \"\" . join ( out ) # _ is a special char only present on sentencepiece out = out . replace ( \"\u2581\" , \" \" ) # | is a special char used by huggingface as space out = out . replace ( \"|\" , \" \" ) out = self . vocab . remove_special_tokens ( out ) out_list . append ( out ) return out_list from_sentencepiece ( output_dir ) Load the data from a folder that contains the tokenizer.vocab and tokenizer.model outputs from sentencepiece. PARAMETER DESCRIPTION output_dir Output directory of the sentencepiece training, that contains the required files. TYPE: str RETURNS DESCRIPTION 'BatchTextTransformer' Instance of BatchTextTransformer with the corresponding data loaded. Source code in thunder/text_processing/transform.py 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 @classmethod def from_sentencepiece ( cls , output_dir : str ) -> \"BatchTextTransformer\" : \"\"\"Load the data from a folder that contains the `tokenizer.vocab` and `tokenizer.model` outputs from sentencepiece. Args: output_dir: Output directory of the sentencepiece training, that contains the required files. Returns: Instance of `BatchTextTransformer` with the corresponding data loaded. \"\"\" special_tokens = [ \"<s>\" , \"</s>\" , \"<pad>\" , \"<unk>\" ] vocab = [] with open ( f \" { output_dir } /tokenizer.vocab\" , \"r\" ) as f : # Read tokens from each line and parse for vocab for line in f : piece = line . split ( \" \\t \" )[ 0 ] if piece in special_tokens : # skip special tokens continue vocab . append ( piece ) return cls ( tokens = vocab , sentencepiece_model = f \" { output_dir } /tokenizer.model\" , )","title":"Transform"},{"location":"api/Text%20Processing/transform/#thunder.text_processing.transform.BatchTextTransformer","text":"Bases: nn . Module Source code in thunder/text_processing/transform.py 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 class BatchTextTransformer ( nn . Module ): def __init__ ( self , tokens : List [ str ], blank_token : str = \"<blank>\" , pad_token : str = None , unknown_token : str = None , start_token : str = None , end_token : str = None , sentencepiece_model : Optional [ str ] = None , ): \"\"\"That class is the glue code that uses all of the text processing functions to encode/decode an entire batch of text at once. Args: tokens: Basic list of tokens that will be part of the vocabulary. blank_token: Check [`Vocabulary`][thunder.text_processing.vocab.Vocabulary] pad_token: Check [`Vocabulary`][thunder.text_processing.vocab.Vocabulary] unknown_token: Check [`Vocabulary`][thunder.text_processing.vocab.Vocabulary] start_token: Check [`Vocabulary`][thunder.text_processing.vocab.Vocabulary] end_token: Check [`Vocabulary`][thunder.text_processing.vocab.Vocabulary] sentencepiece_model: Path to sentencepiece .model file, if applicable. \"\"\" super () . __init__ () self . vocab = Vocabulary ( tokens , blank_token , pad_token , unknown_token , start_token , end_token , ) self . tokenizer = ( BPETokenizer ( sentencepiece_model ) if sentencepiece_model else char_tokenizer ) def encode ( self , items : List [ str ], return_length : bool = True , device = None ): tokenized = [ self . tokenizer ( x ) for x in items ] expanded_tokenized = [ self . vocab . add_special_tokens ( x ) for x in tokenized ] encoded = [ self . vocab . numericalize ( x ) . to ( device = device ) for x in expanded_tokenized ] encoded_batched = pad_sequence ( encoded , batch_first = True , padding_value = self . vocab . pad_idx ) if return_length : lengths = torch . LongTensor ([ len ( it ) for it in encoded ]) . to ( device = device ) return encoded_batched , lengths else : return encoded_batched @torch . jit . export def decode_prediction ( self , predictions : torch . Tensor , remove_repeated : bool = True ) -> List [ str ]: \"\"\" Args: predictions: Tensor of shape (batch, time) remove_repeated: controls if repeated elements without a blank between them will be removed while decoding Returns: A list of decoded strings, one for each element in the batch. \"\"\" out_list : List [ str ] = [] for element in predictions : # Remove consecutive repeated elements if remove_repeated : element = torch . unique_consecutive ( element ) # Map back to string out = self . vocab . decode_into_text ( element ) # Join prediction into one string out = \"\" . join ( out ) # _ is a special char only present on sentencepiece out = out . replace ( \"\u2581\" , \" \" ) # | is a special char used by huggingface as space out = out . replace ( \"|\" , \" \" ) out = self . vocab . remove_special_tokens ( out ) out_list . append ( out ) return out_list @classmethod def from_sentencepiece ( cls , output_dir : str ) -> \"BatchTextTransformer\" : \"\"\"Load the data from a folder that contains the `tokenizer.vocab` and `tokenizer.model` outputs from sentencepiece. Args: output_dir: Output directory of the sentencepiece training, that contains the required files. Returns: Instance of `BatchTextTransformer` with the corresponding data loaded. \"\"\" special_tokens = [ \"<s>\" , \"</s>\" , \"<pad>\" , \"<unk>\" ] vocab = [] with open ( f \" { output_dir } /tokenizer.vocab\" , \"r\" ) as f : # Read tokens from each line and parse for vocab for line in f : piece = line . split ( \" \\t \" )[ 0 ] if piece in special_tokens : # skip special tokens continue vocab . append ( piece ) return cls ( tokens = vocab , sentencepiece_model = f \" { output_dir } /tokenizer.model\" , ) @property def num_tokens ( self ): return len ( self . vocab . itos )","title":"BatchTextTransformer"},{"location":"api/Text%20Processing/transform/#thunder.text_processing.transform.BatchTextTransformer.__init__","text":"That class is the glue code that uses all of the text processing functions to encode/decode an entire batch of text at once. PARAMETER DESCRIPTION tokens Basic list of tokens that will be part of the vocabulary. TYPE: List [ str ] blank_token Check Vocabulary TYPE: str DEFAULT: '<blank>' pad_token Check Vocabulary TYPE: str DEFAULT: None unknown_token Check Vocabulary TYPE: str DEFAULT: None start_token Check Vocabulary TYPE: str DEFAULT: None end_token Check Vocabulary TYPE: str DEFAULT: None sentencepiece_model Path to sentencepiece .model file, if applicable. TYPE: Optional [ str ] DEFAULT: None Source code in thunder/text_processing/transform.py 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 def __init__ ( self , tokens : List [ str ], blank_token : str = \"<blank>\" , pad_token : str = None , unknown_token : str = None , start_token : str = None , end_token : str = None , sentencepiece_model : Optional [ str ] = None , ): \"\"\"That class is the glue code that uses all of the text processing functions to encode/decode an entire batch of text at once. Args: tokens: Basic list of tokens that will be part of the vocabulary. blank_token: Check [`Vocabulary`][thunder.text_processing.vocab.Vocabulary] pad_token: Check [`Vocabulary`][thunder.text_processing.vocab.Vocabulary] unknown_token: Check [`Vocabulary`][thunder.text_processing.vocab.Vocabulary] start_token: Check [`Vocabulary`][thunder.text_processing.vocab.Vocabulary] end_token: Check [`Vocabulary`][thunder.text_processing.vocab.Vocabulary] sentencepiece_model: Path to sentencepiece .model file, if applicable. \"\"\" super () . __init__ () self . vocab = Vocabulary ( tokens , blank_token , pad_token , unknown_token , start_token , end_token , ) self . tokenizer = ( BPETokenizer ( sentencepiece_model ) if sentencepiece_model else char_tokenizer )","title":"__init__()"},{"location":"api/Text%20Processing/transform/#thunder.text_processing.transform.BatchTextTransformer.decode_prediction","text":"PARAMETER DESCRIPTION predictions Tensor of shape (batch, time) TYPE: torch . Tensor remove_repeated controls if repeated elements without a blank between them will be removed while decoding TYPE: bool DEFAULT: True RETURNS DESCRIPTION List [ str ] A list of decoded strings, one for each element in the batch. Source code in thunder/text_processing/transform.py 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 @torch . jit . export def decode_prediction ( self , predictions : torch . Tensor , remove_repeated : bool = True ) -> List [ str ]: \"\"\" Args: predictions: Tensor of shape (batch, time) remove_repeated: controls if repeated elements without a blank between them will be removed while decoding Returns: A list of decoded strings, one for each element in the batch. \"\"\" out_list : List [ str ] = [] for element in predictions : # Remove consecutive repeated elements if remove_repeated : element = torch . unique_consecutive ( element ) # Map back to string out = self . vocab . decode_into_text ( element ) # Join prediction into one string out = \"\" . join ( out ) # _ is a special char only present on sentencepiece out = out . replace ( \"\u2581\" , \" \" ) # | is a special char used by huggingface as space out = out . replace ( \"|\" , \" \" ) out = self . vocab . remove_special_tokens ( out ) out_list . append ( out ) return out_list","title":"decode_prediction()"},{"location":"api/Text%20Processing/transform/#thunder.text_processing.transform.BatchTextTransformer.from_sentencepiece","text":"Load the data from a folder that contains the tokenizer.vocab and tokenizer.model outputs from sentencepiece. PARAMETER DESCRIPTION output_dir Output directory of the sentencepiece training, that contains the required files. TYPE: str RETURNS DESCRIPTION 'BatchTextTransformer' Instance of BatchTextTransformer with the corresponding data loaded. Source code in thunder/text_processing/transform.py 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 @classmethod def from_sentencepiece ( cls , output_dir : str ) -> \"BatchTextTransformer\" : \"\"\"Load the data from a folder that contains the `tokenizer.vocab` and `tokenizer.model` outputs from sentencepiece. Args: output_dir: Output directory of the sentencepiece training, that contains the required files. Returns: Instance of `BatchTextTransformer` with the corresponding data loaded. \"\"\" special_tokens = [ \"<s>\" , \"</s>\" , \"<pad>\" , \"<unk>\" ] vocab = [] with open ( f \" { output_dir } /tokenizer.vocab\" , \"r\" ) as f : # Read tokens from each line and parse for vocab for line in f : piece = line . split ( \" \\t \" )[ 0 ] if piece in special_tokens : # skip special tokens continue vocab . append ( piece ) return cls ( tokens = vocab , sentencepiece_model = f \" { output_dir } /tokenizer.model\" , )","title":"from_sentencepiece()"},{"location":"api/Text%20Processing/vocab/","text":"Classes that represent the vocabulary used by the model. Vocabulary Bases: nn . Module Source code in thunder/text_processing/vocab.py 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 class Vocabulary ( nn . Module ): def __init__ ( self , tokens : List [ str ], blank_token : str = \"<blank>\" , pad_token : Optional [ str ] = None , unknown_token : Optional [ str ] = None , start_token : Optional [ str ] = None , end_token : Optional [ str ] = None , ): \"\"\"Class that represents a vocabulary, with the related methods to numericalize a sequence of tokens into numbers, and do the reverse mapping of numbers back to tokens. Args: tokens: Basic list of tokens that will be part of the vocabulary. Check [`docs`](https://scart97.github.io/thunder-speech/quick%20reference%20guide/#how-to-get-the-tokens-from-my-dataset) blank_token: Token that will represent the ctc blank. pad_token: Token that will represent padding, might also act as the ctc blank. unknown_token: Token that will represent unknown elements. Notice that this is different than the blank used by ctc. start_token: Token that will represent the beginning of the sequence. end_token: Token that will represent the end of the sequence. \"\"\" super () . __init__ () self . unknown_token = unknown_token self . start_token = start_token self . end_token = end_token self . blank_token = blank_token self . pad_token = pad_token or blank_token self . itos = tokens self . _maybe_add_token ( blank_token ) self . _maybe_add_token ( pad_token ) self . _maybe_add_token ( unknown_token ) self . _maybe_add_token ( start_token ) self . _maybe_add_token ( end_token ) self . stoi = { token : i for i , token in enumerate ( self . itos )} self . blank_idx = self . itos . index ( self . blank_token ) self . pad_idx = self . itos . index ( self . pad_token ) self . _unk_idx = - 1 if self . unknown_token is not None : self . _unk_idx = self . itos . index ( self . unknown_token ) def _maybe_add_token ( self , token : Optional [ str ]): # Only adds tokens if they are not optional # and are not included in the vocabulary already if token and ( token not in self . itos ): self . itos = self . itos + [ token ] def numericalize ( self , tokens : List [ str ]) -> torch . Tensor : \"\"\"Function to transform a list of tokens into the corresponding numeric representation. Args: tokens: A single list of tokens to be transformed Returns: The corresponding numeric representation \"\"\" if self . unknown_token is None : # When in there's no unknown token # we filter out all of the tokens not in the vocab tokens = [ t for t in tokens if t in self . itos ] return torch . tensor ( [ self . stoi . get ( it , self . _unk_idx ) for it in tokens ], dtype = torch . long ) @torch . jit . export def decode_into_text ( self , indices : torch . Tensor ) -> List [ str ]: \"\"\"Function to transform back a list of numbers into the corresponding tokens. Args: indices: Numeric representation. Usually is the result of the model, after a greedy decoding Returns: Corresponding tokens \"\"\" return [ self . itos [ it ] for it in indices ] def add_special_tokens ( self , tokens : List [ str ]) -> List [ str ]: \"\"\"Function to add the special start and end tokens to some tokenized text. Args: tokens: Tokenized text Returns: Text with the special tokens added. \"\"\" if self . start_token is not None : tokens = [ self . start_token ] + tokens if self . end_token is not None : tokens = tokens + [ self . end_token ] return tokens @torch . jit . export def remove_special_tokens ( self , text : str ) -> str : \"\"\"Function to remove the special tokens from the prediction. Args: text: Decoded text Returns: Text with the special tokens removed. \"\"\" text = text . replace ( self . blank_token , \"\" ) text = text . replace ( self . pad_token , \"\" ) if self . start_token is not None : text = text . replace ( self . start_token , \"\" ) if self . end_token is not None : text = text . replace ( self . end_token , \"\" ) return text __init__ ( tokens , blank_token = '<blank>' , pad_token = None , unknown_token = None , start_token = None , end_token = None ) Class that represents a vocabulary, with the related methods to numericalize a sequence of tokens into numbers, and do the reverse mapping of numbers back to tokens. PARAMETER DESCRIPTION tokens Basic list of tokens that will be part of the vocabulary. Check docs TYPE: List [ str ] blank_token Token that will represent the ctc blank. TYPE: str DEFAULT: '<blank>' pad_token Token that will represent padding, might also act as the ctc blank. TYPE: Optional [ str ] DEFAULT: None unknown_token Token that will represent unknown elements. Notice that this is different than the blank used by ctc. TYPE: Optional [ str ] DEFAULT: None start_token Token that will represent the beginning of the sequence. TYPE: Optional [ str ] DEFAULT: None end_token Token that will represent the end of the sequence. TYPE: Optional [ str ] DEFAULT: None Source code in thunder/text_processing/vocab.py 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 def __init__ ( self , tokens : List [ str ], blank_token : str = \"<blank>\" , pad_token : Optional [ str ] = None , unknown_token : Optional [ str ] = None , start_token : Optional [ str ] = None , end_token : Optional [ str ] = None , ): \"\"\"Class that represents a vocabulary, with the related methods to numericalize a sequence of tokens into numbers, and do the reverse mapping of numbers back to tokens. Args: tokens: Basic list of tokens that will be part of the vocabulary. Check [`docs`](https://scart97.github.io/thunder-speech/quick%20reference%20guide/#how-to-get-the-tokens-from-my-dataset) blank_token: Token that will represent the ctc blank. pad_token: Token that will represent padding, might also act as the ctc blank. unknown_token: Token that will represent unknown elements. Notice that this is different than the blank used by ctc. start_token: Token that will represent the beginning of the sequence. end_token: Token that will represent the end of the sequence. \"\"\" super () . __init__ () self . unknown_token = unknown_token self . start_token = start_token self . end_token = end_token self . blank_token = blank_token self . pad_token = pad_token or blank_token self . itos = tokens self . _maybe_add_token ( blank_token ) self . _maybe_add_token ( pad_token ) self . _maybe_add_token ( unknown_token ) self . _maybe_add_token ( start_token ) self . _maybe_add_token ( end_token ) self . stoi = { token : i for i , token in enumerate ( self . itos )} self . blank_idx = self . itos . index ( self . blank_token ) self . pad_idx = self . itos . index ( self . pad_token ) self . _unk_idx = - 1 if self . unknown_token is not None : self . _unk_idx = self . itos . index ( self . unknown_token ) add_special_tokens ( tokens ) Function to add the special start and end tokens to some tokenized text. PARAMETER DESCRIPTION tokens Tokenized text TYPE: List [ str ] RETURNS DESCRIPTION List [ str ] Text with the special tokens added. Source code in thunder/text_processing/vocab.py 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 def add_special_tokens ( self , tokens : List [ str ]) -> List [ str ]: \"\"\"Function to add the special start and end tokens to some tokenized text. Args: tokens: Tokenized text Returns: Text with the special tokens added. \"\"\" if self . start_token is not None : tokens = [ self . start_token ] + tokens if self . end_token is not None : tokens = tokens + [ self . end_token ] return tokens decode_into_text ( indices ) Function to transform back a list of numbers into the corresponding tokens. PARAMETER DESCRIPTION indices Numeric representation. Usually is the result of the model, after a greedy decoding TYPE: torch . Tensor RETURNS DESCRIPTION List [ str ] Corresponding tokens Source code in thunder/text_processing/vocab.py 85 86 87 88 89 90 91 92 93 94 95 96 @torch . jit . export def decode_into_text ( self , indices : torch . Tensor ) -> List [ str ]: \"\"\"Function to transform back a list of numbers into the corresponding tokens. Args: indices: Numeric representation. Usually is the result of the model, after a greedy decoding Returns: Corresponding tokens \"\"\" return [ self . itos [ it ] for it in indices ] numericalize ( tokens ) Function to transform a list of tokens into the corresponding numeric representation. PARAMETER DESCRIPTION tokens A single list of tokens to be transformed TYPE: List [ str ] RETURNS DESCRIPTION torch . Tensor The corresponding numeric representation Source code in thunder/text_processing/vocab.py 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 def numericalize ( self , tokens : List [ str ]) -> torch . Tensor : \"\"\"Function to transform a list of tokens into the corresponding numeric representation. Args: tokens: A single list of tokens to be transformed Returns: The corresponding numeric representation \"\"\" if self . unknown_token is None : # When in there's no unknown token # we filter out all of the tokens not in the vocab tokens = [ t for t in tokens if t in self . itos ] return torch . tensor ( [ self . stoi . get ( it , self . _unk_idx ) for it in tokens ], dtype = torch . long ) remove_special_tokens ( text ) Function to remove the special tokens from the prediction. PARAMETER DESCRIPTION text Decoded text TYPE: str RETURNS DESCRIPTION str Text with the special tokens removed. Source code in thunder/text_processing/vocab.py 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 @torch . jit . export def remove_special_tokens ( self , text : str ) -> str : \"\"\"Function to remove the special tokens from the prediction. Args: text: Decoded text Returns: Text with the special tokens removed. \"\"\" text = text . replace ( self . blank_token , \"\" ) text = text . replace ( self . pad_token , \"\" ) if self . start_token is not None : text = text . replace ( self . start_token , \"\" ) if self . end_token is not None : text = text . replace ( self . end_token , \"\" ) return text","title":"Vocab"},{"location":"api/Text%20Processing/vocab/#thunder.text_processing.vocab.Vocabulary","text":"Bases: nn . Module Source code in thunder/text_processing/vocab.py 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 class Vocabulary ( nn . Module ): def __init__ ( self , tokens : List [ str ], blank_token : str = \"<blank>\" , pad_token : Optional [ str ] = None , unknown_token : Optional [ str ] = None , start_token : Optional [ str ] = None , end_token : Optional [ str ] = None , ): \"\"\"Class that represents a vocabulary, with the related methods to numericalize a sequence of tokens into numbers, and do the reverse mapping of numbers back to tokens. Args: tokens: Basic list of tokens that will be part of the vocabulary. Check [`docs`](https://scart97.github.io/thunder-speech/quick%20reference%20guide/#how-to-get-the-tokens-from-my-dataset) blank_token: Token that will represent the ctc blank. pad_token: Token that will represent padding, might also act as the ctc blank. unknown_token: Token that will represent unknown elements. Notice that this is different than the blank used by ctc. start_token: Token that will represent the beginning of the sequence. end_token: Token that will represent the end of the sequence. \"\"\" super () . __init__ () self . unknown_token = unknown_token self . start_token = start_token self . end_token = end_token self . blank_token = blank_token self . pad_token = pad_token or blank_token self . itos = tokens self . _maybe_add_token ( blank_token ) self . _maybe_add_token ( pad_token ) self . _maybe_add_token ( unknown_token ) self . _maybe_add_token ( start_token ) self . _maybe_add_token ( end_token ) self . stoi = { token : i for i , token in enumerate ( self . itos )} self . blank_idx = self . itos . index ( self . blank_token ) self . pad_idx = self . itos . index ( self . pad_token ) self . _unk_idx = - 1 if self . unknown_token is not None : self . _unk_idx = self . itos . index ( self . unknown_token ) def _maybe_add_token ( self , token : Optional [ str ]): # Only adds tokens if they are not optional # and are not included in the vocabulary already if token and ( token not in self . itos ): self . itos = self . itos + [ token ] def numericalize ( self , tokens : List [ str ]) -> torch . Tensor : \"\"\"Function to transform a list of tokens into the corresponding numeric representation. Args: tokens: A single list of tokens to be transformed Returns: The corresponding numeric representation \"\"\" if self . unknown_token is None : # When in there's no unknown token # we filter out all of the tokens not in the vocab tokens = [ t for t in tokens if t in self . itos ] return torch . tensor ( [ self . stoi . get ( it , self . _unk_idx ) for it in tokens ], dtype = torch . long ) @torch . jit . export def decode_into_text ( self , indices : torch . Tensor ) -> List [ str ]: \"\"\"Function to transform back a list of numbers into the corresponding tokens. Args: indices: Numeric representation. Usually is the result of the model, after a greedy decoding Returns: Corresponding tokens \"\"\" return [ self . itos [ it ] for it in indices ] def add_special_tokens ( self , tokens : List [ str ]) -> List [ str ]: \"\"\"Function to add the special start and end tokens to some tokenized text. Args: tokens: Tokenized text Returns: Text with the special tokens added. \"\"\" if self . start_token is not None : tokens = [ self . start_token ] + tokens if self . end_token is not None : tokens = tokens + [ self . end_token ] return tokens @torch . jit . export def remove_special_tokens ( self , text : str ) -> str : \"\"\"Function to remove the special tokens from the prediction. Args: text: Decoded text Returns: Text with the special tokens removed. \"\"\" text = text . replace ( self . blank_token , \"\" ) text = text . replace ( self . pad_token , \"\" ) if self . start_token is not None : text = text . replace ( self . start_token , \"\" ) if self . end_token is not None : text = text . replace ( self . end_token , \"\" ) return text","title":"Vocabulary"},{"location":"api/Text%20Processing/vocab/#thunder.text_processing.vocab.Vocabulary.__init__","text":"Class that represents a vocabulary, with the related methods to numericalize a sequence of tokens into numbers, and do the reverse mapping of numbers back to tokens. PARAMETER DESCRIPTION tokens Basic list of tokens that will be part of the vocabulary. Check docs TYPE: List [ str ] blank_token Token that will represent the ctc blank. TYPE: str DEFAULT: '<blank>' pad_token Token that will represent padding, might also act as the ctc blank. TYPE: Optional [ str ] DEFAULT: None unknown_token Token that will represent unknown elements. Notice that this is different than the blank used by ctc. TYPE: Optional [ str ] DEFAULT: None start_token Token that will represent the beginning of the sequence. TYPE: Optional [ str ] DEFAULT: None end_token Token that will represent the end of the sequence. TYPE: Optional [ str ] DEFAULT: None Source code in thunder/text_processing/vocab.py 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 def __init__ ( self , tokens : List [ str ], blank_token : str = \"<blank>\" , pad_token : Optional [ str ] = None , unknown_token : Optional [ str ] = None , start_token : Optional [ str ] = None , end_token : Optional [ str ] = None , ): \"\"\"Class that represents a vocabulary, with the related methods to numericalize a sequence of tokens into numbers, and do the reverse mapping of numbers back to tokens. Args: tokens: Basic list of tokens that will be part of the vocabulary. Check [`docs`](https://scart97.github.io/thunder-speech/quick%20reference%20guide/#how-to-get-the-tokens-from-my-dataset) blank_token: Token that will represent the ctc blank. pad_token: Token that will represent padding, might also act as the ctc blank. unknown_token: Token that will represent unknown elements. Notice that this is different than the blank used by ctc. start_token: Token that will represent the beginning of the sequence. end_token: Token that will represent the end of the sequence. \"\"\" super () . __init__ () self . unknown_token = unknown_token self . start_token = start_token self . end_token = end_token self . blank_token = blank_token self . pad_token = pad_token or blank_token self . itos = tokens self . _maybe_add_token ( blank_token ) self . _maybe_add_token ( pad_token ) self . _maybe_add_token ( unknown_token ) self . _maybe_add_token ( start_token ) self . _maybe_add_token ( end_token ) self . stoi = { token : i for i , token in enumerate ( self . itos )} self . blank_idx = self . itos . index ( self . blank_token ) self . pad_idx = self . itos . index ( self . pad_token ) self . _unk_idx = - 1 if self . unknown_token is not None : self . _unk_idx = self . itos . index ( self . unknown_token )","title":"__init__()"},{"location":"api/Text%20Processing/vocab/#thunder.text_processing.vocab.Vocabulary.add_special_tokens","text":"Function to add the special start and end tokens to some tokenized text. PARAMETER DESCRIPTION tokens Tokenized text TYPE: List [ str ] RETURNS DESCRIPTION List [ str ] Text with the special tokens added. Source code in thunder/text_processing/vocab.py 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 def add_special_tokens ( self , tokens : List [ str ]) -> List [ str ]: \"\"\"Function to add the special start and end tokens to some tokenized text. Args: tokens: Tokenized text Returns: Text with the special tokens added. \"\"\" if self . start_token is not None : tokens = [ self . start_token ] + tokens if self . end_token is not None : tokens = tokens + [ self . end_token ] return tokens","title":"add_special_tokens()"},{"location":"api/Text%20Processing/vocab/#thunder.text_processing.vocab.Vocabulary.decode_into_text","text":"Function to transform back a list of numbers into the corresponding tokens. PARAMETER DESCRIPTION indices Numeric representation. Usually is the result of the model, after a greedy decoding TYPE: torch . Tensor RETURNS DESCRIPTION List [ str ] Corresponding tokens Source code in thunder/text_processing/vocab.py 85 86 87 88 89 90 91 92 93 94 95 96 @torch . jit . export def decode_into_text ( self , indices : torch . Tensor ) -> List [ str ]: \"\"\"Function to transform back a list of numbers into the corresponding tokens. Args: indices: Numeric representation. Usually is the result of the model, after a greedy decoding Returns: Corresponding tokens \"\"\" return [ self . itos [ it ] for it in indices ]","title":"decode_into_text()"},{"location":"api/Text%20Processing/vocab/#thunder.text_processing.vocab.Vocabulary.numericalize","text":"Function to transform a list of tokens into the corresponding numeric representation. PARAMETER DESCRIPTION tokens A single list of tokens to be transformed TYPE: List [ str ] RETURNS DESCRIPTION torch . Tensor The corresponding numeric representation Source code in thunder/text_processing/vocab.py 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 def numericalize ( self , tokens : List [ str ]) -> torch . Tensor : \"\"\"Function to transform a list of tokens into the corresponding numeric representation. Args: tokens: A single list of tokens to be transformed Returns: The corresponding numeric representation \"\"\" if self . unknown_token is None : # When in there's no unknown token # we filter out all of the tokens not in the vocab tokens = [ t for t in tokens if t in self . itos ] return torch . tensor ( [ self . stoi . get ( it , self . _unk_idx ) for it in tokens ], dtype = torch . long )","title":"numericalize()"},{"location":"api/Text%20Processing/vocab/#thunder.text_processing.vocab.Vocabulary.remove_special_tokens","text":"Function to remove the special tokens from the prediction. PARAMETER DESCRIPTION text Decoded text TYPE: str RETURNS DESCRIPTION str Text with the special tokens removed. Source code in thunder/text_processing/vocab.py 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 @torch . jit . export def remove_special_tokens ( self , text : str ) -> str : \"\"\"Function to remove the special tokens from the prediction. Args: text: Decoded text Returns: Text with the special tokens removed. \"\"\" text = text . replace ( self . blank_token , \"\" ) text = text . replace ( self . pad_token , \"\" ) if self . start_token is not None : text = text . replace ( self . start_token , \"\" ) if self . end_token is not None : text = text . replace ( self . end_token , \"\" ) return text","title":"remove_special_tokens()"}]}