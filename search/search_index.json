{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Thunder speech A Hackable speech recognition library. Note This project has been set up using PyScaffold 3.3. For details and usage information on PyScaffold see https://pyscaffold.org/.","title":"Home"},{"location":"#thunder-speech","text":"A Hackable speech recognition library.","title":"Thunder speech"},{"location":"#note","text":"This project has been set up using PyScaffold 3.3. For details and usage information on PyScaffold see https://pyscaffold.org/.","title":"Note"},{"location":"api/Quartznet/blocks/","text":"InitMode Weight init methods. Used by init_weights . Note Possible values are xavier_uniform , xavier_normal , kaiming_uniform and kaiming_normal QuartznetBlock __init__ ( self , inplanes , planes , repeat = 5 , kernel_size = [ 11 ], stride = [ 1 ], dilation = [ 1 ], dropout = 0.0 , residual = True , separable = False ) special Quartznet block. This is a refactoring of the Jasperblock present on the NeMo toolkit, but simplified to only support the new quartznet model. Biggest change is that dense residual used on Jasper is not supported here, and masked convolutions were also removed. Parameters: Name Type Description Default inplanes int Number of input planes required planes int Number of output planes required repeat int Repetitions inside block. 5 kernel_size List[int] Kernel size. [11] stride List[int] Stride of each repetition. [1] dilation List[int] Dilation of each repetition. [1] dropout float Dropout used before each activation. 0.0 residual bool Controls the use of residual connection. True separable bool Controls the use of separable convolutions. False Source code in thunder/quartznet/blocks.py def __init__ ( self , inplanes : int , planes : int , repeat : int = 5 , kernel_size : List [ int ] = [ 11 ], stride : List [ int ] = [ 1 ], dilation : List [ int ] = [ 1 ], dropout : float = 0.0 , residual : bool = True , separable : bool = False , ): \"\"\"Quartznet block. This is a refactoring of the Jasperblock present on the NeMo toolkit, but simplified to only support the new quartznet model. Biggest change is that dense residual used on Jasper is not supported here, and masked convolutions were also removed. Args: inplanes : Number of input planes planes : Number of output planes repeat : Repetitions inside block. kernel_size : Kernel size. stride : Stride of each repetition. dilation : Dilation of each repetition. dropout : Dropout used before each activation. residual : Controls the use of residual connection. separable : Controls the use of separable convolutions. \"\"\" super () . __init__ () padding_val = get_same_padding ( kernel_size [ 0 ], stride [ 0 ], dilation [ 0 ]) inplanes_loop = inplanes conv = [] for _ in range ( repeat - 1 ): conv . extend ( self . _get_conv_bn_layer ( inplanes_loop , planes , kernel_size = kernel_size , stride = stride , dilation = dilation , padding = padding_val , separable = separable , bias = False , ) ) conv . extend ( self . _get_act_dropout_layer ( drop_prob = dropout )) inplanes_loop = planes conv . extend ( self . _get_conv_bn_layer ( inplanes_loop , planes , kernel_size = kernel_size , stride = stride , dilation = dilation , padding = padding_val , separable = separable , bias = False , ) ) self . mconv = nn . Sequential ( * conv ) if residual : stride_residual = stride if stride [ 0 ] == 1 else stride [ 0 ] ** repeat self . res = nn . Sequential ( * self . _get_conv_bn_layer ( inplanes , planes , kernel_size = 1 , stride = stride_residual , bias = False , ) ) else : self . res = None self . mout = nn . Sequential ( * self . _get_act_dropout_layer ( drop_prob = dropout )) forward ( self , x ) Parameters: Name Type Description Default x Tensor Tensor of shape (batch, features, time) where #features == inplanes required Returns: Type Description Tensor Result of applying the block on the input Source code in thunder/quartznet/blocks.py def forward ( self , x : torch . Tensor ) -> torch . Tensor : \"\"\" Args: x : Tensor of shape (batch, features, time) where #features == inplanes Returns: Result of applying the block on the input \"\"\" # compute forward convolutions out = self . mconv ( x ) # compute the residuals if self . res is not None : res_out = self . res ( x ) out = out + res_out # compute the output out = self . mout ( out ) return out get_same_padding ( kernel_size , stride , dilation ) Calculates the padding size to obtain same padding. Same padding means that the output will have the shape input_shape / stride. That means, for stride = 1 the output shape is the same as the input, and stride = 2 gives an output that is half of the input shape. Parameters: Name Type Description Default kernel_size int convolution kernel size. Only tested to be correct with odd values. required stride int convolution stride required dilation int convolution dilation required Exceptions: Type Description ValueError Only stride or dilation may be greater than 1 Returns: Type Description int padding value to obtain same padding. Source code in thunder/quartznet/blocks.py def get_same_padding ( kernel_size : int , stride : int , dilation : int ) -> int : \"\"\"Calculates the padding size to obtain same padding. Same padding means that the output will have the shape input_shape / stride. That means, for stride = 1 the output shape is the same as the input, and stride = 2 gives an output that is half of the input shape. Args: kernel_size : convolution kernel size. Only tested to be correct with odd values. stride : convolution stride dilation : convolution dilation Raises: ValueError: Only stride or dilation may be greater than 1 Returns: padding value to obtain same padding. \"\"\" if stride > 1 and dilation > 1 : raise ValueError ( \"Only stride OR dilation may be greater than 1\" ) if dilation > 1 : return ( dilation * ( kernel_size - 1 ) + 1 ) // 2 return kernel_size // 2 init_weights ( m , mode =< InitMode . xavier_uniform : 'xavier_uniform' > ) Initialize Linear, MaskedConv1d/Conv1d or BatchNorm1d weights. There's no return, the operation occurs inplace. Parameters: Name Type Description Default m Module The layer to be initialized required mode InitMode Weight initialization mode. Only applicable to linear and conv layers. <InitMode.xavier_uniform: 'xavier_uniform'> Exceptions: Type Description ValueError Raised when the initial mode is not one of the possible options. Source code in thunder/quartznet/blocks.py def init_weights ( m : nn . Module , mode : InitMode = InitMode . xavier_uniform ): \"\"\"Initialize Linear, MaskedConv1d/Conv1d or BatchNorm1d weights. There's no return, the operation occurs inplace. Args: m: The layer to be initialized mode: Weight initialization mode. Only applicable to linear and conv layers. Raises: ValueError: Raised when the initial mode is not one of the possible options. \"\"\" if isinstance ( m , ( nn . Conv1d , nn . Linear )): if mode == InitMode . xavier_uniform : nn . init . xavier_uniform_ ( m . weight , gain = 1.0 ) elif mode == InitMode . xavier_normal : nn . init . xavier_normal_ ( m . weight , gain = 1.0 ) elif mode == InitMode . kaiming_uniform : nn . init . kaiming_uniform_ ( m . weight , nonlinearity = \"relu\" ) elif mode == InitMode . kaiming_normal : nn . init . kaiming_normal_ ( m . weight , nonlinearity = \"relu\" ) else : raise ValueError ( \"Unknown Initialization mode: {0} \" . format ( mode )) elif isinstance ( m , nn . BatchNorm1d ): if m . track_running_stats : m . running_mean . zero_ () m . running_var . fill_ ( 1 ) m . num_batches_tracked . zero_ () if m . affine : nn . init . ones_ ( m . weight ) nn . init . zeros_ ( m . bias )","title":"Blocks"},{"location":"api/Quartznet/blocks/#thunder.quartznet.blocks","text":"","title":"thunder.quartznet.blocks"},{"location":"api/Quartznet/blocks/#thunder.quartznet.blocks.InitMode","text":"Weight init methods. Used by init_weights . Note Possible values are xavier_uniform , xavier_normal , kaiming_uniform and kaiming_normal","title":"InitMode"},{"location":"api/Quartznet/blocks/#thunder.quartznet.blocks.QuartznetBlock","text":"","title":"QuartznetBlock"},{"location":"api/Quartznet/blocks/#thunder.quartznet.blocks.QuartznetBlock.__init__","text":"Quartznet block. This is a refactoring of the Jasperblock present on the NeMo toolkit, but simplified to only support the new quartznet model. Biggest change is that dense residual used on Jasper is not supported here, and masked convolutions were also removed. Parameters: Name Type Description Default inplanes int Number of input planes required planes int Number of output planes required repeat int Repetitions inside block. 5 kernel_size List[int] Kernel size. [11] stride List[int] Stride of each repetition. [1] dilation List[int] Dilation of each repetition. [1] dropout float Dropout used before each activation. 0.0 residual bool Controls the use of residual connection. True separable bool Controls the use of separable convolutions. False Source code in thunder/quartznet/blocks.py def __init__ ( self , inplanes : int , planes : int , repeat : int = 5 , kernel_size : List [ int ] = [ 11 ], stride : List [ int ] = [ 1 ], dilation : List [ int ] = [ 1 ], dropout : float = 0.0 , residual : bool = True , separable : bool = False , ): \"\"\"Quartznet block. This is a refactoring of the Jasperblock present on the NeMo toolkit, but simplified to only support the new quartznet model. Biggest change is that dense residual used on Jasper is not supported here, and masked convolutions were also removed. Args: inplanes : Number of input planes planes : Number of output planes repeat : Repetitions inside block. kernel_size : Kernel size. stride : Stride of each repetition. dilation : Dilation of each repetition. dropout : Dropout used before each activation. residual : Controls the use of residual connection. separable : Controls the use of separable convolutions. \"\"\" super () . __init__ () padding_val = get_same_padding ( kernel_size [ 0 ], stride [ 0 ], dilation [ 0 ]) inplanes_loop = inplanes conv = [] for _ in range ( repeat - 1 ): conv . extend ( self . _get_conv_bn_layer ( inplanes_loop , planes , kernel_size = kernel_size , stride = stride , dilation = dilation , padding = padding_val , separable = separable , bias = False , ) ) conv . extend ( self . _get_act_dropout_layer ( drop_prob = dropout )) inplanes_loop = planes conv . extend ( self . _get_conv_bn_layer ( inplanes_loop , planes , kernel_size = kernel_size , stride = stride , dilation = dilation , padding = padding_val , separable = separable , bias = False , ) ) self . mconv = nn . Sequential ( * conv ) if residual : stride_residual = stride if stride [ 0 ] == 1 else stride [ 0 ] ** repeat self . res = nn . Sequential ( * self . _get_conv_bn_layer ( inplanes , planes , kernel_size = 1 , stride = stride_residual , bias = False , ) ) else : self . res = None self . mout = nn . Sequential ( * self . _get_act_dropout_layer ( drop_prob = dropout ))","title":"__init__()"},{"location":"api/Quartznet/blocks/#thunder.quartznet.blocks.QuartznetBlock.forward","text":"Parameters: Name Type Description Default x Tensor Tensor of shape (batch, features, time) where #features == inplanes required Returns: Type Description Tensor Result of applying the block on the input Source code in thunder/quartznet/blocks.py def forward ( self , x : torch . Tensor ) -> torch . Tensor : \"\"\" Args: x : Tensor of shape (batch, features, time) where #features == inplanes Returns: Result of applying the block on the input \"\"\" # compute forward convolutions out = self . mconv ( x ) # compute the residuals if self . res is not None : res_out = self . res ( x ) out = out + res_out # compute the output out = self . mout ( out ) return out","title":"forward()"},{"location":"api/Quartznet/blocks/#thunder.quartznet.blocks.get_same_padding","text":"Calculates the padding size to obtain same padding. Same padding means that the output will have the shape input_shape / stride. That means, for stride = 1 the output shape is the same as the input, and stride = 2 gives an output that is half of the input shape. Parameters: Name Type Description Default kernel_size int convolution kernel size. Only tested to be correct with odd values. required stride int convolution stride required dilation int convolution dilation required Exceptions: Type Description ValueError Only stride or dilation may be greater than 1 Returns: Type Description int padding value to obtain same padding. Source code in thunder/quartznet/blocks.py def get_same_padding ( kernel_size : int , stride : int , dilation : int ) -> int : \"\"\"Calculates the padding size to obtain same padding. Same padding means that the output will have the shape input_shape / stride. That means, for stride = 1 the output shape is the same as the input, and stride = 2 gives an output that is half of the input shape. Args: kernel_size : convolution kernel size. Only tested to be correct with odd values. stride : convolution stride dilation : convolution dilation Raises: ValueError: Only stride or dilation may be greater than 1 Returns: padding value to obtain same padding. \"\"\" if stride > 1 and dilation > 1 : raise ValueError ( \"Only stride OR dilation may be greater than 1\" ) if dilation > 1 : return ( dilation * ( kernel_size - 1 ) + 1 ) // 2 return kernel_size // 2","title":"get_same_padding()"},{"location":"api/Quartznet/blocks/#thunder.quartznet.blocks.init_weights","text":"Initialize Linear, MaskedConv1d/Conv1d or BatchNorm1d weights. There's no return, the operation occurs inplace. Parameters: Name Type Description Default m Module The layer to be initialized required mode InitMode Weight initialization mode. Only applicable to linear and conv layers. <InitMode.xavier_uniform: 'xavier_uniform'> Exceptions: Type Description ValueError Raised when the initial mode is not one of the possible options. Source code in thunder/quartznet/blocks.py def init_weights ( m : nn . Module , mode : InitMode = InitMode . xavier_uniform ): \"\"\"Initialize Linear, MaskedConv1d/Conv1d or BatchNorm1d weights. There's no return, the operation occurs inplace. Args: m: The layer to be initialized mode: Weight initialization mode. Only applicable to linear and conv layers. Raises: ValueError: Raised when the initial mode is not one of the possible options. \"\"\" if isinstance ( m , ( nn . Conv1d , nn . Linear )): if mode == InitMode . xavier_uniform : nn . init . xavier_uniform_ ( m . weight , gain = 1.0 ) elif mode == InitMode . xavier_normal : nn . init . xavier_normal_ ( m . weight , gain = 1.0 ) elif mode == InitMode . kaiming_uniform : nn . init . kaiming_uniform_ ( m . weight , nonlinearity = \"relu\" ) elif mode == InitMode . kaiming_normal : nn . init . kaiming_normal_ ( m . weight , nonlinearity = \"relu\" ) else : raise ValueError ( \"Unknown Initialization mode: {0} \" . format ( mode )) elif isinstance ( m , nn . BatchNorm1d ): if m . track_running_stats : m . running_mean . zero_ () m . running_var . fill_ ( 1 ) m . num_batches_tracked . zero_ () if m . affine : nn . init . ones_ ( m . weight ) nn . init . zeros_ ( m . bias )","title":"init_weights()"},{"location":"api/Quartznet/model/","text":"All of the stuff to load the quartznet checkpoint","title":"Model"},{"location":"api/Quartznet/model/#thunder.quartznet.model","text":"All of the stuff to load the quartznet checkpoint","title":"thunder.quartznet.model"}]}