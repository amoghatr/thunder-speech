{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Thunder speech A Hackable speech recognition library. What to expect from this project: End-to-end speech recognition models Simple fine-tuning to new languages Inference support as a first-class feature Developer oriented api What it's not: A general-purpose speech toolkit A collection of complex systems that require thousands of gpu-hours and expert knowledge, only focusing on the state-of-the-art results Quick usage guide Install Install the library from PyPI: pip install thunder-speech Optionally, if you want to train wav2vec 2.0: pip install thunder-speech[transformers] Import desired models from thunder.quartznet.module import QuartznetModule , QuartznetCheckpoint # Tab completion works to discover other QuartznetCheckpoint.* model = QuartznetModule . load_from_nemo ( QuartznetCheckpoint . QuartzNet5x5LS_En ) Load audio and predict import torchaudio audio , sr = torchaudio . load ( \"my_sample_file.wav\" ) transcriptions = model . predict ( audio ) # transcriptions is a list of strings with the captions. More quick tips If you want to know how to export the models using torchscript, access the raw probabilities and decode manually or fine-tune the models you can access the documentation here . Contributing The first step to contribute is to do an editable installation of the library: git clone https://github.com/scart97/thunder-speech.git cd thunder-speech pip install -e .[dev,testing] pre-commit install Then, make sure that everything is working. You can run the test suit, that is based on pytest: RUN_SLOW=1 pytest Here the RUN_SLOW flag is used to run all the tests, including the ones that might download checkpoints or do small training runs and are marked as slow. If you don't have a CUDA capable gpu, some tests will be unconditionally skipped. Influences This library has heavy influence of the best practices in the pytorch ecosystem. The original model code, including checkpoints, is based on the NeMo ASR toolkit. From there also came the inspiration for the fine-tuning and prediction api's. The data loading and processing is loosely based on my experience using fast.ai. It tries to decouple transforms that happen at the item level from the ones that are efficiently implemented for the whole batch at the GPU. Also, the idea that default parameters should be great. The overall organization of code and decoupling follows the pytorch-lightning ideals, with self-contained modules that try to reduce the boilerplate necessary. Finally, the transformers library inspired the simple model implementations, with a clear separation in folders containing the specific code that you need to understand each architecture and preprocessing, and their strong test suit. Note This project has been set up using PyScaffold 3.3. For details and usage information on PyScaffold see https://pyscaffold.org/.","title":"Home"},{"location":"#thunder-speech","text":"A Hackable speech recognition library. What to expect from this project: End-to-end speech recognition models Simple fine-tuning to new languages Inference support as a first-class feature Developer oriented api What it's not: A general-purpose speech toolkit A collection of complex systems that require thousands of gpu-hours and expert knowledge, only focusing on the state-of-the-art results","title":"Thunder speech"},{"location":"#quick-usage-guide","text":"","title":"Quick usage guide"},{"location":"#install","text":"Install the library from PyPI: pip install thunder-speech Optionally, if you want to train wav2vec 2.0: pip install thunder-speech[transformers]","title":"Install"},{"location":"#import-desired-models","text":"from thunder.quartznet.module import QuartznetModule , QuartznetCheckpoint # Tab completion works to discover other QuartznetCheckpoint.* model = QuartznetModule . load_from_nemo ( QuartznetCheckpoint . QuartzNet5x5LS_En )","title":"Import desired models"},{"location":"#load-audio-and-predict","text":"import torchaudio audio , sr = torchaudio . load ( \"my_sample_file.wav\" ) transcriptions = model . predict ( audio ) # transcriptions is a list of strings with the captions.","title":"Load audio and predict"},{"location":"#more-quick-tips","text":"If you want to know how to export the models using torchscript, access the raw probabilities and decode manually or fine-tune the models you can access the documentation here .","title":"More quick tips"},{"location":"#contributing","text":"The first step to contribute is to do an editable installation of the library: git clone https://github.com/scart97/thunder-speech.git cd thunder-speech pip install -e .[dev,testing] pre-commit install Then, make sure that everything is working. You can run the test suit, that is based on pytest: RUN_SLOW=1 pytest Here the RUN_SLOW flag is used to run all the tests, including the ones that might download checkpoints or do small training runs and are marked as slow. If you don't have a CUDA capable gpu, some tests will be unconditionally skipped.","title":"Contributing"},{"location":"#influences","text":"This library has heavy influence of the best practices in the pytorch ecosystem. The original model code, including checkpoints, is based on the NeMo ASR toolkit. From there also came the inspiration for the fine-tuning and prediction api's. The data loading and processing is loosely based on my experience using fast.ai. It tries to decouple transforms that happen at the item level from the ones that are efficiently implemented for the whole batch at the GPU. Also, the idea that default parameters should be great. The overall organization of code and decoupling follows the pytorch-lightning ideals, with self-contained modules that try to reduce the boilerplate necessary. Finally, the transformers library inspired the simple model implementations, with a clear separation in folders containing the specific code that you need to understand each architecture and preprocessing, and their strong test suit.","title":"Influences"},{"location":"#note","text":"This project has been set up using PyScaffold 3.3. For details and usage information on PyScaffold see https://pyscaffold.org/.","title":"Note"},{"location":"Ultimate%20guide/","text":"The ultimate guide to speech recognition (WIP) This guide has the purpose to give you all the steps necessary to achieve a decent (but not necessarily state-of-the-art) speech recognition system in a new language. Gathering the data Speech recognition systems are really sensitive to the quality of data used to train them. Also, they usually require from hundreads to thousands of hours depending on the quality expected. Some good sources for data are Mozilla commonvoice , the OpenSLR project or Tatoeba . After you download some initial data, there's a number of data quality problems that are expected and need to be fixed if you want to increase the performance of the trained models. First, list all the audio files by increasing size and check if there's any corrupted file (usually they're very small). Remove them from the training data. Then install sox , that's the best tool to inspect and convert audio files. It should come with a basic tool to inspect any file in the terminal, called soxi . As an example: $ soxi example_file.wav Input File : 'example_file.wav' Channels : 1 Sample Rate : 16000 Precision : 16-bit Duration : 00:00:04.27 = 94053 samples ~ 319.908 CDDA sectors File Size : 188k Bit Rate : 353k Sample Encoding: 16-bit Signed Integer PCM That's the usual format of files used in speech recognition research. Wav files, encoded with a 16-bit PCM codec and a sample rate of 16 kHz. The file format and codec can vary and will only affect the quality of the audio, but the sample rate is the essential one. Trained models only work with a specific sample rate, and any file with a different one must be resampled either at the file level or directly after loading with torchaudio. Sox has more capabilities than just listing audio metadata. It can read almost any file format and convert to others. If you have a mp3 file at 44.1 kHz, and want to convert into the usual wav format above, you can use: sox input_file.mp3 -r 16000 -c 1 -b 16 output_file.wav The flags used represent: -r 16000 : 16 kHz sample rate -c 1 : convert to mono (1 channel) -b 16 : convert to PCM 16-bit output_file.wav : Sox understand that the output will be wav just by the file extension Ideally all the training and inference audio files should have the same characteristics, so it's a good idea to transform them into a common format before training. As the wav format does not have any compression, the resulting data will demand a huge HDD space. If that's a problem, you can instead convert the files to mp3, that way you lose a small percentage of the performance but can achieve up to 10x smaller dataset sizes. Now take a look at the labels. We are searching for a number of different problems here: Strange symbols: can easily find if you list all unique characters in the dataset Text in another language: remove these files Additional info that should not be there, like speaker identification as part of the transcription (common in subtitles) Regional/temporal differences that can cause the same words to have multiple written forms: mixing data from multiple countries that speak the same language, or using labels that came from old books Try to fix those label problems, or remove them from the training set if you have lots of data. Don't spend weeks just looking at the data, but have a small subset that you can trust is properly cleaned, even if that means manually labeling again. After you train the first couple of models, it's possible to use the model itself to help find problems in the training data. Writing the dataset/datamodule TODO: fill this section with the nemo manifest example load source load audio load text fix text Expand contractions ( I'm becomes I am ) Expand numbers ( 42 becomes forty two ) Optionally remove punctuation datamodule with sources First train For this first train, you should only try to overfit one batch. This is the simplest test, and if you can't get past it then anything more complex that you try will be wasted time. To do it, try to load a training dataset with only one batch worth of data. The validation/test sets can be as usual, you will ignore them at this step. As we are using pytorch lightning, there's a trainer flag to limit the number of training batches ( limit_train_batches=1 ) that can be used. Also, remember to disable any shuffle at the dataloader, to ensure the same batch will be used every epoch. Before you run the training, disable any augmentation, regularization and advanced stuff like learning rate scheduling. You can start with either a pretrained model, or a clean new one, but either way don't freeze any parameters, just let it all train. Start the training, and you should see the loss follow a pattern where, the more time you let it run, the final value will be lower. This means that small bumps will happen, but it will always recover and keep going down. The ideal point is where you run the prediction on the batch that you overfit, and the model doesn't make a single mistake. Some problems that can happen: The loss is negative : There's a blank in the target text, find and remove it. Blanks should only be produced by the model, never at the labels. There're no predictions at all : let it train for more time Still, there're no predictions after a long time : Check if the target texts are being processed correctly. Inside the training step, decode the target text and assert that it returns what you expect The loss does a 'U' curve where it starts normally but then turns around and just keep increasing : try to lower the learning rate Second train Now repeat the first training, but with around 10 hours of data. This number depends on the hardware that you have available, but something that gives you 2 minute epochs is a good amount. This time, you're not trying to overfit anymore. The validation loss will start to get lower, and the metrics will improve compared to the first training. Quickly, the model will reach the point where the data is enough, and it will start to overfit to the training data. TODO: better graphs? Expected train loss: \\ \\ \\ \\ \\______ Expected val loss/metrics: \\ \\ \\ \\ / \\____/ Scaling to the whole dataset TODO: expand this section break long audios - more than 25s is usually bad Use the model to find problems Sort by loss descending and manually check the files Sort by CER descending and manually check the files Sort by CER ascending on the validation/test set to find possible data leak Watch for the loss spikes during training Reducing overfit TODO: expand this section fastai recipe https://youtu.be/4u8FxNEDUeg?t=1333 Add more data Add augmentation Regularization Deploy! TODO: expand this section torch jit Streamlit torchserve bentoml","title":"The ultimate guide to speech recognition (WIP)"},{"location":"Ultimate%20guide/#the-ultimate-guide-to-speech-recognition-wip","text":"This guide has the purpose to give you all the steps necessary to achieve a decent (but not necessarily state-of-the-art) speech recognition system in a new language.","title":"The ultimate guide to speech recognition (WIP)"},{"location":"Ultimate%20guide/#gathering-the-data","text":"Speech recognition systems are really sensitive to the quality of data used to train them. Also, they usually require from hundreads to thousands of hours depending on the quality expected. Some good sources for data are Mozilla commonvoice , the OpenSLR project or Tatoeba . After you download some initial data, there's a number of data quality problems that are expected and need to be fixed if you want to increase the performance of the trained models. First, list all the audio files by increasing size and check if there's any corrupted file (usually they're very small). Remove them from the training data. Then install sox , that's the best tool to inspect and convert audio files. It should come with a basic tool to inspect any file in the terminal, called soxi . As an example: $ soxi example_file.wav Input File : 'example_file.wav' Channels : 1 Sample Rate : 16000 Precision : 16-bit Duration : 00:00:04.27 = 94053 samples ~ 319.908 CDDA sectors File Size : 188k Bit Rate : 353k Sample Encoding: 16-bit Signed Integer PCM That's the usual format of files used in speech recognition research. Wav files, encoded with a 16-bit PCM codec and a sample rate of 16 kHz. The file format and codec can vary and will only affect the quality of the audio, but the sample rate is the essential one. Trained models only work with a specific sample rate, and any file with a different one must be resampled either at the file level or directly after loading with torchaudio. Sox has more capabilities than just listing audio metadata. It can read almost any file format and convert to others. If you have a mp3 file at 44.1 kHz, and want to convert into the usual wav format above, you can use: sox input_file.mp3 -r 16000 -c 1 -b 16 output_file.wav The flags used represent: -r 16000 : 16 kHz sample rate -c 1 : convert to mono (1 channel) -b 16 : convert to PCM 16-bit output_file.wav : Sox understand that the output will be wav just by the file extension Ideally all the training and inference audio files should have the same characteristics, so it's a good idea to transform them into a common format before training. As the wav format does not have any compression, the resulting data will demand a huge HDD space. If that's a problem, you can instead convert the files to mp3, that way you lose a small percentage of the performance but can achieve up to 10x smaller dataset sizes. Now take a look at the labels. We are searching for a number of different problems here: Strange symbols: can easily find if you list all unique characters in the dataset Text in another language: remove these files Additional info that should not be there, like speaker identification as part of the transcription (common in subtitles) Regional/temporal differences that can cause the same words to have multiple written forms: mixing data from multiple countries that speak the same language, or using labels that came from old books Try to fix those label problems, or remove them from the training set if you have lots of data. Don't spend weeks just looking at the data, but have a small subset that you can trust is properly cleaned, even if that means manually labeling again. After you train the first couple of models, it's possible to use the model itself to help find problems in the training data.","title":"Gathering the data"},{"location":"Ultimate%20guide/#writing-the-datasetdatamodule","text":"TODO: fill this section with the nemo manifest example load source load audio load text fix text Expand contractions ( I'm becomes I am ) Expand numbers ( 42 becomes forty two ) Optionally remove punctuation datamodule with sources","title":"Writing the dataset/datamodule"},{"location":"Ultimate%20guide/#first-train","text":"For this first train, you should only try to overfit one batch. This is the simplest test, and if you can't get past it then anything more complex that you try will be wasted time. To do it, try to load a training dataset with only one batch worth of data. The validation/test sets can be as usual, you will ignore them at this step. As we are using pytorch lightning, there's a trainer flag to limit the number of training batches ( limit_train_batches=1 ) that can be used. Also, remember to disable any shuffle at the dataloader, to ensure the same batch will be used every epoch. Before you run the training, disable any augmentation, regularization and advanced stuff like learning rate scheduling. You can start with either a pretrained model, or a clean new one, but either way don't freeze any parameters, just let it all train. Start the training, and you should see the loss follow a pattern where, the more time you let it run, the final value will be lower. This means that small bumps will happen, but it will always recover and keep going down. The ideal point is where you run the prediction on the batch that you overfit, and the model doesn't make a single mistake. Some problems that can happen: The loss is negative : There's a blank in the target text, find and remove it. Blanks should only be produced by the model, never at the labels. There're no predictions at all : let it train for more time Still, there're no predictions after a long time : Check if the target texts are being processed correctly. Inside the training step, decode the target text and assert that it returns what you expect The loss does a 'U' curve where it starts normally but then turns around and just keep increasing : try to lower the learning rate","title":"First train"},{"location":"Ultimate%20guide/#second-train","text":"Now repeat the first training, but with around 10 hours of data. This number depends on the hardware that you have available, but something that gives you 2 minute epochs is a good amount. This time, you're not trying to overfit anymore. The validation loss will start to get lower, and the metrics will improve compared to the first training. Quickly, the model will reach the point where the data is enough, and it will start to overfit to the training data. TODO: better graphs? Expected train loss: \\ \\ \\ \\ \\______ Expected val loss/metrics: \\ \\ \\ \\ / \\____/","title":"Second train"},{"location":"Ultimate%20guide/#scaling-to-the-whole-dataset","text":"TODO: expand this section break long audios - more than 25s is usually bad Use the model to find problems Sort by loss descending and manually check the files Sort by CER descending and manually check the files Sort by CER ascending on the validation/test set to find possible data leak Watch for the loss spikes during training","title":"Scaling to the whole dataset"},{"location":"Ultimate%20guide/#reducing-overfit","text":"TODO: expand this section fastai recipe https://youtu.be/4u8FxNEDUeg?t=1333 Add more data Add augmentation Regularization","title":"Reducing overfit"},{"location":"Ultimate%20guide/#deploy","text":"TODO: expand this section torch jit Streamlit torchserve bentoml","title":"Deploy!"},{"location":"quick%20reference%20guide/","text":"Quick reference guide How to export a Quartznet .nemo file to a pure pytorch model? from thunder.quartznet.module import QuartznetModule from thunder.data.dataset import AudioFileLoader import torch module = QuartznetModule . load_from_nemo ( \"/path/to/checkpoint.nemo\" ) module . to_torchscript ( \"model_ready_for_inference.pt\" ) # Optional step: also export audio loading pipeline loader = AudioFileLoader ( sample_rate = 16000 ) scripted_loader = torch . jit . script ( loader ) scripted_loader . save ( \"audio_loader.pt\" ) How to run inference on that exported file? import torch import torchaudio model = torch . jit . load ( \"model_ready_for_inference.pt\" ) loader = torch . jit . load ( \"audio_loader.pt\" ) # Open audio audio = loader ( \"audio_file.wav\" ) # transcriptions is a list of strings with the captions. transcriptions = model . predict ( audio ) What if I want the probabilities instead of the captions? Instead of model.predict(audio) , use just model(audio) import torch import torchaudio model = torch . jit . load ( \"model_ready_for_inference.pt\" ) loader = torch . jit . load ( \"audio_loader.pt\" ) # Open audio audio = loader ( \"audio_file.wav\" ) probs = model ( audio ) # If you also want the transcriptions: transcriptions = model . text_transform . decode_prediction ( probs . argmax ( 1 )) How to finetune a model if I already have the nemo manifests prepared? import pytorch_lightning as pl from thunder.data.datamodule import ManifestDatamodule from thunder.quartznet.module import QuartznetModule , QuartznetCheckpoint dm = ManifestDatamodule ( train_manifest = \"/path/to/train_manifest.json\" , val_manifest = \"/path/to/val_manifest.json\" , test_manifest = \"/path/to/test_manifest.json\" , ) # Tab completion works to discover other QuartznetCheckpoint.* model = QuartznetModule . load_from_nemo ( QuartznetCheckpoint . QuartzNet5x5LS_En ) trainer = pl . Trainer ( gpus =- 1 , # Use all gpus max_epochs = 10 , ) trainer . fit ( model = model , datamodule = dm ) How to get the initial_vocab_tokens from my dataset? from thunder.text_processing.tokenizer import char_tokenizer , get_most_frequent_tokens my_datamodule = CustomDatamodule ( ... ) my_datamodule . prepare_data () my_datamodule . setup ( None ) train_corpus = \" \" . join ( my_datamodule . train_dataset . all_outputs ()) initial_vocab_tokens = get_most_frequent_tokens ( train_corpus , char_tokenizer )","title":"Quick reference guide"},{"location":"quick%20reference%20guide/#quick-reference-guide","text":"","title":"Quick reference guide"},{"location":"quick%20reference%20guide/#how-to-export-a-quartznet-nemo-file-to-a-pure-pytorch-model","text":"from thunder.quartznet.module import QuartznetModule from thunder.data.dataset import AudioFileLoader import torch module = QuartznetModule . load_from_nemo ( \"/path/to/checkpoint.nemo\" ) module . to_torchscript ( \"model_ready_for_inference.pt\" ) # Optional step: also export audio loading pipeline loader = AudioFileLoader ( sample_rate = 16000 ) scripted_loader = torch . jit . script ( loader ) scripted_loader . save ( \"audio_loader.pt\" )","title":"How to export a Quartznet .nemo file to a pure pytorch model?"},{"location":"quick%20reference%20guide/#how-to-run-inference-on-that-exported-file","text":"import torch import torchaudio model = torch . jit . load ( \"model_ready_for_inference.pt\" ) loader = torch . jit . load ( \"audio_loader.pt\" ) # Open audio audio = loader ( \"audio_file.wav\" ) # transcriptions is a list of strings with the captions. transcriptions = model . predict ( audio )","title":"How to run inference on that exported file?"},{"location":"quick%20reference%20guide/#what-if-i-want-the-probabilities-instead-of-the-captions","text":"Instead of model.predict(audio) , use just model(audio) import torch import torchaudio model = torch . jit . load ( \"model_ready_for_inference.pt\" ) loader = torch . jit . load ( \"audio_loader.pt\" ) # Open audio audio = loader ( \"audio_file.wav\" ) probs = model ( audio ) # If you also want the transcriptions: transcriptions = model . text_transform . decode_prediction ( probs . argmax ( 1 ))","title":"What if I want the probabilities instead of the captions?"},{"location":"quick%20reference%20guide/#how-to-finetune-a-model-if-i-already-have-the-nemo-manifests-prepared","text":"import pytorch_lightning as pl from thunder.data.datamodule import ManifestDatamodule from thunder.quartznet.module import QuartznetModule , QuartznetCheckpoint dm = ManifestDatamodule ( train_manifest = \"/path/to/train_manifest.json\" , val_manifest = \"/path/to/val_manifest.json\" , test_manifest = \"/path/to/test_manifest.json\" , ) # Tab completion works to discover other QuartznetCheckpoint.* model = QuartznetModule . load_from_nemo ( QuartznetCheckpoint . QuartzNet5x5LS_En ) trainer = pl . Trainer ( gpus =- 1 , # Use all gpus max_epochs = 10 , ) trainer . fit ( model = model , datamodule = dm )","title":"How to finetune a model if I already have the nemo manifests prepared?"},{"location":"quick%20reference%20guide/#how-to-get-the-initial_vocab_tokens-from-my-dataset","text":"from thunder.text_processing.tokenizer import char_tokenizer , get_most_frequent_tokens my_datamodule = CustomDatamodule ( ... ) my_datamodule . prepare_data () my_datamodule . setup ( None ) train_corpus = \" \" . join ( my_datamodule . train_dataset . all_outputs ()) initial_vocab_tokens = get_most_frequent_tokens ( train_corpus , char_tokenizer )","title":"How to get the initial_vocab_tokens from my dataset?"},{"location":"api/metrics/","text":"CER Metric to compute the character error rate of predictions during the training loop. Accepts lists of predictions and references, correctly accumulating the metrics and computing the final value when requested. Check EditBaseMetric for more details on the possible methods. update_func ( self , predicted , reference ) Compute the statistics used to compare two strings using character error rate. Parameters: Name Type Description Default predicted str Single model prediction required reference str Corresponding reference required Returns: Type Description Tuple[int, int] Tuple containing the pure edit distance between predicted and reference, and the normalizing factor. Source code in thunder/metrics.py def update_func ( self , predicted : str , reference : str ) -> Tuple [ int , int ]: \"\"\"Compute the statistics used to compare two strings using character error rate. Args: predicted : Single model prediction reference : Corresponding reference Returns: Tuple containing the pure edit distance between predicted and reference, and the normalizing factor. \"\"\" return _cer_update ( predicted , reference ) EditBaseMetric __init__ ( self , compute_on_step = True , dist_sync_on_step = False , process_group = None , dist_sync_fn = None ) special Base metric for computations based on edit distance. Parameters: Name Type Description Default compute_on_step bool Forward only calls update() and returns None if this is set to False. True dist_sync_on_step bool Synchronize metric state across processes at each forward() before returning the value at the step. False process_group Optional[Any] Specify the process group on which synchronization is called. default: None (which selects the entire world) None dist_sync_fn Callable Callback that performs the allgather operation on the metric state. When None, DDP will be used to perform the allgather. None Source code in thunder/metrics.py def __init__ ( self , compute_on_step : bool = True , dist_sync_on_step : bool = False , process_group : Optional [ Any ] = None , dist_sync_fn : Callable = None , ): \"\"\"Base metric for computations based on edit distance. Args: compute_on_step: Forward only calls update() and returns None if this is set to False. dist_sync_on_step: Synchronize metric state across processes at each forward() before returning the value at the step. process_group: Specify the process group on which synchronization is called. default: None (which selects the entire world) dist_sync_fn: Callback that performs the allgather operation on the metric state. When None, DDP will be used to perform the allgather. \"\"\" super () . __init__ ( compute_on_step = compute_on_step , dist_sync_on_step = dist_sync_on_step , process_group = process_group , dist_sync_fn = dist_sync_fn , ) self . add_state ( \"distance\" , default = tensor ( 0 ), dist_reduce_fx = \"sum\" ) self . add_state ( \"total\" , default = tensor ( 0 ), dist_reduce_fx = \"sum\" ) compute ( self ) Uses the aggregated counters to calculate the final metric value. Returns: Type Description Tensor Float tensor between 0.0 and 1.0 representing the error rate. Source code in thunder/metrics.py def compute ( self ) -> Tensor : \"\"\"Uses the aggregated counters to calculate the final metric value. Returns: Float tensor between 0.0 and 1.0 representing the error rate. \"\"\" return tensor ( _edit_compute ( self . distance , self . total )) update ( self , preds , target ) Method used to update the internal counters of the metric at every batch. Subclasses should leave this function untouched and implement update_func instead. Parameters: Name Type Description Default preds List[str] List of predictions of the model, already decoded into string form. required target List[str] List of corresponding references. required Source code in thunder/metrics.py def update ( self , preds : List [ str ], target : List [ str ]): \"\"\"Method used to update the internal counters of the metric at every batch. Subclasses should leave this function untouched and implement update_func instead. Args: preds : List of predictions of the model, already decoded into string form. target : List of corresponding references. \"\"\" # fmt: off assert len ( target ) > 0 , \"You need to pass at least one pair\" assert len ( preds ) == len ( target ), \"The number of predictions and targets must be the same\" # fmt: on for predicted , reference in zip ( preds , target ): distance , total = self . update_func ( predicted , reference ) self . distance += distance self . total += total update_func ( self , predicted , reference ) Function to calculate the statistics from one pair of elements. This function should take the two strings, split them according to characters/words/phonemes based on the metric implemented, calculate the edit distance between the two splitted strings and return also the normalizing factor, that is the number of elements in the splitted reference. Parameters: Name Type Description Default predicted str Single model prediction required reference str Corresponding reference required Returns: Type Description Tuple[int, int] Tuple containing the pure edit distance between predicted and reference, and the normalizing factor. Source code in thunder/metrics.py def update_func ( self , predicted : str , reference : str ) -> Tuple [ int , int ]: \"\"\"Function to calculate the statistics from one pair of elements. This function should take the two strings, split them according to characters/words/phonemes based on the metric implemented, calculate the edit distance between the two splitted strings and return also the normalizing factor, that is the number of elements in the splitted reference. Args: predicted : Single model prediction reference : Corresponding reference Returns: Tuple containing the pure edit distance between predicted and reference, and the normalizing factor. \"\"\" pass WER Metric to compute the word error rate of predictions during the training loop. Accepts lists of predictions and references, correctly accumulating the metrics and computing the final value when requested. Check EditBaseMetric for more details on the possible methods. update_func ( self , predicted , reference ) Compute the statistics used to compare two strings using word error rate. Parameters: Name Type Description Default predicted str Single model prediction required reference str Corresponding reference required Returns: Type Description Tuple[int, int] Tuple containing the pure edit distance between predicted and reference, and the normalizing factor. Source code in thunder/metrics.py def update_func ( self , predicted : str , reference : str ) -> Tuple [ int , int ]: \"\"\"Compute the statistics used to compare two strings using word error rate. Args: predicted : Single model prediction reference : Corresponding reference Returns: Tuple containing the pure edit distance between predicted and reference, and the normalizing factor. \"\"\" return _wer_update ( predicted , reference ) single_cer ( predicted , reference ) Computes the character error rate between one prediction and the corresponding reference. This is the functional form, and it's recommended to use inside the training loop the class CER instead. Parameters: Name Type Description Default predicted str Model prediction after decoding back to string required reference str Reference text required Returns: Type Description float Value between 0.0 and 1.0 that measures the error rate. Source code in thunder/metrics.py def single_cer ( predicted : str , reference : str ) -> float : \"\"\"Computes the character error rate between one prediction and the corresponding reference. This is the functional form, and it's recommended to use inside the training loop the class [`CER`][thunder.metrics.CER] instead. Args: predicted : Model prediction after decoding back to string reference : Reference text Returns: Value between 0.0 and 1.0 that measures the error rate. \"\"\" distance , total = _cer_update ( predicted , reference ) return _edit_compute ( distance , total ) single_wer ( predicted , reference ) Computes the word error rate between one prediction and the corresponding reference. This is the functional form, and it's recommended to use inside the training loop the class WER instead. Parameters: Name Type Description Default predicted str Model prediction after decoding back to string required reference str Reference text required Returns: Type Description float Value between 0.0 and 1.0 that measures the error rate. Source code in thunder/metrics.py def single_wer ( predicted : str , reference : str ) -> float : \"\"\"Computes the word error rate between one prediction and the corresponding reference. This is the functional form, and it's recommended to use inside the training loop the class [`WER`][thunder.metrics.WER] instead. Args: predicted : Model prediction after decoding back to string reference : Reference text Returns: Value between 0.0 and 1.0 that measures the error rate. \"\"\" distance , total = _wer_update ( predicted , reference ) return _edit_compute ( distance , total )","title":"Metrics"},{"location":"api/metrics/#thunder.metrics.CER","text":"Metric to compute the character error rate of predictions during the training loop. Accepts lists of predictions and references, correctly accumulating the metrics and computing the final value when requested. Check EditBaseMetric for more details on the possible methods.","title":"CER"},{"location":"api/metrics/#thunder.metrics.CER.update_func","text":"Compute the statistics used to compare two strings using character error rate. Parameters: Name Type Description Default predicted str Single model prediction required reference str Corresponding reference required Returns: Type Description Tuple[int, int] Tuple containing the pure edit distance between predicted and reference, and the normalizing factor. Source code in thunder/metrics.py def update_func ( self , predicted : str , reference : str ) -> Tuple [ int , int ]: \"\"\"Compute the statistics used to compare two strings using character error rate. Args: predicted : Single model prediction reference : Corresponding reference Returns: Tuple containing the pure edit distance between predicted and reference, and the normalizing factor. \"\"\" return _cer_update ( predicted , reference )","title":"update_func()"},{"location":"api/metrics/#thunder.metrics.EditBaseMetric","text":"","title":"EditBaseMetric"},{"location":"api/metrics/#thunder.metrics.EditBaseMetric.__init__","text":"Base metric for computations based on edit distance. Parameters: Name Type Description Default compute_on_step bool Forward only calls update() and returns None if this is set to False. True dist_sync_on_step bool Synchronize metric state across processes at each forward() before returning the value at the step. False process_group Optional[Any] Specify the process group on which synchronization is called. default: None (which selects the entire world) None dist_sync_fn Callable Callback that performs the allgather operation on the metric state. When None, DDP will be used to perform the allgather. None Source code in thunder/metrics.py def __init__ ( self , compute_on_step : bool = True , dist_sync_on_step : bool = False , process_group : Optional [ Any ] = None , dist_sync_fn : Callable = None , ): \"\"\"Base metric for computations based on edit distance. Args: compute_on_step: Forward only calls update() and returns None if this is set to False. dist_sync_on_step: Synchronize metric state across processes at each forward() before returning the value at the step. process_group: Specify the process group on which synchronization is called. default: None (which selects the entire world) dist_sync_fn: Callback that performs the allgather operation on the metric state. When None, DDP will be used to perform the allgather. \"\"\" super () . __init__ ( compute_on_step = compute_on_step , dist_sync_on_step = dist_sync_on_step , process_group = process_group , dist_sync_fn = dist_sync_fn , ) self . add_state ( \"distance\" , default = tensor ( 0 ), dist_reduce_fx = \"sum\" ) self . add_state ( \"total\" , default = tensor ( 0 ), dist_reduce_fx = \"sum\" )","title":"__init__()"},{"location":"api/metrics/#thunder.metrics.EditBaseMetric.compute","text":"Uses the aggregated counters to calculate the final metric value. Returns: Type Description Tensor Float tensor between 0.0 and 1.0 representing the error rate. Source code in thunder/metrics.py def compute ( self ) -> Tensor : \"\"\"Uses the aggregated counters to calculate the final metric value. Returns: Float tensor between 0.0 and 1.0 representing the error rate. \"\"\" return tensor ( _edit_compute ( self . distance , self . total ))","title":"compute()"},{"location":"api/metrics/#thunder.metrics.EditBaseMetric.update","text":"Method used to update the internal counters of the metric at every batch. Subclasses should leave this function untouched and implement update_func instead. Parameters: Name Type Description Default preds List[str] List of predictions of the model, already decoded into string form. required target List[str] List of corresponding references. required Source code in thunder/metrics.py def update ( self , preds : List [ str ], target : List [ str ]): \"\"\"Method used to update the internal counters of the metric at every batch. Subclasses should leave this function untouched and implement update_func instead. Args: preds : List of predictions of the model, already decoded into string form. target : List of corresponding references. \"\"\" # fmt: off assert len ( target ) > 0 , \"You need to pass at least one pair\" assert len ( preds ) == len ( target ), \"The number of predictions and targets must be the same\" # fmt: on for predicted , reference in zip ( preds , target ): distance , total = self . update_func ( predicted , reference ) self . distance += distance self . total += total","title":"update()"},{"location":"api/metrics/#thunder.metrics.EditBaseMetric.update_func","text":"Function to calculate the statistics from one pair of elements. This function should take the two strings, split them according to characters/words/phonemes based on the metric implemented, calculate the edit distance between the two splitted strings and return also the normalizing factor, that is the number of elements in the splitted reference. Parameters: Name Type Description Default predicted str Single model prediction required reference str Corresponding reference required Returns: Type Description Tuple[int, int] Tuple containing the pure edit distance between predicted and reference, and the normalizing factor. Source code in thunder/metrics.py def update_func ( self , predicted : str , reference : str ) -> Tuple [ int , int ]: \"\"\"Function to calculate the statistics from one pair of elements. This function should take the two strings, split them according to characters/words/phonemes based on the metric implemented, calculate the edit distance between the two splitted strings and return also the normalizing factor, that is the number of elements in the splitted reference. Args: predicted : Single model prediction reference : Corresponding reference Returns: Tuple containing the pure edit distance between predicted and reference, and the normalizing factor. \"\"\" pass","title":"update_func()"},{"location":"api/metrics/#thunder.metrics.WER","text":"Metric to compute the word error rate of predictions during the training loop. Accepts lists of predictions and references, correctly accumulating the metrics and computing the final value when requested. Check EditBaseMetric for more details on the possible methods.","title":"WER"},{"location":"api/metrics/#thunder.metrics.WER.update_func","text":"Compute the statistics used to compare two strings using word error rate. Parameters: Name Type Description Default predicted str Single model prediction required reference str Corresponding reference required Returns: Type Description Tuple[int, int] Tuple containing the pure edit distance between predicted and reference, and the normalizing factor. Source code in thunder/metrics.py def update_func ( self , predicted : str , reference : str ) -> Tuple [ int , int ]: \"\"\"Compute the statistics used to compare two strings using word error rate. Args: predicted : Single model prediction reference : Corresponding reference Returns: Tuple containing the pure edit distance between predicted and reference, and the normalizing factor. \"\"\" return _wer_update ( predicted , reference )","title":"update_func()"},{"location":"api/metrics/#thunder.metrics.single_cer","text":"Computes the character error rate between one prediction and the corresponding reference. This is the functional form, and it's recommended to use inside the training loop the class CER instead. Parameters: Name Type Description Default predicted str Model prediction after decoding back to string required reference str Reference text required Returns: Type Description float Value between 0.0 and 1.0 that measures the error rate. Source code in thunder/metrics.py def single_cer ( predicted : str , reference : str ) -> float : \"\"\"Computes the character error rate between one prediction and the corresponding reference. This is the functional form, and it's recommended to use inside the training loop the class [`CER`][thunder.metrics.CER] instead. Args: predicted : Model prediction after decoding back to string reference : Reference text Returns: Value between 0.0 and 1.0 that measures the error rate. \"\"\" distance , total = _cer_update ( predicted , reference ) return _edit_compute ( distance , total )","title":"single_cer()"},{"location":"api/metrics/#thunder.metrics.single_wer","text":"Computes the word error rate between one prediction and the corresponding reference. This is the functional form, and it's recommended to use inside the training loop the class WER instead. Parameters: Name Type Description Default predicted str Model prediction after decoding back to string required reference str Reference text required Returns: Type Description float Value between 0.0 and 1.0 that measures the error rate. Source code in thunder/metrics.py def single_wer ( predicted : str , reference : str ) -> float : \"\"\"Computes the word error rate between one prediction and the corresponding reference. This is the functional form, and it's recommended to use inside the training loop the class [`WER`][thunder.metrics.WER] instead. Args: predicted : Model prediction after decoding back to string reference : Reference text Returns: Value between 0.0 and 1.0 that measures the error rate. \"\"\" distance , total = _wer_update ( predicted , reference ) return _edit_compute ( distance , total )","title":"single_wer()"},{"location":"api/utils/","text":"BaseCheckpoint An enumeration. audio_len ( item ) Returns the length of the audio file Parameters: Name Type Description Default item Union[pathlib.Path, str] Audio path required Returns: Type Description float Lenght in seconds of the audio Source code in thunder/utils.py def audio_len ( item : Union [ Path , str ]) -> float : \"\"\"Returns the length of the audio file Args: item : Audio path Returns: Lenght in seconds of the audio \"\"\" metadata = torchaudio . info ( item ) return metadata . num_frames / metadata . sample_rate chain_calls ( * funcs ) Chain multiple functions that take only one argument, producing a new function that is the result of calling the individual functions in sequence. Examples: f1 = lambda x : 2 * x f2 = lambda x : 3 * x f3 = lambda x : 4 * x g = chain_calls ( f1 , f2 , f3 ) assert g ( 1 ) == 24 Returns: Type Description Callable Single chained function Source code in thunder/utils.py def chain_calls ( * funcs : List [ Callable ]) -> Callable : \"\"\"Chain multiple functions that take only one argument, producing a new function that is the result of calling the individual functions in sequence. Example: ```python f1 = lambda x: 2 * x f2 = lambda x: 3 * x f3 = lambda x: 4 * x g = chain_calls(f1, f2, f3) assert g(1) == 24 ``` Returns: Single chained function \"\"\" def call ( x , f ): return f ( x ) def _inner ( arg ): return functools . reduce ( call , funcs , arg ) return _inner default_list ( elements ) Function to create default values on dataclasses that are lists Parameters: Name Type Description Default elements List[~T] List of elements to be the default required Returns: Type Description List[~T] field compatible with the way dataclasses handle mutable defaults Source code in thunder/utils.py def default_list ( elements : List [ T ]) -> List [ T ]: \"\"\"Function to create default values on dataclasses that are lists Args: elements : List of elements to be the default Returns: field compatible with the way dataclasses handle mutable defaults \"\"\" return field ( default_factory = lambda : copy ( elements )) download_checkpoint ( name , checkpoint_folder = None ) Download checkpoint by identifier. Parameters: Name Type Description Default name BaseCheckpoint Model identifier. Check checkpoint_archives.keys() required checkpoint_folder str Folder where the checkpoint will be saved to. None Returns: Type Description Path Path to the saved checkpoint file. Source code in thunder/utils.py def download_checkpoint ( name : BaseCheckpoint , checkpoint_folder : str = None ) -> Path : \"\"\"Download checkpoint by identifier. Args: name: Model identifier. Check checkpoint_archives.keys() checkpoint_folder: Folder where the checkpoint will be saved to. Returns: Path to the saved checkpoint file. \"\"\" if checkpoint_folder is None : checkpoint_folder = get_default_cache_folder () url = name . value filename = url . split ( \"/\" )[ - 1 ] checkpoint_path = Path ( checkpoint_folder ) / filename if not checkpoint_path . exists (): wget . download ( url , out = str ( checkpoint_path )) return checkpoint_path get_default_cache_folder () Get the default folder where the cached stuff will be saved. Returns: Type Description Path Path of the cache folder. Source code in thunder/utils.py def get_default_cache_folder () -> Path : \"\"\"Get the default folder where the cached stuff will be saved. Returns: Path of the cache folder. \"\"\" folder = Path . home () / \".thunder\" folder . mkdir ( exist_ok = True ) return folder get_files ( directory , extension ) Find all files in directory with extension. Parameters: Name Type Description Default directory Union[str, pathlib.Path] Directory to recursively find the files required extension str File extension to search for required Returns: Type Description List[pathlib.Path] List of all the files that match the extension Source code in thunder/utils.py def get_files ( directory : Union [ str , Path ], extension : str ) -> List [ Path ]: \"\"\"Find all files in directory with extension. Args: directory : Directory to recursively find the files extension : File extension to search for Returns: List of all the files that match the extension \"\"\" files_found = [] for root , _ , files in os . walk ( directory , followlinks = True ): files_found += [ Path ( root ) / f for f in files if f . endswith ( extension )] return files_found","title":"Utils"},{"location":"api/utils/#thunder.utils.BaseCheckpoint","text":"An enumeration.","title":"BaseCheckpoint"},{"location":"api/utils/#thunder.utils.audio_len","text":"Returns the length of the audio file Parameters: Name Type Description Default item Union[pathlib.Path, str] Audio path required Returns: Type Description float Lenght in seconds of the audio Source code in thunder/utils.py def audio_len ( item : Union [ Path , str ]) -> float : \"\"\"Returns the length of the audio file Args: item : Audio path Returns: Lenght in seconds of the audio \"\"\" metadata = torchaudio . info ( item ) return metadata . num_frames / metadata . sample_rate","title":"audio_len()"},{"location":"api/utils/#thunder.utils.chain_calls","text":"Chain multiple functions that take only one argument, producing a new function that is the result of calling the individual functions in sequence. Examples: f1 = lambda x : 2 * x f2 = lambda x : 3 * x f3 = lambda x : 4 * x g = chain_calls ( f1 , f2 , f3 ) assert g ( 1 ) == 24 Returns: Type Description Callable Single chained function Source code in thunder/utils.py def chain_calls ( * funcs : List [ Callable ]) -> Callable : \"\"\"Chain multiple functions that take only one argument, producing a new function that is the result of calling the individual functions in sequence. Example: ```python f1 = lambda x: 2 * x f2 = lambda x: 3 * x f3 = lambda x: 4 * x g = chain_calls(f1, f2, f3) assert g(1) == 24 ``` Returns: Single chained function \"\"\" def call ( x , f ): return f ( x ) def _inner ( arg ): return functools . reduce ( call , funcs , arg ) return _inner","title":"chain_calls()"},{"location":"api/utils/#thunder.utils.default_list","text":"Function to create default values on dataclasses that are lists Parameters: Name Type Description Default elements List[~T] List of elements to be the default required Returns: Type Description List[~T] field compatible with the way dataclasses handle mutable defaults Source code in thunder/utils.py def default_list ( elements : List [ T ]) -> List [ T ]: \"\"\"Function to create default values on dataclasses that are lists Args: elements : List of elements to be the default Returns: field compatible with the way dataclasses handle mutable defaults \"\"\" return field ( default_factory = lambda : copy ( elements ))","title":"default_list()"},{"location":"api/utils/#thunder.utils.download_checkpoint","text":"Download checkpoint by identifier. Parameters: Name Type Description Default name BaseCheckpoint Model identifier. Check checkpoint_archives.keys() required checkpoint_folder str Folder where the checkpoint will be saved to. None Returns: Type Description Path Path to the saved checkpoint file. Source code in thunder/utils.py def download_checkpoint ( name : BaseCheckpoint , checkpoint_folder : str = None ) -> Path : \"\"\"Download checkpoint by identifier. Args: name: Model identifier. Check checkpoint_archives.keys() checkpoint_folder: Folder where the checkpoint will be saved to. Returns: Path to the saved checkpoint file. \"\"\" if checkpoint_folder is None : checkpoint_folder = get_default_cache_folder () url = name . value filename = url . split ( \"/\" )[ - 1 ] checkpoint_path = Path ( checkpoint_folder ) / filename if not checkpoint_path . exists (): wget . download ( url , out = str ( checkpoint_path )) return checkpoint_path","title":"download_checkpoint()"},{"location":"api/utils/#thunder.utils.get_default_cache_folder","text":"Get the default folder where the cached stuff will be saved. Returns: Type Description Path Path of the cache folder. Source code in thunder/utils.py def get_default_cache_folder () -> Path : \"\"\"Get the default folder where the cached stuff will be saved. Returns: Path of the cache folder. \"\"\" folder = Path . home () / \".thunder\" folder . mkdir ( exist_ok = True ) return folder","title":"get_default_cache_folder()"},{"location":"api/utils/#thunder.utils.get_files","text":"Find all files in directory with extension. Parameters: Name Type Description Default directory Union[str, pathlib.Path] Directory to recursively find the files required extension str File extension to search for required Returns: Type Description List[pathlib.Path] List of all the files that match the extension Source code in thunder/utils.py def get_files ( directory : Union [ str , Path ], extension : str ) -> List [ Path ]: \"\"\"Find all files in directory with extension. Args: directory : Directory to recursively find the files extension : File extension to search for Returns: List of all the files that match the extension \"\"\" files_found = [] for root , _ , files in os . walk ( directory , followlinks = True ): files_found += [ Path ( root ) / f for f in files if f . endswith ( extension )] return files_found","title":"get_files()"},{"location":"api/Citrinet/blocks/","text":"Basic building blocks to create the Citrinet model CitrinetBlock __init__ ( self , in_channels , out_channels , repeat = 5 , kernel_size = ( 11 ,), stride = ( 1 ,), dilation = ( 1 ,), dropout = 0.0 , residual = True , separable = False ) special Citrinet block. This is a refactoring of the Jasperblock present on the NeMo toolkit, but simplified to only support the new citrinet model. Biggest change is that dense residual used on Jasper is not supported here. Parameters: Name Type Description Default in_channels int Number of input channels required out_channels int Number of output channels required repeat int Repetitions inside block. 5 kernel_size Union[int, Tuple[int]] Kernel size. (11,) stride Union[int, Tuple[int]] Stride of each repetition. (1,) dilation Union[int, Tuple[int]] Dilation of each repetition. (1,) dropout float Dropout used before each activation. 0.0 residual bool Controls the use of residual connection. True separable bool Controls the use of separable convolutions. False Source code in thunder/citrinet/blocks.py def __init__ ( self , in_channels : int , out_channels : int , repeat : int = 5 , kernel_size : _size_1_t = ( 11 ,), stride : _size_1_t = ( 1 ,), dilation : _size_1_t = ( 1 ,), dropout : float = 0.0 , residual : bool = True , separable : bool = False , ): \"\"\"Citrinet block. This is a refactoring of the Jasperblock present on the NeMo toolkit, but simplified to only support the new citrinet model. Biggest change is that dense residual used on Jasper is not supported here. Args: in_channels : Number of input channels out_channels : Number of output channels repeat : Repetitions inside block. kernel_size : Kernel size. stride : Stride of each repetition. dilation : Dilation of each repetition. dropout : Dropout used before each activation. residual : Controls the use of residual connection. separable : Controls the use of separable convolutions. \"\"\" super () . __init__ () padding_val = get_same_padding ( kernel_size [ 0 ], 1 , dilation [ 0 ]) inplanes_loop = in_channels conv = [] for _ in range ( repeat - 1 ): conv . extend ( _get_conv_bn_layer ( inplanes_loop , out_channels , kernel_size = kernel_size , stride = ( 1 ,), # Only stride the last one dilation = dilation , padding = padding_val , separable = separable , bias = False , ) ) conv . extend ( _get_act_dropout_layer ( drop_prob = dropout )) inplanes_loop = out_channels padding_val = get_same_padding ( kernel_size [ 0 ], stride [ 0 ], dilation [ 0 ]) conv . extend ( _get_conv_bn_layer ( inplanes_loop , out_channels , kernel_size = kernel_size , stride = stride , dilation = dilation , padding = padding_val , separable = separable , bias = False , ) ) conv . append ( SqueezeExcite ( out_channels , reduction_ratio = 8 )) self . mconv = nn . Sequential ( * conv ) if residual : stride_residual = stride if stride [ 0 ] == 1 else stride [ 0 ] self . res = nn . Sequential ( * _get_conv_bn_layer ( in_channels , out_channels , kernel_size = 1 , stride = stride_residual , bias = False , ) ) else : self . res = None self . mout = nn . Sequential ( * _get_act_dropout_layer ( drop_prob = dropout )) forward ( self , x ) Parameters: Name Type Description Default x Tensor Tensor of shape (batch, features, time) where #features == inplanes required Returns: Type Description Tensor Result of applying the block on the input, and corresponding output lengths Source code in thunder/citrinet/blocks.py def forward ( self , x : torch . Tensor ) -> torch . Tensor : \"\"\" Args: x : Tensor of shape (batch, features, time) where #features == inplanes Returns: Result of applying the block on the input, and corresponding output lengths \"\"\" # compute forward convolutions out = self . mconv ( x ) # compute the residuals if self . res is not None : res_out = self . res ( x ) out = out + res_out # compute the output return self . mout ( out ) EncoderConfig dataclass Configuration to create Citrinet_encoder Attributes: Name Type Description filters List[int] List of filter sizes used to create the encoder blocks. required. kernel_sizes List[int] List of kernel sizes corresponding to each filter size. required. strides List[int] List of stride corresponding to each filter size. required. feat_in Number of input features to the model. defaults to 80. SqueezeExcite __init__ ( self , channels , reduction_ratio ) special Squeeze-and-Excitation sub-module. Parameters: Name Type Description Default channels int Input number of channels. required reduction_ratio int Reduction ratio for \"squeeze\" layer. required Source code in thunder/citrinet/blocks.py def __init__ ( self , channels : int , reduction_ratio : int , ): \"\"\" Squeeze-and-Excitation sub-module. Args: channels: Input number of channels. reduction_ratio: Reduction ratio for \"squeeze\" layer. \"\"\" super ( SqueezeExcite , self ) . __init__ () self . pool = nn . AdaptiveAvgPool1d ( 1 ) # context window = T self . fc = nn . Sequential ( nn . Linear ( channels , channels // reduction_ratio , bias = False ), nn . ReLU ( True ), nn . Linear ( channels // reduction_ratio , channels , bias = False ), ) forward ( self , x ) Parameters: Name Type Description Default x Tensor Tensor of shape [batch, channels, time] required Returns: Type Description Tensor Tensor of shape [batch, channels, time] Source code in thunder/citrinet/blocks.py def forward ( self , x : torch . Tensor ) -> torch . Tensor : \"\"\" Args: x : Tensor of shape [batch, channels, time] Returns: Tensor of shape [batch, channels, time] \"\"\" y = self . pool ( x ) # [B, C, T - context_window + 1] y = y . transpose ( 1 , - 1 ) # [B, T - context_window + 1, C] y = self . fc ( y ) # [B, T - context_window + 1, C] y = y . transpose ( 1 , - 1 ) # [B, C, T - context_window + 1] y = torch . sigmoid ( y ) return x * y body ( filters , kernel_size , strides ) Creates the body of the Citrinet model. That is the middle part. Parameters: Name Type Description Default filters List[int] List of filters inside each block in the body. required kernel_size List[int] Corresponding list of kernel sizes for each block. Should have the same length as the first argument. required strides List[int] Corresponding list of strides for each block. Should have the same length as the first argument. required Returns: Type Description List[thunder.citrinet.blocks.CitrinetBlock] List of layers that form the body of the network. Source code in thunder/citrinet/blocks.py def body ( filters : List [ int ], kernel_size : List [ int ], strides : List [ int ], ) -> List [ CitrinetBlock ]: \"\"\"Creates the body of the Citrinet model. That is the middle part. Args: filters : List of filters inside each block in the body. kernel_size : Corresponding list of kernel sizes for each block. Should have the same length as the first argument. strides: Corresponding list of strides for each block. Should have the same length as the first argument. Returns: List of layers that form the body of the network. \"\"\" layers = [] f_in = 256 for f , k , s in zip ( filters , kernel_size , strides ): layers . append ( CitrinetBlock ( f_in , f , kernel_size = ( k ,), stride = ( s ,), separable = True ) ) f_in = f layers . append ( CitrinetBlock ( f_in , 640 , repeat = 1 , kernel_size = ( 41 ,), residual = False , separable = True , ) ) return layers Citrinet_encoder ( cfg ) Basic Citrinet encoder setup. Parameters: Name Type Description Default cfg EncoderConfig required config to create instance required Returns: Type Description Module Pytorch model corresponding to the encoder. Source code in thunder/citrinet/blocks.py def Citrinet_encoder ( cfg : EncoderConfig ) -> nn . Module : \"\"\"Basic Citrinet encoder setup. Args: cfg: required config to create instance Returns: Pytorch model corresponding to the encoder. \"\"\" return nn . Sequential ( stem ( cfg . feat_in ), * body ( cfg . filters , cfg . kernel_sizes , cfg . strides ), ) stem ( feat_in ) Creates the Citrinet stem. That is the first block of the model, that process the input directly. Parameters: Name Type Description Default feat_in int Number of input features required Returns: Type Description CitrinetBlock Citrinet stem block Source code in thunder/citrinet/blocks.py def stem ( feat_in : int ) -> CitrinetBlock : \"\"\"Creates the Citrinet stem. That is the first block of the model, that process the input directly. Args: feat_in : Number of input features Returns: Citrinet stem block \"\"\" return CitrinetBlock ( feat_in , 256 , repeat = 1 , kernel_size = ( 5 ,), residual = False , separable = True , )","title":"Blocks"},{"location":"api/Citrinet/blocks/#thunder.citrinet.blocks.CitrinetBlock","text":"","title":"CitrinetBlock"},{"location":"api/Citrinet/blocks/#thunder.citrinet.blocks.CitrinetBlock.__init__","text":"Citrinet block. This is a refactoring of the Jasperblock present on the NeMo toolkit, but simplified to only support the new citrinet model. Biggest change is that dense residual used on Jasper is not supported here. Parameters: Name Type Description Default in_channels int Number of input channels required out_channels int Number of output channels required repeat int Repetitions inside block. 5 kernel_size Union[int, Tuple[int]] Kernel size. (11,) stride Union[int, Tuple[int]] Stride of each repetition. (1,) dilation Union[int, Tuple[int]] Dilation of each repetition. (1,) dropout float Dropout used before each activation. 0.0 residual bool Controls the use of residual connection. True separable bool Controls the use of separable convolutions. False Source code in thunder/citrinet/blocks.py def __init__ ( self , in_channels : int , out_channels : int , repeat : int = 5 , kernel_size : _size_1_t = ( 11 ,), stride : _size_1_t = ( 1 ,), dilation : _size_1_t = ( 1 ,), dropout : float = 0.0 , residual : bool = True , separable : bool = False , ): \"\"\"Citrinet block. This is a refactoring of the Jasperblock present on the NeMo toolkit, but simplified to only support the new citrinet model. Biggest change is that dense residual used on Jasper is not supported here. Args: in_channels : Number of input channels out_channels : Number of output channels repeat : Repetitions inside block. kernel_size : Kernel size. stride : Stride of each repetition. dilation : Dilation of each repetition. dropout : Dropout used before each activation. residual : Controls the use of residual connection. separable : Controls the use of separable convolutions. \"\"\" super () . __init__ () padding_val = get_same_padding ( kernel_size [ 0 ], 1 , dilation [ 0 ]) inplanes_loop = in_channels conv = [] for _ in range ( repeat - 1 ): conv . extend ( _get_conv_bn_layer ( inplanes_loop , out_channels , kernel_size = kernel_size , stride = ( 1 ,), # Only stride the last one dilation = dilation , padding = padding_val , separable = separable , bias = False , ) ) conv . extend ( _get_act_dropout_layer ( drop_prob = dropout )) inplanes_loop = out_channels padding_val = get_same_padding ( kernel_size [ 0 ], stride [ 0 ], dilation [ 0 ]) conv . extend ( _get_conv_bn_layer ( inplanes_loop , out_channels , kernel_size = kernel_size , stride = stride , dilation = dilation , padding = padding_val , separable = separable , bias = False , ) ) conv . append ( SqueezeExcite ( out_channels , reduction_ratio = 8 )) self . mconv = nn . Sequential ( * conv ) if residual : stride_residual = stride if stride [ 0 ] == 1 else stride [ 0 ] self . res = nn . Sequential ( * _get_conv_bn_layer ( in_channels , out_channels , kernel_size = 1 , stride = stride_residual , bias = False , ) ) else : self . res = None self . mout = nn . Sequential ( * _get_act_dropout_layer ( drop_prob = dropout ))","title":"__init__()"},{"location":"api/Citrinet/blocks/#thunder.citrinet.blocks.CitrinetBlock.forward","text":"Parameters: Name Type Description Default x Tensor Tensor of shape (batch, features, time) where #features == inplanes required Returns: Type Description Tensor Result of applying the block on the input, and corresponding output lengths Source code in thunder/citrinet/blocks.py def forward ( self , x : torch . Tensor ) -> torch . Tensor : \"\"\" Args: x : Tensor of shape (batch, features, time) where #features == inplanes Returns: Result of applying the block on the input, and corresponding output lengths \"\"\" # compute forward convolutions out = self . mconv ( x ) # compute the residuals if self . res is not None : res_out = self . res ( x ) out = out + res_out # compute the output return self . mout ( out )","title":"forward()"},{"location":"api/Citrinet/blocks/#thunder.citrinet.blocks.EncoderConfig","text":"Configuration to create Citrinet_encoder Attributes: Name Type Description filters List[int] List of filter sizes used to create the encoder blocks. required. kernel_sizes List[int] List of kernel sizes corresponding to each filter size. required. strides List[int] List of stride corresponding to each filter size. required. feat_in Number of input features to the model. defaults to 80.","title":"EncoderConfig"},{"location":"api/Citrinet/blocks/#thunder.citrinet.blocks.SqueezeExcite","text":"","title":"SqueezeExcite"},{"location":"api/Citrinet/blocks/#thunder.citrinet.blocks.SqueezeExcite.__init__","text":"Squeeze-and-Excitation sub-module. Parameters: Name Type Description Default channels int Input number of channels. required reduction_ratio int Reduction ratio for \"squeeze\" layer. required Source code in thunder/citrinet/blocks.py def __init__ ( self , channels : int , reduction_ratio : int , ): \"\"\" Squeeze-and-Excitation sub-module. Args: channels: Input number of channels. reduction_ratio: Reduction ratio for \"squeeze\" layer. \"\"\" super ( SqueezeExcite , self ) . __init__ () self . pool = nn . AdaptiveAvgPool1d ( 1 ) # context window = T self . fc = nn . Sequential ( nn . Linear ( channels , channels // reduction_ratio , bias = False ), nn . ReLU ( True ), nn . Linear ( channels // reduction_ratio , channels , bias = False ), )","title":"__init__()"},{"location":"api/Citrinet/blocks/#thunder.citrinet.blocks.SqueezeExcite.forward","text":"Parameters: Name Type Description Default x Tensor Tensor of shape [batch, channels, time] required Returns: Type Description Tensor Tensor of shape [batch, channels, time] Source code in thunder/citrinet/blocks.py def forward ( self , x : torch . Tensor ) -> torch . Tensor : \"\"\" Args: x : Tensor of shape [batch, channels, time] Returns: Tensor of shape [batch, channels, time] \"\"\" y = self . pool ( x ) # [B, C, T - context_window + 1] y = y . transpose ( 1 , - 1 ) # [B, T - context_window + 1, C] y = self . fc ( y ) # [B, T - context_window + 1, C] y = y . transpose ( 1 , - 1 ) # [B, C, T - context_window + 1] y = torch . sigmoid ( y ) return x * y","title":"forward()"},{"location":"api/Citrinet/blocks/#thunder.citrinet.blocks.body","text":"Creates the body of the Citrinet model. That is the middle part. Parameters: Name Type Description Default filters List[int] List of filters inside each block in the body. required kernel_size List[int] Corresponding list of kernel sizes for each block. Should have the same length as the first argument. required strides List[int] Corresponding list of strides for each block. Should have the same length as the first argument. required Returns: Type Description List[thunder.citrinet.blocks.CitrinetBlock] List of layers that form the body of the network. Source code in thunder/citrinet/blocks.py def body ( filters : List [ int ], kernel_size : List [ int ], strides : List [ int ], ) -> List [ CitrinetBlock ]: \"\"\"Creates the body of the Citrinet model. That is the middle part. Args: filters : List of filters inside each block in the body. kernel_size : Corresponding list of kernel sizes for each block. Should have the same length as the first argument. strides: Corresponding list of strides for each block. Should have the same length as the first argument. Returns: List of layers that form the body of the network. \"\"\" layers = [] f_in = 256 for f , k , s in zip ( filters , kernel_size , strides ): layers . append ( CitrinetBlock ( f_in , f , kernel_size = ( k ,), stride = ( s ,), separable = True ) ) f_in = f layers . append ( CitrinetBlock ( f_in , 640 , repeat = 1 , kernel_size = ( 41 ,), residual = False , separable = True , ) ) return layers","title":"body()"},{"location":"api/Citrinet/blocks/#thunder.citrinet.blocks.Citrinet_encoder","text":"Basic Citrinet encoder setup. Parameters: Name Type Description Default cfg EncoderConfig required config to create instance required Returns: Type Description Module Pytorch model corresponding to the encoder. Source code in thunder/citrinet/blocks.py def Citrinet_encoder ( cfg : EncoderConfig ) -> nn . Module : \"\"\"Basic Citrinet encoder setup. Args: cfg: required config to create instance Returns: Pytorch model corresponding to the encoder. \"\"\" return nn . Sequential ( stem ( cfg . feat_in ), * body ( cfg . filters , cfg . kernel_sizes , cfg . strides ), )","title":"Citrinet_encoder()"},{"location":"api/Citrinet/blocks/#thunder.citrinet.blocks.stem","text":"Creates the Citrinet stem. That is the first block of the model, that process the input directly. Parameters: Name Type Description Default feat_in int Number of input features required Returns: Type Description CitrinetBlock Citrinet stem block Source code in thunder/citrinet/blocks.py def stem ( feat_in : int ) -> CitrinetBlock : \"\"\"Creates the Citrinet stem. That is the first block of the model, that process the input directly. Args: feat_in : Number of input features Returns: Citrinet stem block \"\"\" return CitrinetBlock ( feat_in , 256 , repeat = 1 , kernel_size = ( 5 ,), residual = False , separable = True , )","title":"stem()"},{"location":"api/Citrinet/compatibility/","text":"Helper functions to load the Citrinet model from original Nemo released checkpoint files. CitrinetCheckpoint Trained model weight checkpoints. Used by download_checkpoint and CitrinetModule.load_from_nemo . Note Possible values are stt_en_citrinet_256 , stt_en_citrinet_512 , stt_en_citrinet_1024 , stt_es_citrinet_512 fix_vocab ( vocab_tokens ) Transform the nemo vocab tokens back to the sentencepiece sytle with the _ prefix Parameters: Name Type Description Default vocab_tokens List[str] List of tokens in the vocabulary required Returns: Type Description List[str] New list of tokens with the new prefix Source code in thunder/citrinet/compatibility.py def fix_vocab ( vocab_tokens : List [ str ]) -> List [ str ]: \"\"\"Transform the nemo vocab tokens back to the sentencepiece sytle with the _ prefix Args: vocab_tokens : List of tokens in the vocabulary Returns: New list of tokens with the new prefix \"\"\" out_tokens = [] for token in vocab_tokens : if token . startswith ( \"##\" ): out_tokens . append ( token [ 2 :]) else : out_tokens . append ( \"\u2581\" + token ) return out_tokens read_params_from_config_citrinet ( config_path ) Read the important parameters from the config stored inside the .nemo checkpoint. Parameters: Name Type Description Default config_path Union[str, pathlib.Path] Path to the .yaml file, usually called model_config.yaml required Returns: Type Description Tuple[Dict, List[str], Dict] A tuple containing, in this order, the encoder hyperparameters, the vocabulary, and the preprocessing hyperparameters Source code in thunder/citrinet/compatibility.py def read_params_from_config_citrinet ( config_path : Union [ str , Path ] ) -> Tuple [ Dict , List [ str ], Dict ]: \"\"\"Read the important parameters from the config stored inside the .nemo checkpoint. Args: config_path : Path to the .yaml file, usually called model_config.yaml Returns: A tuple containing, in this order, the encoder hyperparameters, the vocabulary, and the preprocessing hyperparameters \"\"\" conf = OmegaConf . load ( config_path ) encoder_params = conf [ \"encoder\" ] quartznet_conf = OmegaConf . to_container ( encoder_params [ \"jasper\" ]) body_config = quartznet_conf [ 1 : - 1 ] filters = [ cfg [ \"filters\" ] for cfg in body_config ] kernel_sizes = [ cfg [ \"kernel\" ][ 0 ] for cfg in body_config ] strides = [ cfg [ \"stride\" ][ 0 ] for cfg in body_config ] encoder_cfg = { \"filters\" : filters , \"kernel_sizes\" : kernel_sizes , \"strides\" : strides , } preprocess = conf [ \"preprocessor\" ] preprocess_cfg = { \"sample_rate\" : preprocess [ \"sample_rate\" ], \"n_window_size\" : int ( preprocess [ \"window_size\" ] * preprocess [ \"sample_rate\" ]), \"n_window_stride\" : int ( preprocess [ \"window_stride\" ] * preprocess [ \"sample_rate\" ]), \"n_fft\" : preprocess [ \"n_fft\" ], \"nfilt\" : preprocess [ \"features\" ], \"dither\" : preprocess [ \"dither\" ], } labels = conf [ \"labels\" ] if \"labels\" in conf else conf [ \"decoder\" ][ \"vocabulary\" ] return ( encoder_cfg , OmegaConf . to_container ( labels ), preprocess_cfg , )","title":"Compatibility"},{"location":"api/Citrinet/compatibility/#thunder.citrinet.compatibility.CitrinetCheckpoint","text":"Trained model weight checkpoints. Used by download_checkpoint and CitrinetModule.load_from_nemo . Note Possible values are stt_en_citrinet_256 , stt_en_citrinet_512 , stt_en_citrinet_1024 , stt_es_citrinet_512","title":"CitrinetCheckpoint"},{"location":"api/Citrinet/compatibility/#thunder.citrinet.compatibility.fix_vocab","text":"Transform the nemo vocab tokens back to the sentencepiece sytle with the _ prefix Parameters: Name Type Description Default vocab_tokens List[str] List of tokens in the vocabulary required Returns: Type Description List[str] New list of tokens with the new prefix Source code in thunder/citrinet/compatibility.py def fix_vocab ( vocab_tokens : List [ str ]) -> List [ str ]: \"\"\"Transform the nemo vocab tokens back to the sentencepiece sytle with the _ prefix Args: vocab_tokens : List of tokens in the vocabulary Returns: New list of tokens with the new prefix \"\"\" out_tokens = [] for token in vocab_tokens : if token . startswith ( \"##\" ): out_tokens . append ( token [ 2 :]) else : out_tokens . append ( \"\u2581\" + token ) return out_tokens","title":"fix_vocab()"},{"location":"api/Citrinet/compatibility/#thunder.citrinet.compatibility.read_params_from_config_citrinet","text":"Read the important parameters from the config stored inside the .nemo checkpoint. Parameters: Name Type Description Default config_path Union[str, pathlib.Path] Path to the .yaml file, usually called model_config.yaml required Returns: Type Description Tuple[Dict, List[str], Dict] A tuple containing, in this order, the encoder hyperparameters, the vocabulary, and the preprocessing hyperparameters Source code in thunder/citrinet/compatibility.py def read_params_from_config_citrinet ( config_path : Union [ str , Path ] ) -> Tuple [ Dict , List [ str ], Dict ]: \"\"\"Read the important parameters from the config stored inside the .nemo checkpoint. Args: config_path : Path to the .yaml file, usually called model_config.yaml Returns: A tuple containing, in this order, the encoder hyperparameters, the vocabulary, and the preprocessing hyperparameters \"\"\" conf = OmegaConf . load ( config_path ) encoder_params = conf [ \"encoder\" ] quartznet_conf = OmegaConf . to_container ( encoder_params [ \"jasper\" ]) body_config = quartznet_conf [ 1 : - 1 ] filters = [ cfg [ \"filters\" ] for cfg in body_config ] kernel_sizes = [ cfg [ \"kernel\" ][ 0 ] for cfg in body_config ] strides = [ cfg [ \"stride\" ][ 0 ] for cfg in body_config ] encoder_cfg = { \"filters\" : filters , \"kernel_sizes\" : kernel_sizes , \"strides\" : strides , } preprocess = conf [ \"preprocessor\" ] preprocess_cfg = { \"sample_rate\" : preprocess [ \"sample_rate\" ], \"n_window_size\" : int ( preprocess [ \"window_size\" ] * preprocess [ \"sample_rate\" ]), \"n_window_stride\" : int ( preprocess [ \"window_stride\" ] * preprocess [ \"sample_rate\" ]), \"n_fft\" : preprocess [ \"n_fft\" ], \"nfilt\" : preprocess [ \"features\" ], \"dither\" : preprocess [ \"dither\" ], } labels = conf [ \"labels\" ] if \"labels\" in conf else conf [ \"decoder\" ][ \"vocabulary\" ] return ( encoder_cfg , OmegaConf . to_container ( labels ), preprocess_cfg , )","title":"read_params_from_config_citrinet()"},{"location":"api/Citrinet/module/","text":"Citrinet LightningModule that combines all of the individual parts to enable easy finetuning and inference. CitrinetModule __init__ ( self , text_cfg , encoder_cfg , audio_cfg = FilterbankConfig ( sample_rate = 16000 , n_window_size = 320 , n_window_stride = 160 , n_fft = 512 , preemph = 0.97 , nfilt = 64 , dither = 1e-05 ), optim_cfg = OptimizerConfig ( learning_rate = 0.0003 , betas = ( 0.8 , 0.5 ))) special Module containing both the basic citrinet model and helper functionality, such as feature creation and text processing. Parameters: Name Type Description Default text_cfg TextTransformConfig Configuration for the text processing pipeline required encoder_cfg EncoderConfig Configuration for the citrinet encoder required audio_cfg FilterbankConfig Configuration for the filterbank features applied to the input audio FilterbankConfig(sample_rate=16000, n_window_size=320, n_window_stride=160, n_fft=512, preemph=0.97, nfilt=64, dither=1e-05) optim_cfg OptimizerConfig Configuration for the optimizer used during training OptimizerConfig(learning_rate=0.0003, betas=(0.8, 0.5)) Source code in thunder/citrinet/module.py def __init__ ( self , text_cfg : TextTransformConfig , encoder_cfg : EncoderConfig , audio_cfg : FilterbankConfig = FilterbankConfig (), optim_cfg : OptimizerConfig = OptimizerConfig (), ): \"\"\"Module containing both the basic citrinet model and helper functionality, such as feature creation and text processing. Args: text_cfg: Configuration for the text processing pipeline encoder_cfg: Configuration for the citrinet encoder audio_cfg: Configuration for the filterbank features applied to the input audio optim_cfg: Configuration for the optimizer used during training \"\"\" super () . __init__ () self . save_hyperparameters () self . audio_transform = FilterbankFeatures ( audio_cfg ) self . encoder = Citrinet_encoder ( encoder_cfg ) self . text_transform = BatchTextTransformer ( text_cfg ) self . decoder = conv1d_decoder ( 640 , num_classes = len ( self . text_transform . vocab )) # Metrics self . val_cer = CER () self . val_wer = WER () # Example input is one second of fake audio self . example_input_array = torch . randn (( 10 , audio_cfg . sample_rate )) change_vocab ( self , text_cfg ) Changes the vocabulary of the model. useful when finetuning to another language. Parameters: Name Type Description Default text_cfg TextTransformConfig Configuration for the text processing pipeline required Source code in thunder/citrinet/module.py def change_vocab ( self , text_cfg : TextTransformConfig ): \"\"\"Changes the vocabulary of the model. useful when finetuning to another language. Args: text_cfg: Configuration for the text processing pipeline \"\"\" # Updating hparams so that the saved model can be correctly loaded self . hparams . text_cfg = text_cfg self . text_transform = BatchTextTransformer ( text_cfg ) self . decoder = conv1d_decoder ( 640 , num_classes = len ( self . text_transform . vocab )) configure_optimizers ( self ) Configuring optimizers. Check the original lightning docs for more info. Returns: Type Description Optimizer Optimizer, and optionally the learning rate scheduler. Source code in thunder/citrinet/module.py def configure_optimizers ( self ) -> torch . optim . Optimizer : \"\"\"Configuring optimizers. Check the original lightning docs for more info. Returns: Optimizer, and optionally the learning rate scheduler. \"\"\" return torch . optim . Adam ( filter ( lambda p : p . requires_grad , self . parameters ()), lr = self . hparams . optim_cfg . learning_rate , betas = self . hparams . optim_cfg . betas , ) forward ( self , x ) Process the audio tensor to create the predictions. Parameters: Name Type Description Default x Tensor Audio tensor of shape [batch_size, time] required Returns: Type Description Tensor Tuple with the predictions and output lengths. Notice that the ouput lengths are not normalized, they are a long tensor. Source code in thunder/citrinet/module.py def forward ( self , x : Tensor ) -> Tensor : \"\"\"Process the audio tensor to create the predictions. Args: x : Audio tensor of shape [batch_size, time] Returns: Tuple with the predictions and output lengths. Notice that the ouput lengths are not normalized, they are a long tensor. \"\"\" features = self . audio_transform ( x ) encoded = self . encoder ( features ) return self . decoder ( encoded ) load_from_nemo ( checkpoint , save_folder = None ) classmethod Load from the original nemo checkpoint. Parameters: Name Type Description Default checkpoint Union[str, thunder.citrinet.compatibility.CitrinetCheckpoint] Path to local .nemo file or checkpoint to be downloaded locally and lodaded. required save_folder str Path to save the checkpoint when downloading it. Ignored if you pass a .nemo file as the first argument. None Returns: Type Description CitrinetModule The model loaded from the checkpoint Source code in thunder/citrinet/module.py @classmethod def load_from_nemo ( cls , checkpoint : Union [ str , CitrinetCheckpoint ], save_folder : str = None ) -> \"CitrinetModule\" : \"\"\"Load from the original nemo checkpoint. Args: checkpoint : Path to local .nemo file or checkpoint to be downloaded locally and lodaded. save_folder : Path to save the checkpoint when downloading it. Ignored if you pass a .nemo file as the first argument. Returns: The model loaded from the checkpoint \"\"\" if isinstance ( checkpoint , CitrinetCheckpoint ): nemo_filepath = download_checkpoint ( checkpoint , save_folder ) else : nemo_filepath = checkpoint with TemporaryDirectory () as extract_path : extract_archive ( str ( nemo_filepath ), extract_path ) extract_path = Path ( extract_path ) config_path = extract_path / \"model_config.yaml\" ( encoder_params , initial_vocab , preprocess_params , ) = read_params_from_config_citrinet ( config_path ) sentencepiece_path = str ( extract_path / \"tokenizer.model\" ) module = cls ( TextTransformConfig ( initial_vocab_tokens = fix_vocab ( initial_vocab ), sentencepiece_model = sentencepiece_path , simple_vocab = True , ), EncoderConfig ( ** encoder_params ), FilterbankConfig ( ** preprocess_params ), ) weights_path = extract_path / \"model_weights.ckpt\" load_quartznet_weights ( module . encoder , module . decoder , str ( weights_path )) # Here we set it in eval mode, so it correctly works during inference # Supposing that the majority of applications will be either (1) load a checkpoint # and directly run inference, or (2) fine-tuning. Either way this will prevent a silent # bug (case 1) or will be ignored (case 2). module . eval () return module predict ( self , x ) Use this function during inference to predict. Parameters: Name Type Description Default x Tensor Audio tensor of shape [batch_size, time] required Returns: Type Description List[str] A list of strings, each one contains the corresponding transcription to the original batch element. Source code in thunder/citrinet/module.py @torch . jit . export def predict ( self , x : Tensor ) -> List [ str ]: \"\"\"Use this function during inference to predict. Args: x : Audio tensor of shape [batch_size, time] Returns: A list of strings, each one contains the corresponding transcription to the original batch element. \"\"\" pred = self ( x ) return self . text_transform . decode_prediction ( pred . argmax ( 1 )) training_step ( self , batch , batch_idx ) Training step. Check the original lightning docs for more information. Parameters: Name Type Description Default batch Tuple[torch.Tensor, torch.Tensor, List[str]] Tuple containing the batched audios, normalized lengths and the corresponding text labels. required batch_idx int Batch index required Returns: Type Description Tensor Training loss for that batch Source code in thunder/citrinet/module.py def training_step ( self , batch : Tuple [ torch . Tensor , torch . Tensor , List [ str ]], batch_idx : int ) -> torch . Tensor : \"\"\"Training step. Check the original lightning docs for more information. Args: batch : Tuple containing the batched audios, normalized lengths and the corresponding text labels. batch_idx : Batch index Returns: Training loss for that batch \"\"\" audio , audio_lens , texts = batch y , y_lens = self . text_transform . encode ( texts , device = self . device ) probabilities = self ( audio ) loss = calculate_ctc ( probabilities , y , audio_lens , y_lens , self . text_transform . vocab . blank_idx ) self . log ( \"loss/train_loss\" , loss ) return loss validation_step ( self , batch , batch_idx ) Validation step. Check the original lightning docs for more information. Parameters: Name Type Description Default batch Tuple[torch.Tensor, torch.Tensor, List[str]] Tuple containing the batched audios, normalized lengths and the corresponding text labels. required batch_idx int Batch index required Returns: Type Description Tensor Validation loss for that batch Source code in thunder/citrinet/module.py def validation_step ( self , batch : Tuple [ torch . Tensor , torch . Tensor , List [ str ]], batch_idx : int ) -> torch . Tensor : \"\"\"Validation step. Check the original lightning docs for more information. Args: batch : Tuple containing the batched audios, normalized lengths and the corresponding text labels. batch_idx : Batch index Returns: Validation loss for that batch \"\"\" audio , audio_lens , texts = batch y , y_lens = self . text_transform . encode ( texts , device = self . device ) probabilities = self ( audio ) loss = calculate_ctc ( probabilities , y , audio_lens , y_lens , self . text_transform . vocab . blank_idx ) decoded_preds = self . text_transform . decode_prediction ( probabilities . argmax ( 1 )) decoded_targets = self . text_transform . decode_prediction ( y ) self . val_cer ( decoded_preds , decoded_targets ) self . val_wer ( decoded_preds , decoded_targets ) self . log ( \"loss/val_loss\" , loss ) self . log ( \"metrics/cer\" , self . val_cer , on_epoch = True ) self . log ( \"metrics/wer\" , self . val_wer , on_epoch = True ) return loss OptimizerConfig dataclass Configuration used by the optimizer Attributes: Name Type Description learning_rate float learning rate. defaults to 3e-4. betas Tuple[float] beta1 and beta2 used by adam. defaults to (0.8, 0.5), similar to the novograd values on nemo.","title":"Module"},{"location":"api/Citrinet/module/#thunder.citrinet.module.CitrinetModule","text":"","title":"CitrinetModule"},{"location":"api/Citrinet/module/#thunder.citrinet.module.CitrinetModule.__init__","text":"Module containing both the basic citrinet model and helper functionality, such as feature creation and text processing. Parameters: Name Type Description Default text_cfg TextTransformConfig Configuration for the text processing pipeline required encoder_cfg EncoderConfig Configuration for the citrinet encoder required audio_cfg FilterbankConfig Configuration for the filterbank features applied to the input audio FilterbankConfig(sample_rate=16000, n_window_size=320, n_window_stride=160, n_fft=512, preemph=0.97, nfilt=64, dither=1e-05) optim_cfg OptimizerConfig Configuration for the optimizer used during training OptimizerConfig(learning_rate=0.0003, betas=(0.8, 0.5)) Source code in thunder/citrinet/module.py def __init__ ( self , text_cfg : TextTransformConfig , encoder_cfg : EncoderConfig , audio_cfg : FilterbankConfig = FilterbankConfig (), optim_cfg : OptimizerConfig = OptimizerConfig (), ): \"\"\"Module containing both the basic citrinet model and helper functionality, such as feature creation and text processing. Args: text_cfg: Configuration for the text processing pipeline encoder_cfg: Configuration for the citrinet encoder audio_cfg: Configuration for the filterbank features applied to the input audio optim_cfg: Configuration for the optimizer used during training \"\"\" super () . __init__ () self . save_hyperparameters () self . audio_transform = FilterbankFeatures ( audio_cfg ) self . encoder = Citrinet_encoder ( encoder_cfg ) self . text_transform = BatchTextTransformer ( text_cfg ) self . decoder = conv1d_decoder ( 640 , num_classes = len ( self . text_transform . vocab )) # Metrics self . val_cer = CER () self . val_wer = WER () # Example input is one second of fake audio self . example_input_array = torch . randn (( 10 , audio_cfg . sample_rate ))","title":"__init__()"},{"location":"api/Citrinet/module/#thunder.citrinet.module.CitrinetModule.change_vocab","text":"Changes the vocabulary of the model. useful when finetuning to another language. Parameters: Name Type Description Default text_cfg TextTransformConfig Configuration for the text processing pipeline required Source code in thunder/citrinet/module.py def change_vocab ( self , text_cfg : TextTransformConfig ): \"\"\"Changes the vocabulary of the model. useful when finetuning to another language. Args: text_cfg: Configuration for the text processing pipeline \"\"\" # Updating hparams so that the saved model can be correctly loaded self . hparams . text_cfg = text_cfg self . text_transform = BatchTextTransformer ( text_cfg ) self . decoder = conv1d_decoder ( 640 , num_classes = len ( self . text_transform . vocab ))","title":"change_vocab()"},{"location":"api/Citrinet/module/#thunder.citrinet.module.CitrinetModule.configure_optimizers","text":"Configuring optimizers. Check the original lightning docs for more info. Returns: Type Description Optimizer Optimizer, and optionally the learning rate scheduler. Source code in thunder/citrinet/module.py def configure_optimizers ( self ) -> torch . optim . Optimizer : \"\"\"Configuring optimizers. Check the original lightning docs for more info. Returns: Optimizer, and optionally the learning rate scheduler. \"\"\" return torch . optim . Adam ( filter ( lambda p : p . requires_grad , self . parameters ()), lr = self . hparams . optim_cfg . learning_rate , betas = self . hparams . optim_cfg . betas , )","title":"configure_optimizers()"},{"location":"api/Citrinet/module/#thunder.citrinet.module.CitrinetModule.forward","text":"Process the audio tensor to create the predictions. Parameters: Name Type Description Default x Tensor Audio tensor of shape [batch_size, time] required Returns: Type Description Tensor Tuple with the predictions and output lengths. Notice that the ouput lengths are not normalized, they are a long tensor. Source code in thunder/citrinet/module.py def forward ( self , x : Tensor ) -> Tensor : \"\"\"Process the audio tensor to create the predictions. Args: x : Audio tensor of shape [batch_size, time] Returns: Tuple with the predictions and output lengths. Notice that the ouput lengths are not normalized, they are a long tensor. \"\"\" features = self . audio_transform ( x ) encoded = self . encoder ( features ) return self . decoder ( encoded )","title":"forward()"},{"location":"api/Citrinet/module/#thunder.citrinet.module.CitrinetModule.load_from_nemo","text":"Load from the original nemo checkpoint. Parameters: Name Type Description Default checkpoint Union[str, thunder.citrinet.compatibility.CitrinetCheckpoint] Path to local .nemo file or checkpoint to be downloaded locally and lodaded. required save_folder str Path to save the checkpoint when downloading it. Ignored if you pass a .nemo file as the first argument. None Returns: Type Description CitrinetModule The model loaded from the checkpoint Source code in thunder/citrinet/module.py @classmethod def load_from_nemo ( cls , checkpoint : Union [ str , CitrinetCheckpoint ], save_folder : str = None ) -> \"CitrinetModule\" : \"\"\"Load from the original nemo checkpoint. Args: checkpoint : Path to local .nemo file or checkpoint to be downloaded locally and lodaded. save_folder : Path to save the checkpoint when downloading it. Ignored if you pass a .nemo file as the first argument. Returns: The model loaded from the checkpoint \"\"\" if isinstance ( checkpoint , CitrinetCheckpoint ): nemo_filepath = download_checkpoint ( checkpoint , save_folder ) else : nemo_filepath = checkpoint with TemporaryDirectory () as extract_path : extract_archive ( str ( nemo_filepath ), extract_path ) extract_path = Path ( extract_path ) config_path = extract_path / \"model_config.yaml\" ( encoder_params , initial_vocab , preprocess_params , ) = read_params_from_config_citrinet ( config_path ) sentencepiece_path = str ( extract_path / \"tokenizer.model\" ) module = cls ( TextTransformConfig ( initial_vocab_tokens = fix_vocab ( initial_vocab ), sentencepiece_model = sentencepiece_path , simple_vocab = True , ), EncoderConfig ( ** encoder_params ), FilterbankConfig ( ** preprocess_params ), ) weights_path = extract_path / \"model_weights.ckpt\" load_quartznet_weights ( module . encoder , module . decoder , str ( weights_path )) # Here we set it in eval mode, so it correctly works during inference # Supposing that the majority of applications will be either (1) load a checkpoint # and directly run inference, or (2) fine-tuning. Either way this will prevent a silent # bug (case 1) or will be ignored (case 2). module . eval () return module","title":"load_from_nemo()"},{"location":"api/Citrinet/module/#thunder.citrinet.module.CitrinetModule.predict","text":"Use this function during inference to predict. Parameters: Name Type Description Default x Tensor Audio tensor of shape [batch_size, time] required Returns: Type Description List[str] A list of strings, each one contains the corresponding transcription to the original batch element. Source code in thunder/citrinet/module.py @torch . jit . export def predict ( self , x : Tensor ) -> List [ str ]: \"\"\"Use this function during inference to predict. Args: x : Audio tensor of shape [batch_size, time] Returns: A list of strings, each one contains the corresponding transcription to the original batch element. \"\"\" pred = self ( x ) return self . text_transform . decode_prediction ( pred . argmax ( 1 ))","title":"predict()"},{"location":"api/Citrinet/module/#thunder.citrinet.module.CitrinetModule.training_step","text":"Training step. Check the original lightning docs for more information. Parameters: Name Type Description Default batch Tuple[torch.Tensor, torch.Tensor, List[str]] Tuple containing the batched audios, normalized lengths and the corresponding text labels. required batch_idx int Batch index required Returns: Type Description Tensor Training loss for that batch Source code in thunder/citrinet/module.py def training_step ( self , batch : Tuple [ torch . Tensor , torch . Tensor , List [ str ]], batch_idx : int ) -> torch . Tensor : \"\"\"Training step. Check the original lightning docs for more information. Args: batch : Tuple containing the batched audios, normalized lengths and the corresponding text labels. batch_idx : Batch index Returns: Training loss for that batch \"\"\" audio , audio_lens , texts = batch y , y_lens = self . text_transform . encode ( texts , device = self . device ) probabilities = self ( audio ) loss = calculate_ctc ( probabilities , y , audio_lens , y_lens , self . text_transform . vocab . blank_idx ) self . log ( \"loss/train_loss\" , loss ) return loss","title":"training_step()"},{"location":"api/Citrinet/module/#thunder.citrinet.module.CitrinetModule.validation_step","text":"Validation step. Check the original lightning docs for more information. Parameters: Name Type Description Default batch Tuple[torch.Tensor, torch.Tensor, List[str]] Tuple containing the batched audios, normalized lengths and the corresponding text labels. required batch_idx int Batch index required Returns: Type Description Tensor Validation loss for that batch Source code in thunder/citrinet/module.py def validation_step ( self , batch : Tuple [ torch . Tensor , torch . Tensor , List [ str ]], batch_idx : int ) -> torch . Tensor : \"\"\"Validation step. Check the original lightning docs for more information. Args: batch : Tuple containing the batched audios, normalized lengths and the corresponding text labels. batch_idx : Batch index Returns: Validation loss for that batch \"\"\" audio , audio_lens , texts = batch y , y_lens = self . text_transform . encode ( texts , device = self . device ) probabilities = self ( audio ) loss = calculate_ctc ( probabilities , y , audio_lens , y_lens , self . text_transform . vocab . blank_idx ) decoded_preds = self . text_transform . decode_prediction ( probabilities . argmax ( 1 )) decoded_targets = self . text_transform . decode_prediction ( y ) self . val_cer ( decoded_preds , decoded_targets ) self . val_wer ( decoded_preds , decoded_targets ) self . log ( \"loss/val_loss\" , loss ) self . log ( \"metrics/cer\" , self . val_cer , on_epoch = True ) self . log ( \"metrics/wer\" , self . val_wer , on_epoch = True ) return loss","title":"validation_step()"},{"location":"api/Citrinet/module/#thunder.citrinet.module.OptimizerConfig","text":"Configuration used by the optimizer Attributes: Name Type Description learning_rate float learning rate. defaults to 3e-4. betas Tuple[float] beta1 and beta2 used by adam. defaults to (0.8, 0.5), similar to the novograd values on nemo.","title":"OptimizerConfig"},{"location":"api/Data/dataloader%20utils/","text":"Helper functions used by the speech dataloaders. asr_collate ( samples ) Function that collect samples and adds padding. Parameters: Name Type Description Default samples List[Tuple[torch.Tensor, str]] Samples produced by dataloader required Returns: Type Description Tuple[torch.Tensor, torch.Tensor, List[str]] Tuple containing padded audios, audio lengths (normalized to 0.0 <-> 1.0 range) and the list of corresponding transcriptions in that order. Source code in thunder/data/dataloader_utils.py def asr_collate ( samples : List [ Tuple [ Tensor , str ]]) -> Tuple [ Tensor , Tensor , List [ str ]]: \"\"\"Function that collect samples and adds padding. Args: samples : Samples produced by dataloader Returns: Tuple containing padded audios, audio lengths (normalized to 0.0 <-> 1.0 range) and the list of corresponding transcriptions in that order. \"\"\" samples = sorted ( samples , key = lambda sample : sample [ 0 ] . size ( - 1 ), reverse = True ) padded_audios = pad_sequence ([ s [ 0 ] . squeeze () for s in samples ], batch_first = True ) audio_lengths = Tensor ([ s [ 0 ] . size ( - 1 ) for s in samples ]) audio_lengths = audio_lengths / audio_lengths . max () # Normalize by max length texts = [ s [ 1 ] for s in samples ] return ( padded_audios , audio_lengths , texts )","title":"Dataloader utils"},{"location":"api/Data/dataloader%20utils/#thunder.data.dataloader_utils.asr_collate","text":"Function that collect samples and adds padding. Parameters: Name Type Description Default samples List[Tuple[torch.Tensor, str]] Samples produced by dataloader required Returns: Type Description Tuple[torch.Tensor, torch.Tensor, List[str]] Tuple containing padded audios, audio lengths (normalized to 0.0 <-> 1.0 range) and the list of corresponding transcriptions in that order. Source code in thunder/data/dataloader_utils.py def asr_collate ( samples : List [ Tuple [ Tensor , str ]]) -> Tuple [ Tensor , Tensor , List [ str ]]: \"\"\"Function that collect samples and adds padding. Args: samples : Samples produced by dataloader Returns: Tuple containing padded audios, audio lengths (normalized to 0.0 <-> 1.0 range) and the list of corresponding transcriptions in that order. \"\"\" samples = sorted ( samples , key = lambda sample : sample [ 0 ] . size ( - 1 ), reverse = True ) padded_audios = pad_sequence ([ s [ 0 ] . squeeze () for s in samples ], batch_first = True ) audio_lengths = Tensor ([ s [ 0 ] . size ( - 1 ) for s in samples ]) audio_lengths = audio_lengths / audio_lengths . max () # Normalize by max length texts = [ s [ 1 ] for s in samples ] return ( padded_audios , audio_lengths , texts )","title":"asr_collate()"},{"location":"api/Data/datamodule/","text":"BaseDataModule steps_per_epoch : int property readonly Number of steps for each training epoch. Used for learning rate scheduling. Returns: Type Description int Number of steps get_dataset ( self , split ) Function to get the corresponding dataset to the specified split. This should be implemented by subclasses. Parameters: Name Type Description Default split str One of \"train\", \"valid\" or \"test\". required Returns: Type Description BaseSpeechDataset The corresponding dataset. Source code in thunder/data/datamodule.py def get_dataset ( self , split : str ) -> BaseSpeechDataset : \"\"\"Function to get the corresponding dataset to the specified split. This should be implemented by subclasses. Args: split : One of \"train\", \"valid\" or \"test\". Returns: The corresponding dataset. \"\"\" raise NotImplementedError () setup ( self , stage = None ) Called at the beginning of fit (train + validate), validate, test, and predict. This is a good hook when you need to build models dynamically or adjust something about them. This hook is called on every process when using DDP. Parameters: Name Type Description Default stage Optional[str] either 'fit' , 'validate' , 'test' , or 'predict' None Example:: class LitModel(...): def __init__(self): self.l1 = None def prepare_data(self): download_data() tokenize() # don't do this self.something = else def setup(stage): data = Load_data(...) self.l1 = nn.Linear(28, data.num_classes) Source code in thunder/data/datamodule.py def setup ( self , stage : Optional [ str ] = None ): if stage in ( None , \"fit\" ): self . train_dataset = self . get_dataset ( split = \"train\" ) self . val_dataset = self . get_dataset ( split = \"valid\" ) if stage in ( None , \"test\" ): self . test_dataset = self . get_dataset ( split = \"test\" ) test_dataloader ( self ) Implement one or multiple PyTorch DataLoaders for testing. The dataloader you return will not be called every epoch unless you set :paramref: ~pytorch_lightning.trainer.Trainer.reload_dataloaders_every_epoch to True . For data processing use the following pattern: - download in :meth:`prepare_data` - process and split in :meth:`setup` However, the above are only necessary for distributed processing. .. warning:: do not assign state in prepare_data :meth: ~pytorch_lightning.trainer.Trainer.fit ... :meth: prepare_data :meth: setup :meth: train_dataloader :meth: val_dataloader :meth: test_dataloader Note Lightning adds the correct sampler for distributed and arbitrary hardware. There is no need to set it yourself. Returns: Type Description DataLoader Single or multiple PyTorch DataLoaders. Example:: def test_dataloader(self): transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (1.0,))]) dataset = MNIST(root='/path/to/mnist/', train=False, transform=transform, download=True) loader = torch.utils.data.DataLoader( dataset=dataset, batch_size=self.batch_size, shuffle=False ) return loader # can also return multiple dataloaders def test_dataloader(self): return [loader_a, loader_b, ..., loader_n] Note If you don't need a test dataset and a :meth: test_step , you don't need to implement this method. Note In the case where you return multiple test dataloaders, the :meth: test_step will have an argument dataloader_idx which matches the order here. Source code in thunder/data/datamodule.py def test_dataloader ( self ) -> DataLoader : return DataLoader ( self . test_dataset , batch_size = self . batch_size , shuffle = False , collate_fn = asr_collate , num_workers = self . num_workers , pin_memory = True , ) train_dataloader ( self ) Implement one or more PyTorch DataLoaders for training. Returns: Type Description DataLoader Either a single PyTorch :class: ~torch.utils.data.DataLoader or a collection of these (list, dict, nested lists and dicts). In the case of multiple dataloaders, please see this :ref: page <multiple-training-dataloaders> The dataloader you return will not be called every epoch unless you set :paramref: ~pytorch_lightning.trainer.Trainer.reload_dataloaders_every_epoch to True . For data processing use the following pattern: - download in :meth:`prepare_data` - process and split in :meth:`setup` However, the above are only necessary for distributed processing. .. warning:: do not assign state in prepare_data :meth: ~pytorch_lightning.trainer.Trainer.fit ... :meth: prepare_data :meth: setup :meth: train_dataloader Note Lightning adds the correct sampler for distributed and arbitrary hardware. There is no need to set it yourself. Example:: # single dataloader def train_dataloader(self): transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (1.0,))]) dataset = MNIST(root='/path/to/mnist/', train=True, transform=transform, download=True) loader = torch.utils.data.DataLoader( dataset=dataset, batch_size=self.batch_size, shuffle=True ) return loader # multiple dataloaders, return as list def train_dataloader(self): mnist = MNIST(...) cifar = CIFAR(...) mnist_loader = torch.utils.data.DataLoader( dataset=mnist, batch_size=self.batch_size, shuffle=True ) cifar_loader = torch.utils.data.DataLoader( dataset=cifar, batch_size=self.batch_size, shuffle=True ) # each batch will be a list of tensors: [batch_mnist, batch_cifar] return [mnist_loader, cifar_loader] # multiple dataloader, return as dict def train_dataloader(self): mnist = MNIST(...) cifar = CIFAR(...) mnist_loader = torch.utils.data.DataLoader( dataset=mnist, batch_size=self.batch_size, shuffle=True ) cifar_loader = torch.utils.data.DataLoader( dataset=cifar, batch_size=self.batch_size, shuffle=True ) # each batch will be a dict of tensors: {'mnist': batch_mnist, 'cifar': batch_cifar} return {'mnist': mnist_loader, 'cifar': cifar_loader} Source code in thunder/data/datamodule.py def train_dataloader ( self ) -> DataLoader : return DataLoader ( self . train_dataset , batch_size = self . batch_size , collate_fn = asr_collate , num_workers = self . num_workers , shuffle = True , pin_memory = True , ) val_dataloader ( self ) Implement one or multiple PyTorch DataLoaders for validation. The dataloader you return will not be called every epoch unless you set :paramref: ~pytorch_lightning.trainer.Trainer.reload_dataloaders_every_epoch to True . It's recommended that all data downloads and preparation happen in :meth: prepare_data . :meth: ~pytorch_lightning.trainer.Trainer.fit ... :meth: prepare_data :meth: train_dataloader :meth: val_dataloader :meth: test_dataloader Note Lightning adds the correct sampler for distributed and arbitrary hardware There is no need to set it yourself. Returns: Type Description DataLoader Single or multiple PyTorch DataLoaders. Examples:: def val_dataloader(self): transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (1.0,))]) dataset = MNIST(root='/path/to/mnist/', train=False, transform=transform, download=True) loader = torch.utils.data.DataLoader( dataset=dataset, batch_size=self.batch_size, shuffle=False ) return loader # can also return multiple dataloaders def val_dataloader(self): return [loader_a, loader_b, ..., loader_n] Note If you don't need a validation dataset and a :meth: validation_step , you don't need to implement this method. Note In the case where you return multiple validation dataloaders, the :meth: validation_step will have an argument dataloader_idx which matches the order here. Source code in thunder/data/datamodule.py def val_dataloader ( self ) -> DataLoader : return DataLoader ( self . val_dataset , batch_size = self . batch_size , shuffle = False , collate_fn = asr_collate , num_workers = self . num_workers , pin_memory = True , ) ManifestDatamodule get_dataset ( self , split ) Function to get the corresponding dataset to the specified split. This should be implemented by subclasses. Parameters: Name Type Description Default split str One of \"train\", \"valid\" or \"test\". required Returns: Type Description ManifestSpeechDataset The corresponding dataset. Source code in thunder/data/datamodule.py def get_dataset ( self , split : str ) -> ManifestSpeechDataset : return ManifestSpeechDataset ( self . manifest_mapping [ split ], self . force_mono , self . sample_rate )","title":"Datamodule"},{"location":"api/Data/datamodule/#thunder.data.datamodule.BaseDataModule","text":"","title":"BaseDataModule"},{"location":"api/Data/datamodule/#thunder.data.datamodule.BaseDataModule.steps_per_epoch","text":"Number of steps for each training epoch. Used for learning rate scheduling. Returns: Type Description int Number of steps","title":"steps_per_epoch"},{"location":"api/Data/datamodule/#thunder.data.datamodule.BaseDataModule.get_dataset","text":"Function to get the corresponding dataset to the specified split. This should be implemented by subclasses. Parameters: Name Type Description Default split str One of \"train\", \"valid\" or \"test\". required Returns: Type Description BaseSpeechDataset The corresponding dataset. Source code in thunder/data/datamodule.py def get_dataset ( self , split : str ) -> BaseSpeechDataset : \"\"\"Function to get the corresponding dataset to the specified split. This should be implemented by subclasses. Args: split : One of \"train\", \"valid\" or \"test\". Returns: The corresponding dataset. \"\"\" raise NotImplementedError ()","title":"get_dataset()"},{"location":"api/Data/datamodule/#thunder.data.datamodule.BaseDataModule.setup","text":"Called at the beginning of fit (train + validate), validate, test, and predict. This is a good hook when you need to build models dynamically or adjust something about them. This hook is called on every process when using DDP. Parameters: Name Type Description Default stage Optional[str] either 'fit' , 'validate' , 'test' , or 'predict' None Example:: class LitModel(...): def __init__(self): self.l1 = None def prepare_data(self): download_data() tokenize() # don't do this self.something = else def setup(stage): data = Load_data(...) self.l1 = nn.Linear(28, data.num_classes) Source code in thunder/data/datamodule.py def setup ( self , stage : Optional [ str ] = None ): if stage in ( None , \"fit\" ): self . train_dataset = self . get_dataset ( split = \"train\" ) self . val_dataset = self . get_dataset ( split = \"valid\" ) if stage in ( None , \"test\" ): self . test_dataset = self . get_dataset ( split = \"test\" )","title":"setup()"},{"location":"api/Data/datamodule/#thunder.data.datamodule.BaseDataModule.test_dataloader","text":"Implement one or multiple PyTorch DataLoaders for testing. The dataloader you return will not be called every epoch unless you set :paramref: ~pytorch_lightning.trainer.Trainer.reload_dataloaders_every_epoch to True . For data processing use the following pattern: - download in :meth:`prepare_data` - process and split in :meth:`setup` However, the above are only necessary for distributed processing. .. warning:: do not assign state in prepare_data :meth: ~pytorch_lightning.trainer.Trainer.fit ... :meth: prepare_data :meth: setup :meth: train_dataloader :meth: val_dataloader :meth: test_dataloader Note Lightning adds the correct sampler for distributed and arbitrary hardware. There is no need to set it yourself. Returns: Type Description DataLoader Single or multiple PyTorch DataLoaders. Example:: def test_dataloader(self): transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (1.0,))]) dataset = MNIST(root='/path/to/mnist/', train=False, transform=transform, download=True) loader = torch.utils.data.DataLoader( dataset=dataset, batch_size=self.batch_size, shuffle=False ) return loader # can also return multiple dataloaders def test_dataloader(self): return [loader_a, loader_b, ..., loader_n] Note If you don't need a test dataset and a :meth: test_step , you don't need to implement this method. Note In the case where you return multiple test dataloaders, the :meth: test_step will have an argument dataloader_idx which matches the order here. Source code in thunder/data/datamodule.py def test_dataloader ( self ) -> DataLoader : return DataLoader ( self . test_dataset , batch_size = self . batch_size , shuffle = False , collate_fn = asr_collate , num_workers = self . num_workers , pin_memory = True , )","title":"test_dataloader()"},{"location":"api/Data/datamodule/#thunder.data.datamodule.BaseDataModule.train_dataloader","text":"Implement one or more PyTorch DataLoaders for training. Returns: Type Description DataLoader Either a single PyTorch :class: ~torch.utils.data.DataLoader or a collection of these (list, dict, nested lists and dicts). In the case of multiple dataloaders, please see this :ref: page <multiple-training-dataloaders> The dataloader you return will not be called every epoch unless you set :paramref: ~pytorch_lightning.trainer.Trainer.reload_dataloaders_every_epoch to True . For data processing use the following pattern: - download in :meth:`prepare_data` - process and split in :meth:`setup` However, the above are only necessary for distributed processing. .. warning:: do not assign state in prepare_data :meth: ~pytorch_lightning.trainer.Trainer.fit ... :meth: prepare_data :meth: setup :meth: train_dataloader Note Lightning adds the correct sampler for distributed and arbitrary hardware. There is no need to set it yourself. Example:: # single dataloader def train_dataloader(self): transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (1.0,))]) dataset = MNIST(root='/path/to/mnist/', train=True, transform=transform, download=True) loader = torch.utils.data.DataLoader( dataset=dataset, batch_size=self.batch_size, shuffle=True ) return loader # multiple dataloaders, return as list def train_dataloader(self): mnist = MNIST(...) cifar = CIFAR(...) mnist_loader = torch.utils.data.DataLoader( dataset=mnist, batch_size=self.batch_size, shuffle=True ) cifar_loader = torch.utils.data.DataLoader( dataset=cifar, batch_size=self.batch_size, shuffle=True ) # each batch will be a list of tensors: [batch_mnist, batch_cifar] return [mnist_loader, cifar_loader] # multiple dataloader, return as dict def train_dataloader(self): mnist = MNIST(...) cifar = CIFAR(...) mnist_loader = torch.utils.data.DataLoader( dataset=mnist, batch_size=self.batch_size, shuffle=True ) cifar_loader = torch.utils.data.DataLoader( dataset=cifar, batch_size=self.batch_size, shuffle=True ) # each batch will be a dict of tensors: {'mnist': batch_mnist, 'cifar': batch_cifar} return {'mnist': mnist_loader, 'cifar': cifar_loader} Source code in thunder/data/datamodule.py def train_dataloader ( self ) -> DataLoader : return DataLoader ( self . train_dataset , batch_size = self . batch_size , collate_fn = asr_collate , num_workers = self . num_workers , shuffle = True , pin_memory = True , )","title":"train_dataloader()"},{"location":"api/Data/datamodule/#thunder.data.datamodule.BaseDataModule.val_dataloader","text":"Implement one or multiple PyTorch DataLoaders for validation. The dataloader you return will not be called every epoch unless you set :paramref: ~pytorch_lightning.trainer.Trainer.reload_dataloaders_every_epoch to True . It's recommended that all data downloads and preparation happen in :meth: prepare_data . :meth: ~pytorch_lightning.trainer.Trainer.fit ... :meth: prepare_data :meth: train_dataloader :meth: val_dataloader :meth: test_dataloader Note Lightning adds the correct sampler for distributed and arbitrary hardware There is no need to set it yourself. Returns: Type Description DataLoader Single or multiple PyTorch DataLoaders. Examples:: def val_dataloader(self): transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (1.0,))]) dataset = MNIST(root='/path/to/mnist/', train=False, transform=transform, download=True) loader = torch.utils.data.DataLoader( dataset=dataset, batch_size=self.batch_size, shuffle=False ) return loader # can also return multiple dataloaders def val_dataloader(self): return [loader_a, loader_b, ..., loader_n] Note If you don't need a validation dataset and a :meth: validation_step , you don't need to implement this method. Note In the case where you return multiple validation dataloaders, the :meth: validation_step will have an argument dataloader_idx which matches the order here. Source code in thunder/data/datamodule.py def val_dataloader ( self ) -> DataLoader : return DataLoader ( self . val_dataset , batch_size = self . batch_size , shuffle = False , collate_fn = asr_collate , num_workers = self . num_workers , pin_memory = True , )","title":"val_dataloader()"},{"location":"api/Data/datamodule/#thunder.data.datamodule.ManifestDatamodule","text":"","title":"ManifestDatamodule"},{"location":"api/Data/datamodule/#thunder.data.datamodule.ManifestDatamodule.get_dataset","text":"Function to get the corresponding dataset to the specified split. This should be implemented by subclasses. Parameters: Name Type Description Default split str One of \"train\", \"valid\" or \"test\". required Returns: Type Description ManifestSpeechDataset The corresponding dataset. Source code in thunder/data/datamodule.py def get_dataset ( self , split : str ) -> ManifestSpeechDataset : return ManifestSpeechDataset ( self . manifest_mapping [ split ], self . force_mono , self . sample_rate )","title":"get_dataset()"},{"location":"api/Data/dataset/","text":"AudioFileLoader __init__ ( self , force_mono = True , sample_rate = 16000 ) special Module containing the data loading and basic preprocessing. It's used internally by the datasets, but can be exported so that during inference time there's no code dependency. Parameters: Name Type Description Default force_mono bool If true, convert all the loaded samples to mono. True sample_rate int Sample rate used by the dataset. All of the samples that have different rate will be resampled. 16000 Source code in thunder/data/dataset.py def __init__ ( self , force_mono : bool = True , sample_rate : int = 16000 ): \"\"\"Module containing the data loading and basic preprocessing. It's used internally by the datasets, but can be exported so that during inference time there's no code dependency. Args: force_mono : If true, convert all the loaded samples to mono. sample_rate : Sample rate used by the dataset. All of the samples that have different rate will be resampled. \"\"\" super () . __init__ () self . force_mono = force_mono self . sample_rate = sample_rate forward ( self , item ) Opens audio item and do basic preprocessing Parameters: Name Type Description Default item str Path to the audio to be opened required Returns: Type Description Tensor Audio tensor after preprocessing Source code in thunder/data/dataset.py def forward ( self , item : str ) -> Tensor : \"\"\"Opens audio item and do basic preprocessing Args: item : Path to the audio to be opened Returns: Audio tensor after preprocessing \"\"\" audio , sample_rate = self . open_audio ( item ) return self . preprocess_audio ( audio , sample_rate ) open_audio ( self , item ) Uses the data returned by get_item to open the audio Parameters: Name Type Description Default item str Data returned by get_item(index) required Returns: Type Description Tuple[torch.Tensor, int] Tuple containing the audio tensor with shape (channels, time), and the corresponding sample rate. Source code in thunder/data/dataset.py @torch . jit . export def open_audio ( self , item : str ) -> Tuple [ Tensor , int ]: \"\"\"Uses the data returned by get_item to open the audio Args: item : Data returned by get_item(index) Returns: Tuple containing the audio tensor with shape (channels, time), and the corresponding sample rate. \"\"\" return torchaudio . load ( item ) preprocess_audio ( self , audio , sample_rate ) Apply some base transforms to the audio, that fix silent problems. It transforms all the audios to mono (depending on class creation parameter), remove the possible DC bias present and then resamples the audios to a common sample rate. Parameters: Name Type Description Default audio Tensor Audio tensor required sample_rate int Sample rate required Returns: Type Description Tensor Audio tensor after the transforms. Source code in thunder/data/dataset.py @torch . jit . export def preprocess_audio ( self , audio : Tensor , sample_rate : int ) -> Tensor : \"\"\"Apply some base transforms to the audio, that fix silent problems. It transforms all the audios to mono (depending on class creation parameter), remove the possible DC bias present and then resamples the audios to a common sample rate. Args: audio : Audio tensor sample_rate : Sample rate Returns: Audio tensor after the transforms. \"\"\" if self . force_mono and ( audio . shape [ 0 ] > 1 ): audio = audio . mean ( 0 , keepdim = True ) # Removing the dc component from the audio # It happens when a faulty capture device introduce # an offset into the recorded waveform, and this can # cause problems with later transforms. # https://en.wikipedia.org/wiki/DC_bias audio = audio - audio . mean ( 1 ) if self . sample_rate != sample_rate : audio = resample ( audio , orig_freq = float ( sample_rate ), new_freq = float ( self . sample_rate ) ) return audio BaseSpeechDataset __init__ ( self , items , force_mono = True , sample_rate = 16000 ) special This is the base class that implements the minimal functionality to have a compatible speech dataset, in a way that can be easily customized by subclassing. Parameters: Name Type Description Default items Sequence Source of items in the dataset, sorted by audio duration. This can be a list of files, a pandas dataframe or any other iterable structure where you record your data. required force_mono bool If true, convert all the loaded samples to mono. True sample_rate int Sample rate used by the dataset. All of the samples that have different rate will be resampled. 16000 Source code in thunder/data/dataset.py def __init__ ( self , items : Sequence , force_mono : bool = True , sample_rate : int = 16000 ): \"\"\"This is the base class that implements the minimal functionality to have a compatible speech dataset, in a way that can be easily customized by subclassing. Args: items : Source of items in the dataset, sorted by audio duration. This can be a list of files, a pandas dataframe or any other iterable structure where you record your data. force_mono : If true, convert all the loaded samples to mono. sample_rate : Sample rate used by the dataset. All of the samples that have different rate will be resampled. \"\"\" super () . __init__ () self . items = items self . loader = AudioFileLoader ( force_mono , sample_rate ) all_outputs ( self ) Return a list with just the outputs for the whole dataset. Useful when creating the initial vocab tokens, or to train a language model. Returns: Type Description List[str] All of the outputs of the dataset, with the corresponding preprocessing applied. Source code in thunder/data/dataset.py def all_outputs ( self ) -> List [ str ]: \"\"\"Return a list with just the outputs for the whole dataset. Useful when creating the initial vocab tokens, or to train a language model. Returns: All of the outputs of the dataset, with the corresponding preprocessing applied. \"\"\" outputs = [] for index in range ( len ( self )): item = self . get_item ( index ) text = self . open_text ( item ) text = self . preprocess_text ( text ) outputs . append ( text ) return outputs get_item ( self , index ) Get the item source specified by the index. Parameters: Name Type Description Default index int Indicates what item it needs to return information about. required Returns: Type Description Any Whatever data necessary to open the audio and text corresponding to this index. Source code in thunder/data/dataset.py def get_item ( self , index : int ) -> Any : \"\"\"Get the item source specified by the index. Args: index : Indicates what item it needs to return information about. Returns: Whatever data necessary to open the audio and text corresponding to this index. \"\"\" return self . items [ index ] open_audio ( self , item ) Uses the data returned by get_item to open the audio Parameters: Name Type Description Default item Any Data returned by get_item(index) required Returns: Type Description Tuple[torch.Tensor, int] Tuple containing the audio tensor with shape (channels, time), and the corresponding sample rate. Source code in thunder/data/dataset.py def open_audio ( self , item : Any ) -> Tuple [ Tensor , int ]: \"\"\"Uses the data returned by get_item to open the audio Args: item : Data returned by get_item(index) Returns: Tuple containing the audio tensor with shape (channels, time), and the corresponding sample rate. \"\"\" return self . loader . open_audio ( item ) open_text ( self , item ) Opens the transcription based on the data returned by get_item(index) Parameters: Name Type Description Default item Any The data returned by get_item. required Returns: Type Description str The transcription corresponding to the item. Source code in thunder/data/dataset.py def open_text ( self , item : Any ) -> str : \"\"\"Opens the transcription based on the data returned by get_item(index) Args: item : The data returned by get_item. Returns: The transcription corresponding to the item. \"\"\" raise NotImplementedError () preprocess_audio ( self , audio , sample_rate ) Apply some base transforms to the audio, that fix silent problems. It transforms all the audios to mono (depending on class creation parameter), remove the possible DC bias present and then resamples the audios to a common sample rate. Parameters: Name Type Description Default audio Tensor Audio tensor required sample_rate int Sample rate required Returns: Type Description Tensor Audio tensor after the transforms. Source code in thunder/data/dataset.py def preprocess_audio ( self , audio : Tensor , sample_rate : int ) -> Tensor : \"\"\"Apply some base transforms to the audio, that fix silent problems. It transforms all the audios to mono (depending on class creation parameter), remove the possible DC bias present and then resamples the audios to a common sample rate. Args: audio : Audio tensor sample_rate : Sample rate Returns: Audio tensor after the transforms. \"\"\" return self . loader . preprocess_audio ( audio , sample_rate ) preprocess_text ( self , text ) Add here preprocessing steps to remove some common problems in the text. Parameters: Name Type Description Default text str Label text required Returns: Type Description str Label text after processing Source code in thunder/data/dataset.py def preprocess_text ( self , text : str ) -> str : \"\"\"Add here preprocessing steps to remove some common problems in the text. Args: text : Label text Returns: Label text after processing \"\"\" return text ManifestSpeechDataset __init__ ( self , file , force_mono , sample_rate ) special Dataset that loads from nemo manifest files. Parameters: Name Type Description Default file Union[str, pathlib.Path] Nemo manifest file. required force_mono bool If true, convert all the loaded samples to mono. required sample_rate int Sample rate used by the dataset. All of the samples that have different rate will be resampled. required Source code in thunder/data/dataset.py def __init__ ( self , file : Union [ str , Path ], force_mono : bool , sample_rate : int ): \"\"\"Dataset that loads from nemo manifest files. Args: file : Nemo manifest file. force_mono : If true, convert all the loaded samples to mono. sample_rate : Sample rate used by the dataset. All of the samples that have different rate will be resampled. \"\"\" file = Path ( file ) # Reading from the manifest file items = [ json . loads ( line ) for line in file . read_text () . strip () . splitlines ()] super () . __init__ ( items , force_mono = force_mono , sample_rate = sample_rate ) open_audio ( self , item ) Uses the data returned by get_item to open the audio Parameters: Name Type Description Default item dict Data returned by get_item(index) required Returns: Type Description Tuple[torch.Tensor, int] Tuple containing the audio tensor with shape (channels, time), and the corresponding sample rate. Source code in thunder/data/dataset.py def open_audio ( self , item : dict ) -> Tuple [ Tensor , int ]: return self . loader . open_audio ( item [ \"audio_filepath\" ]) open_text ( self , item ) Opens the transcription based on the data returned by get_item(index) Parameters: Name Type Description Default item dict The data returned by get_item. required Returns: Type Description str The transcription corresponding to the item. Source code in thunder/data/dataset.py def open_text ( self , item : dict ) -> str : return item [ \"text\" ]","title":"Dataset"},{"location":"api/Data/dataset/#thunder.data.dataset.AudioFileLoader","text":"","title":"AudioFileLoader"},{"location":"api/Data/dataset/#thunder.data.dataset.AudioFileLoader.__init__","text":"Module containing the data loading and basic preprocessing. It's used internally by the datasets, but can be exported so that during inference time there's no code dependency. Parameters: Name Type Description Default force_mono bool If true, convert all the loaded samples to mono. True sample_rate int Sample rate used by the dataset. All of the samples that have different rate will be resampled. 16000 Source code in thunder/data/dataset.py def __init__ ( self , force_mono : bool = True , sample_rate : int = 16000 ): \"\"\"Module containing the data loading and basic preprocessing. It's used internally by the datasets, but can be exported so that during inference time there's no code dependency. Args: force_mono : If true, convert all the loaded samples to mono. sample_rate : Sample rate used by the dataset. All of the samples that have different rate will be resampled. \"\"\" super () . __init__ () self . force_mono = force_mono self . sample_rate = sample_rate","title":"__init__()"},{"location":"api/Data/dataset/#thunder.data.dataset.AudioFileLoader.forward","text":"Opens audio item and do basic preprocessing Parameters: Name Type Description Default item str Path to the audio to be opened required Returns: Type Description Tensor Audio tensor after preprocessing Source code in thunder/data/dataset.py def forward ( self , item : str ) -> Tensor : \"\"\"Opens audio item and do basic preprocessing Args: item : Path to the audio to be opened Returns: Audio tensor after preprocessing \"\"\" audio , sample_rate = self . open_audio ( item ) return self . preprocess_audio ( audio , sample_rate )","title":"forward()"},{"location":"api/Data/dataset/#thunder.data.dataset.AudioFileLoader.open_audio","text":"Uses the data returned by get_item to open the audio Parameters: Name Type Description Default item str Data returned by get_item(index) required Returns: Type Description Tuple[torch.Tensor, int] Tuple containing the audio tensor with shape (channels, time), and the corresponding sample rate. Source code in thunder/data/dataset.py @torch . jit . export def open_audio ( self , item : str ) -> Tuple [ Tensor , int ]: \"\"\"Uses the data returned by get_item to open the audio Args: item : Data returned by get_item(index) Returns: Tuple containing the audio tensor with shape (channels, time), and the corresponding sample rate. \"\"\" return torchaudio . load ( item )","title":"open_audio()"},{"location":"api/Data/dataset/#thunder.data.dataset.AudioFileLoader.preprocess_audio","text":"Apply some base transforms to the audio, that fix silent problems. It transforms all the audios to mono (depending on class creation parameter), remove the possible DC bias present and then resamples the audios to a common sample rate. Parameters: Name Type Description Default audio Tensor Audio tensor required sample_rate int Sample rate required Returns: Type Description Tensor Audio tensor after the transforms. Source code in thunder/data/dataset.py @torch . jit . export def preprocess_audio ( self , audio : Tensor , sample_rate : int ) -> Tensor : \"\"\"Apply some base transforms to the audio, that fix silent problems. It transforms all the audios to mono (depending on class creation parameter), remove the possible DC bias present and then resamples the audios to a common sample rate. Args: audio : Audio tensor sample_rate : Sample rate Returns: Audio tensor after the transforms. \"\"\" if self . force_mono and ( audio . shape [ 0 ] > 1 ): audio = audio . mean ( 0 , keepdim = True ) # Removing the dc component from the audio # It happens when a faulty capture device introduce # an offset into the recorded waveform, and this can # cause problems with later transforms. # https://en.wikipedia.org/wiki/DC_bias audio = audio - audio . mean ( 1 ) if self . sample_rate != sample_rate : audio = resample ( audio , orig_freq = float ( sample_rate ), new_freq = float ( self . sample_rate ) ) return audio","title":"preprocess_audio()"},{"location":"api/Data/dataset/#thunder.data.dataset.BaseSpeechDataset","text":"","title":"BaseSpeechDataset"},{"location":"api/Data/dataset/#thunder.data.dataset.BaseSpeechDataset.__init__","text":"This is the base class that implements the minimal functionality to have a compatible speech dataset, in a way that can be easily customized by subclassing. Parameters: Name Type Description Default items Sequence Source of items in the dataset, sorted by audio duration. This can be a list of files, a pandas dataframe or any other iterable structure where you record your data. required force_mono bool If true, convert all the loaded samples to mono. True sample_rate int Sample rate used by the dataset. All of the samples that have different rate will be resampled. 16000 Source code in thunder/data/dataset.py def __init__ ( self , items : Sequence , force_mono : bool = True , sample_rate : int = 16000 ): \"\"\"This is the base class that implements the minimal functionality to have a compatible speech dataset, in a way that can be easily customized by subclassing. Args: items : Source of items in the dataset, sorted by audio duration. This can be a list of files, a pandas dataframe or any other iterable structure where you record your data. force_mono : If true, convert all the loaded samples to mono. sample_rate : Sample rate used by the dataset. All of the samples that have different rate will be resampled. \"\"\" super () . __init__ () self . items = items self . loader = AudioFileLoader ( force_mono , sample_rate )","title":"__init__()"},{"location":"api/Data/dataset/#thunder.data.dataset.BaseSpeechDataset.all_outputs","text":"Return a list with just the outputs for the whole dataset. Useful when creating the initial vocab tokens, or to train a language model. Returns: Type Description List[str] All of the outputs of the dataset, with the corresponding preprocessing applied. Source code in thunder/data/dataset.py def all_outputs ( self ) -> List [ str ]: \"\"\"Return a list with just the outputs for the whole dataset. Useful when creating the initial vocab tokens, or to train a language model. Returns: All of the outputs of the dataset, with the corresponding preprocessing applied. \"\"\" outputs = [] for index in range ( len ( self )): item = self . get_item ( index ) text = self . open_text ( item ) text = self . preprocess_text ( text ) outputs . append ( text ) return outputs","title":"all_outputs()"},{"location":"api/Data/dataset/#thunder.data.dataset.BaseSpeechDataset.get_item","text":"Get the item source specified by the index. Parameters: Name Type Description Default index int Indicates what item it needs to return information about. required Returns: Type Description Any Whatever data necessary to open the audio and text corresponding to this index. Source code in thunder/data/dataset.py def get_item ( self , index : int ) -> Any : \"\"\"Get the item source specified by the index. Args: index : Indicates what item it needs to return information about. Returns: Whatever data necessary to open the audio and text corresponding to this index. \"\"\" return self . items [ index ]","title":"get_item()"},{"location":"api/Data/dataset/#thunder.data.dataset.BaseSpeechDataset.open_audio","text":"Uses the data returned by get_item to open the audio Parameters: Name Type Description Default item Any Data returned by get_item(index) required Returns: Type Description Tuple[torch.Tensor, int] Tuple containing the audio tensor with shape (channels, time), and the corresponding sample rate. Source code in thunder/data/dataset.py def open_audio ( self , item : Any ) -> Tuple [ Tensor , int ]: \"\"\"Uses the data returned by get_item to open the audio Args: item : Data returned by get_item(index) Returns: Tuple containing the audio tensor with shape (channels, time), and the corresponding sample rate. \"\"\" return self . loader . open_audio ( item )","title":"open_audio()"},{"location":"api/Data/dataset/#thunder.data.dataset.BaseSpeechDataset.open_text","text":"Opens the transcription based on the data returned by get_item(index) Parameters: Name Type Description Default item Any The data returned by get_item. required Returns: Type Description str The transcription corresponding to the item. Source code in thunder/data/dataset.py def open_text ( self , item : Any ) -> str : \"\"\"Opens the transcription based on the data returned by get_item(index) Args: item : The data returned by get_item. Returns: The transcription corresponding to the item. \"\"\" raise NotImplementedError ()","title":"open_text()"},{"location":"api/Data/dataset/#thunder.data.dataset.BaseSpeechDataset.preprocess_audio","text":"Apply some base transforms to the audio, that fix silent problems. It transforms all the audios to mono (depending on class creation parameter), remove the possible DC bias present and then resamples the audios to a common sample rate. Parameters: Name Type Description Default audio Tensor Audio tensor required sample_rate int Sample rate required Returns: Type Description Tensor Audio tensor after the transforms. Source code in thunder/data/dataset.py def preprocess_audio ( self , audio : Tensor , sample_rate : int ) -> Tensor : \"\"\"Apply some base transforms to the audio, that fix silent problems. It transforms all the audios to mono (depending on class creation parameter), remove the possible DC bias present and then resamples the audios to a common sample rate. Args: audio : Audio tensor sample_rate : Sample rate Returns: Audio tensor after the transforms. \"\"\" return self . loader . preprocess_audio ( audio , sample_rate )","title":"preprocess_audio()"},{"location":"api/Data/dataset/#thunder.data.dataset.BaseSpeechDataset.preprocess_text","text":"Add here preprocessing steps to remove some common problems in the text. Parameters: Name Type Description Default text str Label text required Returns: Type Description str Label text after processing Source code in thunder/data/dataset.py def preprocess_text ( self , text : str ) -> str : \"\"\"Add here preprocessing steps to remove some common problems in the text. Args: text : Label text Returns: Label text after processing \"\"\" return text","title":"preprocess_text()"},{"location":"api/Data/dataset/#thunder.data.dataset.ManifestSpeechDataset","text":"","title":"ManifestSpeechDataset"},{"location":"api/Data/dataset/#thunder.data.dataset.ManifestSpeechDataset.__init__","text":"Dataset that loads from nemo manifest files. Parameters: Name Type Description Default file Union[str, pathlib.Path] Nemo manifest file. required force_mono bool If true, convert all the loaded samples to mono. required sample_rate int Sample rate used by the dataset. All of the samples that have different rate will be resampled. required Source code in thunder/data/dataset.py def __init__ ( self , file : Union [ str , Path ], force_mono : bool , sample_rate : int ): \"\"\"Dataset that loads from nemo manifest files. Args: file : Nemo manifest file. force_mono : If true, convert all the loaded samples to mono. sample_rate : Sample rate used by the dataset. All of the samples that have different rate will be resampled. \"\"\" file = Path ( file ) # Reading from the manifest file items = [ json . loads ( line ) for line in file . read_text () . strip () . splitlines ()] super () . __init__ ( items , force_mono = force_mono , sample_rate = sample_rate )","title":"__init__()"},{"location":"api/Data/dataset/#thunder.data.dataset.ManifestSpeechDataset.open_audio","text":"Uses the data returned by get_item to open the audio Parameters: Name Type Description Default item dict Data returned by get_item(index) required Returns: Type Description Tuple[torch.Tensor, int] Tuple containing the audio tensor with shape (channels, time), and the corresponding sample rate. Source code in thunder/data/dataset.py def open_audio ( self , item : dict ) -> Tuple [ Tensor , int ]: return self . loader . open_audio ( item [ \"audio_filepath\" ])","title":"open_audio()"},{"location":"api/Data/dataset/#thunder.data.dataset.ManifestSpeechDataset.open_text","text":"Opens the transcription based on the data returned by get_item(index) Parameters: Name Type Description Default item dict The data returned by get_item. required Returns: Type Description str The transcription corresponding to the item. Source code in thunder/data/dataset.py def open_text ( self , item : dict ) -> str : return item [ \"text\" ]","title":"open_text()"},{"location":"api/Quartznet/blocks/","text":"Basic building blocks to create the Quartznet model EncoderConfig dataclass Configuration to create Quartznet_encoder Attributes: Name Type Description feat_in int Number of input features to the model. defaults to 64. filters List[int] List of filter sizes used to create the encoder blocks. defaults to [256, 256, 512, 512, 512]. kernel_sizes List[int] List of kernel sizes corresponding to each filter size. defaults to [33, 39, 51, 63, 75]. repeat_blocks int Number of repetitions of each block. defaults to 1. InitMode Weight init methods. Used by init_weights . Note Possible values are xavier_uniform , xavier_normal , kaiming_uniform and kaiming_normal MaskedBatchNorm1d forward ( self , x ) Defines the computation performed at every call. Should be overridden by all subclasses. .. note:: Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them. Source code in thunder/quartznet/blocks.py def forward ( self , x : torch . Tensor ) -> torch . Tensor : if self . training : mask = x == 0.0 result = self . layer ( x ) return torch . masked_fill ( result , mask , 0.0 ) else : return self . layer ( x ) QuartznetBlock __init__ ( self , in_channels , out_channels , repeat = 5 , kernel_size = ( 11 ,), stride = ( 1 ,), dilation = ( 1 ,), dropout = 0.0 , residual = True , separable = False ) special Quartznet block. This is a refactoring of the Jasperblock present on the NeMo toolkit, but simplified to only support the new quartznet model. Biggest change is that dense residual used on Jasper is not supported here, and masked convolutions were also removed. Parameters: Name Type Description Default in_channels int Number of input channels required out_channels int Number of output channels required repeat int Repetitions inside block. 5 kernel_size Union[int, Tuple[int]] Kernel size. (11,) stride Union[int, Tuple[int]] Stride of each repetition. (1,) dilation Union[int, Tuple[int]] Dilation of each repetition. (1,) dropout float Dropout used before each activation. 0.0 residual bool Controls the use of residual connection. True separable bool Controls the use of separable convolutions. False Source code in thunder/quartznet/blocks.py def __init__ ( self , in_channels : int , out_channels : int , repeat : int = 5 , kernel_size : _size_1_t = ( 11 ,), stride : _size_1_t = ( 1 ,), dilation : _size_1_t = ( 1 ,), dropout : float = 0.0 , residual : bool = True , separable : bool = False , ): \"\"\"Quartznet block. This is a refactoring of the Jasperblock present on the NeMo toolkit, but simplified to only support the new quartznet model. Biggest change is that dense residual used on Jasper is not supported here, and masked convolutions were also removed. Args: in_channels : Number of input channels out_channels : Number of output channels repeat : Repetitions inside block. kernel_size : Kernel size. stride : Stride of each repetition. dilation : Dilation of each repetition. dropout : Dropout used before each activation. residual : Controls the use of residual connection. separable : Controls the use of separable convolutions. \"\"\" super () . __init__ () padding_val = get_same_padding ( kernel_size [ 0 ], stride [ 0 ], dilation [ 0 ]) inplanes_loop = in_channels conv = [] for _ in range ( repeat - 1 ): conv . extend ( _get_conv_bn_layer ( inplanes_loop , out_channels , kernel_size = kernel_size , stride = stride , dilation = dilation , padding = padding_val , separable = separable , bias = False , ) ) conv . extend ( _get_act_dropout_layer ( drop_prob = dropout )) inplanes_loop = out_channels conv . extend ( _get_conv_bn_layer ( inplanes_loop , out_channels , kernel_size = kernel_size , stride = stride , dilation = dilation , padding = padding_val , separable = separable , bias = False , ) ) self . mconv = nn . Sequential ( * conv ) if residual : stride_residual = stride if stride [ 0 ] == 1 else stride [ 0 ] ** repeat self . res = nn . Sequential ( * _get_conv_bn_layer ( in_channels , out_channels , kernel_size = 1 , stride = stride_residual , bias = False , ) ) else : self . res = None self . mout = nn . Sequential ( * _get_act_dropout_layer ( drop_prob = dropout )) forward ( self , x ) Parameters: Name Type Description Default x Tensor Tensor of shape (batch, features, time) where #features == inplanes required Returns: Type Description Tensor Result of applying the block on the input, and corresponding output lengths Source code in thunder/quartznet/blocks.py def forward ( self , x : torch . Tensor ) -> torch . Tensor : \"\"\" Args: x : Tensor of shape (batch, features, time) where #features == inplanes Returns: Result of applying the block on the input, and corresponding output lengths \"\"\" # compute forward convolutions out = self . mconv ( x ) # compute the residuals if self . res is not None : res_out = self . res ( x ) out = out + res_out # compute the output return self . mout ( out ) body ( filters , kernel_size , repeat_blocks = 1 ) Creates the body of the Quartznet model. That is the middle part. Parameters: Name Type Description Default filters List[int] List of filters inside each block in the body. required kernel_size List[int] Corresponding list of kernel sizes for each block. Should have the same length as the first argument. required repeat_blocks int Number of repetitions of each block inside the body. 1 Returns: Type Description List[thunder.quartznet.blocks.QuartznetBlock] List of layers that form the body of the network. Source code in thunder/quartznet/blocks.py def body ( filters : List [ int ], kernel_size : List [ int ], repeat_blocks : int = 1 ) -> List [ QuartznetBlock ]: \"\"\"Creates the body of the Quartznet model. That is the middle part. Args: filters : List of filters inside each block in the body. kernel_size : Corresponding list of kernel sizes for each block. Should have the same length as the first argument. repeat_blocks : Number of repetitions of each block inside the body. Returns: List of layers that form the body of the network. \"\"\" layers = [] f_in = 256 for f , k in zip ( filters , kernel_size ): for _ in range ( repeat_blocks ): layers . append ( QuartznetBlock ( f_in , f , kernel_size = ( k ,), separable = True )) f_in = f layers . extend ( [ QuartznetBlock ( f_in , 512 , repeat = 1 , dilation = ( 2 ,), kernel_size = ( 87 ,), residual = False , separable = True , ), QuartznetBlock ( 512 , 1024 , repeat = 1 , kernel_size = ( 1 ,), residual = False , separable = False ), ] ) return layers init_weights ( m , mode =< InitMode . xavier_uniform : 'xavier_uniform' > ) Initialize Linear, Conv1d or BatchNorm1d weights. There's no return, the operation occurs inplace. Parameters: Name Type Description Default m Module The layer to be initialized required mode InitMode Weight initialization mode. Only applicable to linear and conv layers. <InitMode.xavier_uniform: 'xavier_uniform'> Exceptions: Type Description ValueError Raised when the initial mode is not one of the possible options. Source code in thunder/quartznet/blocks.py def init_weights ( m : nn . Module , mode : InitMode = InitMode . xavier_uniform ): \"\"\"Initialize Linear, Conv1d or BatchNorm1d weights. There's no return, the operation occurs inplace. Args: m: The layer to be initialized mode: Weight initialization mode. Only applicable to linear and conv layers. Raises: ValueError: Raised when the initial mode is not one of the possible options. \"\"\" if isinstance ( m , ( nn . Conv1d , nn . Linear )): if mode == InitMode . xavier_uniform : nn . init . xavier_uniform_ ( m . weight , gain = 1.0 ) elif mode == InitMode . xavier_normal : nn . init . xavier_normal_ ( m . weight , gain = 1.0 ) elif mode == InitMode . kaiming_uniform : nn . init . kaiming_uniform_ ( m . weight , nonlinearity = \"relu\" ) elif mode == InitMode . kaiming_normal : nn . init . kaiming_normal_ ( m . weight , nonlinearity = \"relu\" ) else : raise ValueError ( \"Unknown Initialization mode: {0} \" . format ( mode )) elif isinstance ( m , nn . BatchNorm1d ): if m . track_running_stats : m . running_mean . zero_ () m . running_var . fill_ ( 1 ) m . num_batches_tracked . zero_ () if m . affine : nn . init . ones_ ( m . weight ) nn . init . zeros_ ( m . bias ) Quartznet_encoder ( cfg = EncoderConfig ( feat_in = 64 , filters = [ 256 , 256 , 512 , 512 , 512 ], kernel_sizes = [ 33 , 39 , 51 , 63 , 75 ], repeat_blocks = 1 )) Basic Quartznet encoder setup. Can be used to build either Quartznet5x5 (repeat_blocks=1) or Quartznet15x5 (repeat_blocks=3) Parameters: Name Type Description Default cfg EncoderConfig required config to create instance EncoderConfig(feat_in=64, filters=[256, 256, 512, 512, 512], kernel_sizes=[33, 39, 51, 63, 75], repeat_blocks=1) Returns: Type Description Module Pytorch model corresponding to the encoder. Source code in thunder/quartznet/blocks.py def Quartznet_encoder ( cfg : EncoderConfig = EncoderConfig ()) -> nn . Module : \"\"\"Basic Quartznet encoder setup. Can be used to build either Quartznet5x5 (repeat_blocks=1) or Quartznet15x5 (repeat_blocks=3) Args: cfg: required config to create instance Returns: Pytorch model corresponding to the encoder. \"\"\" return nn . Sequential ( stem ( cfg . feat_in ), * body ( cfg . filters , cfg . kernel_sizes , cfg . repeat_blocks ), ) stem ( feat_in ) Creates the Quartznet stem. That is the first block of the model, that process the input directly. Parameters: Name Type Description Default feat_in int Number of input features required Returns: Type Description QuartznetBlock Quartznet stem block Source code in thunder/quartznet/blocks.py def stem ( feat_in : int ) -> QuartznetBlock : \"\"\"Creates the Quartznet stem. That is the first block of the model, that process the input directly. Args: feat_in : Number of input features Returns: Quartznet stem block \"\"\" return QuartznetBlock ( feat_in , 256 , repeat = 1 , stride = ( 2 ,), kernel_size = ( 33 ,), residual = False , separable = True , )","title":"Blocks"},{"location":"api/Quartznet/blocks/#thunder.quartznet.blocks.EncoderConfig","text":"Configuration to create Quartznet_encoder Attributes: Name Type Description feat_in int Number of input features to the model. defaults to 64. filters List[int] List of filter sizes used to create the encoder blocks. defaults to [256, 256, 512, 512, 512]. kernel_sizes List[int] List of kernel sizes corresponding to each filter size. defaults to [33, 39, 51, 63, 75]. repeat_blocks int Number of repetitions of each block. defaults to 1.","title":"EncoderConfig"},{"location":"api/Quartznet/blocks/#thunder.quartznet.blocks.InitMode","text":"Weight init methods. Used by init_weights . Note Possible values are xavier_uniform , xavier_normal , kaiming_uniform and kaiming_normal","title":"InitMode"},{"location":"api/Quartznet/blocks/#thunder.quartznet.blocks.MaskedBatchNorm1d","text":"","title":"MaskedBatchNorm1d"},{"location":"api/Quartznet/blocks/#thunder.quartznet.blocks.MaskedBatchNorm1d.forward","text":"Defines the computation performed at every call. Should be overridden by all subclasses. .. note:: Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them. Source code in thunder/quartznet/blocks.py def forward ( self , x : torch . Tensor ) -> torch . Tensor : if self . training : mask = x == 0.0 result = self . layer ( x ) return torch . masked_fill ( result , mask , 0.0 ) else : return self . layer ( x )","title":"forward()"},{"location":"api/Quartznet/blocks/#thunder.quartznet.blocks.QuartznetBlock","text":"","title":"QuartznetBlock"},{"location":"api/Quartznet/blocks/#thunder.quartznet.blocks.QuartznetBlock.__init__","text":"Quartznet block. This is a refactoring of the Jasperblock present on the NeMo toolkit, but simplified to only support the new quartznet model. Biggest change is that dense residual used on Jasper is not supported here, and masked convolutions were also removed. Parameters: Name Type Description Default in_channels int Number of input channels required out_channels int Number of output channels required repeat int Repetitions inside block. 5 kernel_size Union[int, Tuple[int]] Kernel size. (11,) stride Union[int, Tuple[int]] Stride of each repetition. (1,) dilation Union[int, Tuple[int]] Dilation of each repetition. (1,) dropout float Dropout used before each activation. 0.0 residual bool Controls the use of residual connection. True separable bool Controls the use of separable convolutions. False Source code in thunder/quartznet/blocks.py def __init__ ( self , in_channels : int , out_channels : int , repeat : int = 5 , kernel_size : _size_1_t = ( 11 ,), stride : _size_1_t = ( 1 ,), dilation : _size_1_t = ( 1 ,), dropout : float = 0.0 , residual : bool = True , separable : bool = False , ): \"\"\"Quartznet block. This is a refactoring of the Jasperblock present on the NeMo toolkit, but simplified to only support the new quartznet model. Biggest change is that dense residual used on Jasper is not supported here, and masked convolutions were also removed. Args: in_channels : Number of input channels out_channels : Number of output channels repeat : Repetitions inside block. kernel_size : Kernel size. stride : Stride of each repetition. dilation : Dilation of each repetition. dropout : Dropout used before each activation. residual : Controls the use of residual connection. separable : Controls the use of separable convolutions. \"\"\" super () . __init__ () padding_val = get_same_padding ( kernel_size [ 0 ], stride [ 0 ], dilation [ 0 ]) inplanes_loop = in_channels conv = [] for _ in range ( repeat - 1 ): conv . extend ( _get_conv_bn_layer ( inplanes_loop , out_channels , kernel_size = kernel_size , stride = stride , dilation = dilation , padding = padding_val , separable = separable , bias = False , ) ) conv . extend ( _get_act_dropout_layer ( drop_prob = dropout )) inplanes_loop = out_channels conv . extend ( _get_conv_bn_layer ( inplanes_loop , out_channels , kernel_size = kernel_size , stride = stride , dilation = dilation , padding = padding_val , separable = separable , bias = False , ) ) self . mconv = nn . Sequential ( * conv ) if residual : stride_residual = stride if stride [ 0 ] == 1 else stride [ 0 ] ** repeat self . res = nn . Sequential ( * _get_conv_bn_layer ( in_channels , out_channels , kernel_size = 1 , stride = stride_residual , bias = False , ) ) else : self . res = None self . mout = nn . Sequential ( * _get_act_dropout_layer ( drop_prob = dropout ))","title":"__init__()"},{"location":"api/Quartznet/blocks/#thunder.quartznet.blocks.QuartznetBlock.forward","text":"Parameters: Name Type Description Default x Tensor Tensor of shape (batch, features, time) where #features == inplanes required Returns: Type Description Tensor Result of applying the block on the input, and corresponding output lengths Source code in thunder/quartznet/blocks.py def forward ( self , x : torch . Tensor ) -> torch . Tensor : \"\"\" Args: x : Tensor of shape (batch, features, time) where #features == inplanes Returns: Result of applying the block on the input, and corresponding output lengths \"\"\" # compute forward convolutions out = self . mconv ( x ) # compute the residuals if self . res is not None : res_out = self . res ( x ) out = out + res_out # compute the output return self . mout ( out )","title":"forward()"},{"location":"api/Quartznet/blocks/#thunder.quartznet.blocks.body","text":"Creates the body of the Quartznet model. That is the middle part. Parameters: Name Type Description Default filters List[int] List of filters inside each block in the body. required kernel_size List[int] Corresponding list of kernel sizes for each block. Should have the same length as the first argument. required repeat_blocks int Number of repetitions of each block inside the body. 1 Returns: Type Description List[thunder.quartznet.blocks.QuartznetBlock] List of layers that form the body of the network. Source code in thunder/quartznet/blocks.py def body ( filters : List [ int ], kernel_size : List [ int ], repeat_blocks : int = 1 ) -> List [ QuartznetBlock ]: \"\"\"Creates the body of the Quartznet model. That is the middle part. Args: filters : List of filters inside each block in the body. kernel_size : Corresponding list of kernel sizes for each block. Should have the same length as the first argument. repeat_blocks : Number of repetitions of each block inside the body. Returns: List of layers that form the body of the network. \"\"\" layers = [] f_in = 256 for f , k in zip ( filters , kernel_size ): for _ in range ( repeat_blocks ): layers . append ( QuartznetBlock ( f_in , f , kernel_size = ( k ,), separable = True )) f_in = f layers . extend ( [ QuartznetBlock ( f_in , 512 , repeat = 1 , dilation = ( 2 ,), kernel_size = ( 87 ,), residual = False , separable = True , ), QuartznetBlock ( 512 , 1024 , repeat = 1 , kernel_size = ( 1 ,), residual = False , separable = False ), ] ) return layers","title":"body()"},{"location":"api/Quartznet/blocks/#thunder.quartznet.blocks.init_weights","text":"Initialize Linear, Conv1d or BatchNorm1d weights. There's no return, the operation occurs inplace. Parameters: Name Type Description Default m Module The layer to be initialized required mode InitMode Weight initialization mode. Only applicable to linear and conv layers. <InitMode.xavier_uniform: 'xavier_uniform'> Exceptions: Type Description ValueError Raised when the initial mode is not one of the possible options. Source code in thunder/quartznet/blocks.py def init_weights ( m : nn . Module , mode : InitMode = InitMode . xavier_uniform ): \"\"\"Initialize Linear, Conv1d or BatchNorm1d weights. There's no return, the operation occurs inplace. Args: m: The layer to be initialized mode: Weight initialization mode. Only applicable to linear and conv layers. Raises: ValueError: Raised when the initial mode is not one of the possible options. \"\"\" if isinstance ( m , ( nn . Conv1d , nn . Linear )): if mode == InitMode . xavier_uniform : nn . init . xavier_uniform_ ( m . weight , gain = 1.0 ) elif mode == InitMode . xavier_normal : nn . init . xavier_normal_ ( m . weight , gain = 1.0 ) elif mode == InitMode . kaiming_uniform : nn . init . kaiming_uniform_ ( m . weight , nonlinearity = \"relu\" ) elif mode == InitMode . kaiming_normal : nn . init . kaiming_normal_ ( m . weight , nonlinearity = \"relu\" ) else : raise ValueError ( \"Unknown Initialization mode: {0} \" . format ( mode )) elif isinstance ( m , nn . BatchNorm1d ): if m . track_running_stats : m . running_mean . zero_ () m . running_var . fill_ ( 1 ) m . num_batches_tracked . zero_ () if m . affine : nn . init . ones_ ( m . weight ) nn . init . zeros_ ( m . bias )","title":"init_weights()"},{"location":"api/Quartznet/blocks/#thunder.quartznet.blocks.Quartznet_encoder","text":"Basic Quartznet encoder setup. Can be used to build either Quartznet5x5 (repeat_blocks=1) or Quartznet15x5 (repeat_blocks=3) Parameters: Name Type Description Default cfg EncoderConfig required config to create instance EncoderConfig(feat_in=64, filters=[256, 256, 512, 512, 512], kernel_sizes=[33, 39, 51, 63, 75], repeat_blocks=1) Returns: Type Description Module Pytorch model corresponding to the encoder. Source code in thunder/quartznet/blocks.py def Quartznet_encoder ( cfg : EncoderConfig = EncoderConfig ()) -> nn . Module : \"\"\"Basic Quartznet encoder setup. Can be used to build either Quartznet5x5 (repeat_blocks=1) or Quartznet15x5 (repeat_blocks=3) Args: cfg: required config to create instance Returns: Pytorch model corresponding to the encoder. \"\"\" return nn . Sequential ( stem ( cfg . feat_in ), * body ( cfg . filters , cfg . kernel_sizes , cfg . repeat_blocks ), )","title":"Quartznet_encoder()"},{"location":"api/Quartznet/blocks/#thunder.quartznet.blocks.stem","text":"Creates the Quartznet stem. That is the first block of the model, that process the input directly. Parameters: Name Type Description Default feat_in int Number of input features required Returns: Type Description QuartznetBlock Quartznet stem block Source code in thunder/quartznet/blocks.py def stem ( feat_in : int ) -> QuartznetBlock : \"\"\"Creates the Quartznet stem. That is the first block of the model, that process the input directly. Args: feat_in : Number of input features Returns: Quartznet stem block \"\"\" return QuartznetBlock ( feat_in , 256 , repeat = 1 , stride = ( 2 ,), kernel_size = ( 33 ,), residual = False , separable = True , )","title":"stem()"},{"location":"api/Quartznet/compatibility/","text":"Helper functions to load the Quartznet model from original Nemo released checkpoint files. QuartznetCheckpoint Trained model weight checkpoints. Used by download_checkpoint and QuartznetModule.load_from_nemo . Note Possible values are QuartzNet15x5Base_En , QuartzNet15x5Base_Zh , QuartzNet5x5LS_En , QuartzNet15x5NR_En , stt_ca_quartznet15x5 , stt_it_quartznet15x5 , stt_fr_quartznet15x5 , stt_es_quartznet15x5 , stt_de_quartznet15x5 , stt_pl_quartznet15x5 , stt_ru_quartznet15x5 , stt_en_quartznet15x5 , stt_zh_quartznet15x5 load_quartznet_weights ( encoder , decoder , weights_path ) Load Quartznet model weights from data present inside .nemo file Parameters: Name Type Description Default encoder Module Encoder module to load the weights into required decoder Module Decoder module to load the weights into required weights_path str Path to the pytorch weights checkpoint required Source code in thunder/quartznet/compatibility.py def load_quartznet_weights ( encoder : nn . Module , decoder : nn . Module , weights_path : str ): \"\"\"Load Quartznet model weights from data present inside .nemo file Args: encoder: Encoder module to load the weights into decoder: Decoder module to load the weights into weights_path : Path to the pytorch weights checkpoint \"\"\" weights = torch . load ( weights_path ) def fix_encoder_name ( x : str ) -> str : x = x . replace ( \"encoder.\" , \"\" ) . replace ( \".res.0\" , \".res\" ) # Add another abstraction layer # This is caused by the new MaskedBatchNorm # Skip convolution and SqueezeExcite layers if ( \".conv\" not in x ) and ( \".fc\" not in x ): parts = x . split ( \".\" ) x = \".\" . join ( parts [: 3 ] + [ \"layer\" ] + parts [ 3 :]) return x . replace ( \".conv\" , \"\" ) # Fix convolutions # We remove the 'encoder.' and 'decoder.' prefix from the weights to enable # compatibility to load with plain nn.Modules created by reading the config encoder_weights = { fix_encoder_name ( k ): v for k , v in weights . items () if \"encoder\" in k } encoder . load_state_dict ( encoder_weights , strict = True ) decoder_weights = { k . replace ( \"decoder.decoder_layers.0.\" , \"\" ): v for k , v in weights . items () if \"decoder\" in k } decoder . load_state_dict ( decoder_weights , strict = True ) read_params_from_config ( config_path ) Read the important parameters from the config stored inside the .nemo checkpoint. Parameters: Name Type Description Default config_path Union[str, pathlib.Path] Path to the .yaml file, usually called model_config.yaml required Returns: Type Description Tuple[Dict, List[str], Dict] A tuple containing, in this order, the encoder hyperparameters, the vocabulary, and the preprocessing hyperparameters Source code in thunder/quartznet/compatibility.py def read_params_from_config ( config_path : Union [ str , Path ] ) -> Tuple [ Dict , List [ str ], Dict ]: \"\"\"Read the important parameters from the config stored inside the .nemo checkpoint. Args: config_path : Path to the .yaml file, usually called model_config.yaml Returns: A tuple containing, in this order, the encoder hyperparameters, the vocabulary, and the preprocessing hyperparameters \"\"\" conf = OmegaConf . load ( config_path ) encoder_params = conf [ \"encoder\" ][ \"params\" ] quartznet_conf = OmegaConf . to_container ( encoder_params [ \"jasper\" ]) body_config = quartznet_conf [ 1 : - 2 ] filters = [ cfg [ \"filters\" ] for cfg in body_config ] kernel_sizes = [ cfg [ \"kernel\" ][ 0 ] for cfg in body_config ] encoder_cfg = { \"filters\" : filters , \"kernel_sizes\" : kernel_sizes , } preprocess = conf [ \"preprocessor\" ][ \"params\" ] preprocess_cfg = { \"sample_rate\" : preprocess [ \"sample_rate\" ], \"n_window_size\" : int ( preprocess [ \"window_size\" ] * preprocess [ \"sample_rate\" ]), \"n_window_stride\" : int ( preprocess [ \"window_stride\" ] * preprocess [ \"sample_rate\" ]), \"n_fft\" : preprocess [ \"n_fft\" ], \"nfilt\" : preprocess [ \"features\" ], \"dither\" : preprocess [ \"dither\" ], } labels = ( conf [ \"labels\" ] if \"labels\" in conf else conf [ \"decoder\" ][ \"params\" ][ \"vocabulary\" ] ) return ( encoder_cfg , OmegaConf . to_container ( labels ), preprocess_cfg , )","title":"Compatibility"},{"location":"api/Quartznet/compatibility/#thunder.quartznet.compatibility.QuartznetCheckpoint","text":"Trained model weight checkpoints. Used by download_checkpoint and QuartznetModule.load_from_nemo . Note Possible values are QuartzNet15x5Base_En , QuartzNet15x5Base_Zh , QuartzNet5x5LS_En , QuartzNet15x5NR_En , stt_ca_quartznet15x5 , stt_it_quartznet15x5 , stt_fr_quartznet15x5 , stt_es_quartznet15x5 , stt_de_quartznet15x5 , stt_pl_quartznet15x5 , stt_ru_quartznet15x5 , stt_en_quartznet15x5 , stt_zh_quartznet15x5","title":"QuartznetCheckpoint"},{"location":"api/Quartznet/compatibility/#thunder.quartznet.compatibility.load_quartznet_weights","text":"Load Quartznet model weights from data present inside .nemo file Parameters: Name Type Description Default encoder Module Encoder module to load the weights into required decoder Module Decoder module to load the weights into required weights_path str Path to the pytorch weights checkpoint required Source code in thunder/quartznet/compatibility.py def load_quartznet_weights ( encoder : nn . Module , decoder : nn . Module , weights_path : str ): \"\"\"Load Quartznet model weights from data present inside .nemo file Args: encoder: Encoder module to load the weights into decoder: Decoder module to load the weights into weights_path : Path to the pytorch weights checkpoint \"\"\" weights = torch . load ( weights_path ) def fix_encoder_name ( x : str ) -> str : x = x . replace ( \"encoder.\" , \"\" ) . replace ( \".res.0\" , \".res\" ) # Add another abstraction layer # This is caused by the new MaskedBatchNorm # Skip convolution and SqueezeExcite layers if ( \".conv\" not in x ) and ( \".fc\" not in x ): parts = x . split ( \".\" ) x = \".\" . join ( parts [: 3 ] + [ \"layer\" ] + parts [ 3 :]) return x . replace ( \".conv\" , \"\" ) # Fix convolutions # We remove the 'encoder.' and 'decoder.' prefix from the weights to enable # compatibility to load with plain nn.Modules created by reading the config encoder_weights = { fix_encoder_name ( k ): v for k , v in weights . items () if \"encoder\" in k } encoder . load_state_dict ( encoder_weights , strict = True ) decoder_weights = { k . replace ( \"decoder.decoder_layers.0.\" , \"\" ): v for k , v in weights . items () if \"decoder\" in k } decoder . load_state_dict ( decoder_weights , strict = True )","title":"load_quartznet_weights()"},{"location":"api/Quartznet/compatibility/#thunder.quartznet.compatibility.read_params_from_config","text":"Read the important parameters from the config stored inside the .nemo checkpoint. Parameters: Name Type Description Default config_path Union[str, pathlib.Path] Path to the .yaml file, usually called model_config.yaml required Returns: Type Description Tuple[Dict, List[str], Dict] A tuple containing, in this order, the encoder hyperparameters, the vocabulary, and the preprocessing hyperparameters Source code in thunder/quartznet/compatibility.py def read_params_from_config ( config_path : Union [ str , Path ] ) -> Tuple [ Dict , List [ str ], Dict ]: \"\"\"Read the important parameters from the config stored inside the .nemo checkpoint. Args: config_path : Path to the .yaml file, usually called model_config.yaml Returns: A tuple containing, in this order, the encoder hyperparameters, the vocabulary, and the preprocessing hyperparameters \"\"\" conf = OmegaConf . load ( config_path ) encoder_params = conf [ \"encoder\" ][ \"params\" ] quartznet_conf = OmegaConf . to_container ( encoder_params [ \"jasper\" ]) body_config = quartznet_conf [ 1 : - 2 ] filters = [ cfg [ \"filters\" ] for cfg in body_config ] kernel_sizes = [ cfg [ \"kernel\" ][ 0 ] for cfg in body_config ] encoder_cfg = { \"filters\" : filters , \"kernel_sizes\" : kernel_sizes , } preprocess = conf [ \"preprocessor\" ][ \"params\" ] preprocess_cfg = { \"sample_rate\" : preprocess [ \"sample_rate\" ], \"n_window_size\" : int ( preprocess [ \"window_size\" ] * preprocess [ \"sample_rate\" ]), \"n_window_stride\" : int ( preprocess [ \"window_stride\" ] * preprocess [ \"sample_rate\" ]), \"n_fft\" : preprocess [ \"n_fft\" ], \"nfilt\" : preprocess [ \"features\" ], \"dither\" : preprocess [ \"dither\" ], } labels = ( conf [ \"labels\" ] if \"labels\" in conf else conf [ \"decoder\" ][ \"params\" ][ \"vocabulary\" ] ) return ( encoder_cfg , OmegaConf . to_container ( labels ), preprocess_cfg , )","title":"read_params_from_config()"},{"location":"api/Quartznet/module/","text":"OptimizerConfig dataclass Configuration used by the optimizer Attributes: Name Type Description learning_rate float learning rate. defaults to 3e-4. betas Tuple[float] beta1 and beta2 used by adam. defaults to (0.8, 0.5), similar to the novograd values on nemo. QuartznetModule __init__ ( self , text_cfg , encoder_cfg = EncoderConfig ( feat_in = 64 , filters = [ 256 , 256 , 512 , 512 , 512 ], kernel_sizes = [ 33 , 39 , 51 , 63 , 75 ], repeat_blocks = 1 ), audio_cfg = FilterbankConfig ( sample_rate = 16000 , n_window_size = 320 , n_window_stride = 160 , n_fft = 512 , preemph = 0.97 , nfilt = 64 , dither = 1e-05 ), optim_cfg = OptimizerConfig ( learning_rate = 0.0003 , betas = ( 0.8 , 0.5 ))) special Module containing both the basic quartznet model and helper functionality, such as feature creation and text processing. Parameters: Name Type Description Default text_cfg TextTransformConfig Configuration for the text processing pipeline required encoder_cfg EncoderConfig Configuration for the quartznet encoder EncoderConfig(feat_in=64, filters=[256, 256, 512, 512, 512], kernel_sizes=[33, 39, 51, 63, 75], repeat_blocks=1) audio_cfg FilterbankConfig Configuration for the filterbank features applied to the input audio FilterbankConfig(sample_rate=16000, n_window_size=320, n_window_stride=160, n_fft=512, preemph=0.97, nfilt=64, dither=1e-05) optim_cfg OptimizerConfig Configuration for the optimizer used during training OptimizerConfig(learning_rate=0.0003, betas=(0.8, 0.5)) Source code in thunder/quartznet/module.py def __init__ ( self , text_cfg : TextTransformConfig , encoder_cfg : EncoderConfig = EncoderConfig (), audio_cfg : FilterbankConfig = FilterbankConfig (), optim_cfg : OptimizerConfig = OptimizerConfig (), ): \"\"\"Module containing both the basic quartznet model and helper functionality, such as feature creation and text processing. Args: text_cfg: Configuration for the text processing pipeline encoder_cfg: Configuration for the quartznet encoder audio_cfg: Configuration for the filterbank features applied to the input audio optim_cfg: Configuration for the optimizer used during training \"\"\" super () . __init__ () self . save_hyperparameters () self . audio_transform = FilterbankFeatures ( audio_cfg ) self . encoder = Quartznet_encoder ( encoder_cfg ) self . text_transform = BatchTextTransformer ( text_cfg ) self . decoder = conv1d_decoder ( 1024 , len ( self . text_transform . vocab )) # Metrics self . val_cer = CER () self . val_wer = WER () # Example input is one second of fake audio self . example_input_array = torch . randn (( 10 , audio_cfg . sample_rate )) change_vocab ( self , text_cfg ) Changes the vocabulary of the model. useful when finetuning to another language. Parameters: Name Type Description Default text_cfg TextTransformConfig Configuration for the text processing pipeline required Source code in thunder/quartznet/module.py def change_vocab ( self , text_cfg : TextTransformConfig ): \"\"\"Changes the vocabulary of the model. useful when finetuning to another language. Args: text_cfg: Configuration for the text processing pipeline \"\"\" # Updating hparams so that the saved model can be correctly loaded self . hparams . text_cfg = text_cfg self . text_transform = BatchTextTransformer ( text_cfg ) self . decoder = conv1d_decoder ( 1024 , len ( self . text_transform . vocab )) configure_optimizers ( self ) Configuring optimizers. Check the original lightning docs for more info. Returns: Type Description Optimizer Optimizer, and optionally the learning rate scheduler. Source code in thunder/quartznet/module.py def configure_optimizers ( self ) -> torch . optim . Optimizer : \"\"\"Configuring optimizers. Check the original lightning docs for more info. Returns: Optimizer, and optionally the learning rate scheduler. \"\"\" return torch . optim . Adam ( filter ( lambda p : p . requires_grad , self . parameters ()), lr = self . hparams . optim_cfg . learning_rate , betas = self . hparams . optim_cfg . betas , ) forward ( self , x ) Process the audio tensor to create the predictions. Parameters: Name Type Description Default x Tensor Audio tensor of shape [batch_size, time] required Returns: Type Description Tensor Tuple with the predictions and output lengths. Notice that the ouput lengths are not normalized, they are a long tensor. Source code in thunder/quartznet/module.py def forward ( self , x : Tensor ) -> Tensor : \"\"\"Process the audio tensor to create the predictions. Args: x : Audio tensor of shape [batch_size, time] Returns: Tuple with the predictions and output lengths. Notice that the ouput lengths are not normalized, they are a long tensor. \"\"\" features = self . audio_transform ( x ) encoded = self . encoder ( features ) return self . decoder ( encoded ) load_from_nemo ( checkpoint , save_folder = None ) classmethod Load from the original nemo checkpoint. Parameters: Name Type Description Default checkpoint Union[str, thunder.quartznet.compatibility.QuartznetCheckpoint] Path to local .nemo file or checkpoint to be downloaded locally and lodaded. required save_folder str Path to save the checkpoint when downloading it. Ignored if you pass a .nemo file as the first argument. None Returns: Type Description QuartznetModule The model loaded from the checkpoint Source code in thunder/quartznet/module.py @classmethod def load_from_nemo ( cls , checkpoint : Union [ str , QuartznetCheckpoint ], save_folder : str = None ) -> \"QuartznetModule\" : \"\"\"Load from the original nemo checkpoint. Args: checkpoint : Path to local .nemo file or checkpoint to be downloaded locally and lodaded. save_folder : Path to save the checkpoint when downloading it. Ignored if you pass a .nemo file as the first argument. Returns: The model loaded from the checkpoint \"\"\" if isinstance ( checkpoint , QuartznetCheckpoint ): nemo_filepath = download_checkpoint ( checkpoint , save_folder ) else : nemo_filepath = checkpoint with TemporaryDirectory () as extract_path : extract_archive ( str ( nemo_filepath ), extract_path ) extract_path = Path ( extract_path ) config_path = extract_path / \"model_config.yaml\" encoder_params , initial_vocab , preprocess_params = read_params_from_config ( config_path ) module = cls ( TextTransformConfig ( initial_vocab_tokens = initial_vocab , simple_vocab = True , ), EncoderConfig ( ** encoder_params ), FilterbankConfig ( ** preprocess_params ), ) weights_path = extract_path / \"model_weights.ckpt\" load_quartznet_weights ( module . encoder , module . decoder , str ( weights_path )) # Here we set it in eval mode, so it correctly works during inference # Supposing that the majority of applications will be either (1) load a checkpoint # and directly run inference, or (2) fine-tuning. Either way this will prevent a silent # bug (case 1) or will be ignored (case 2). module . eval () return module predict ( self , x ) Use this function during inference to predict. Parameters: Name Type Description Default x Tensor Audio tensor of shape [batch_size, time] required Returns: Type Description List[str] A list of strings, each one contains the corresponding transcription to the original batch element. Source code in thunder/quartznet/module.py @torch . jit . export def predict ( self , x : Tensor ) -> List [ str ]: \"\"\"Use this function during inference to predict. Args: x : Audio tensor of shape [batch_size, time] Returns: A list of strings, each one contains the corresponding transcription to the original batch element. \"\"\" pred = self ( x ) return self . text_transform . decode_prediction ( pred . argmax ( 1 )) training_step ( self , batch , batch_idx ) Training step. Check the original lightning docs for more information. Parameters: Name Type Description Default batch Tuple[torch.Tensor, torch.Tensor, List[str]] Tuple containing the batched audios, normalized lengths and the corresponding text labels. required batch_idx int Batch index required Returns: Type Description Tensor Training loss for that batch Source code in thunder/quartznet/module.py def training_step ( self , batch : Tuple [ torch . Tensor , torch . Tensor , List [ str ]], batch_idx : int ) -> torch . Tensor : \"\"\"Training step. Check the original lightning docs for more information. Args: batch : Tuple containing the batched audios, normalized lengths and the corresponding text labels. batch_idx : Batch index Returns: Training loss for that batch \"\"\" audio , audio_lens , texts = batch y , y_lens = self . text_transform . encode ( texts , device = self . device ) probabilities = self ( audio ) loss = calculate_ctc ( probabilities , y , audio_lens , y_lens , self . text_transform . vocab . blank_idx ) self . log ( \"loss/train_loss\" , loss ) return loss validation_step ( self , batch , batch_idx ) Validation step. Check the original lightning docs for more information. Parameters: Name Type Description Default batch Tuple[torch.Tensor, torch.Tensor, List[str]] Tuple containing the batched audios, normalized lengths and the corresponding text labels. required batch_idx int Batch index required Returns: Type Description Tensor Validation loss for that batch Source code in thunder/quartznet/module.py def validation_step ( self , batch : Tuple [ torch . Tensor , torch . Tensor , List [ str ]], batch_idx : int ) -> torch . Tensor : \"\"\"Validation step. Check the original lightning docs for more information. Args: batch : Tuple containing the batched audios, normalized lengths and the corresponding text labels. batch_idx : Batch index Returns: Validation loss for that batch \"\"\" audio , audio_lens , texts = batch y , y_lens = self . text_transform . encode ( texts , device = self . device ) probabilities = self ( audio ) loss = calculate_ctc ( probabilities , y , audio_lens , y_lens , self . text_transform . vocab . blank_idx ) decoded_preds = self . text_transform . decode_prediction ( probabilities . argmax ( 1 )) decoded_targets = self . text_transform . decode_prediction ( y ) self . val_cer ( decoded_preds , decoded_targets ) self . val_wer ( decoded_preds , decoded_targets ) self . log ( \"loss/val_loss\" , loss ) self . log ( \"metrics/cer\" , self . val_cer , on_epoch = True ) self . log ( \"metrics/wer\" , self . val_wer , on_epoch = True ) return loss","title":"Module"},{"location":"api/Quartznet/module/#thunder.quartznet.module.OptimizerConfig","text":"Configuration used by the optimizer Attributes: Name Type Description learning_rate float learning rate. defaults to 3e-4. betas Tuple[float] beta1 and beta2 used by adam. defaults to (0.8, 0.5), similar to the novograd values on nemo.","title":"OptimizerConfig"},{"location":"api/Quartznet/module/#thunder.quartznet.module.QuartznetModule","text":"","title":"QuartznetModule"},{"location":"api/Quartznet/module/#thunder.quartznet.module.QuartznetModule.__init__","text":"Module containing both the basic quartznet model and helper functionality, such as feature creation and text processing. Parameters: Name Type Description Default text_cfg TextTransformConfig Configuration for the text processing pipeline required encoder_cfg EncoderConfig Configuration for the quartznet encoder EncoderConfig(feat_in=64, filters=[256, 256, 512, 512, 512], kernel_sizes=[33, 39, 51, 63, 75], repeat_blocks=1) audio_cfg FilterbankConfig Configuration for the filterbank features applied to the input audio FilterbankConfig(sample_rate=16000, n_window_size=320, n_window_stride=160, n_fft=512, preemph=0.97, nfilt=64, dither=1e-05) optim_cfg OptimizerConfig Configuration for the optimizer used during training OptimizerConfig(learning_rate=0.0003, betas=(0.8, 0.5)) Source code in thunder/quartznet/module.py def __init__ ( self , text_cfg : TextTransformConfig , encoder_cfg : EncoderConfig = EncoderConfig (), audio_cfg : FilterbankConfig = FilterbankConfig (), optim_cfg : OptimizerConfig = OptimizerConfig (), ): \"\"\"Module containing both the basic quartznet model and helper functionality, such as feature creation and text processing. Args: text_cfg: Configuration for the text processing pipeline encoder_cfg: Configuration for the quartznet encoder audio_cfg: Configuration for the filterbank features applied to the input audio optim_cfg: Configuration for the optimizer used during training \"\"\" super () . __init__ () self . save_hyperparameters () self . audio_transform = FilterbankFeatures ( audio_cfg ) self . encoder = Quartznet_encoder ( encoder_cfg ) self . text_transform = BatchTextTransformer ( text_cfg ) self . decoder = conv1d_decoder ( 1024 , len ( self . text_transform . vocab )) # Metrics self . val_cer = CER () self . val_wer = WER () # Example input is one second of fake audio self . example_input_array = torch . randn (( 10 , audio_cfg . sample_rate ))","title":"__init__()"},{"location":"api/Quartznet/module/#thunder.quartznet.module.QuartznetModule.change_vocab","text":"Changes the vocabulary of the model. useful when finetuning to another language. Parameters: Name Type Description Default text_cfg TextTransformConfig Configuration for the text processing pipeline required Source code in thunder/quartznet/module.py def change_vocab ( self , text_cfg : TextTransformConfig ): \"\"\"Changes the vocabulary of the model. useful when finetuning to another language. Args: text_cfg: Configuration for the text processing pipeline \"\"\" # Updating hparams so that the saved model can be correctly loaded self . hparams . text_cfg = text_cfg self . text_transform = BatchTextTransformer ( text_cfg ) self . decoder = conv1d_decoder ( 1024 , len ( self . text_transform . vocab ))","title":"change_vocab()"},{"location":"api/Quartznet/module/#thunder.quartznet.module.QuartznetModule.configure_optimizers","text":"Configuring optimizers. Check the original lightning docs for more info. Returns: Type Description Optimizer Optimizer, and optionally the learning rate scheduler. Source code in thunder/quartznet/module.py def configure_optimizers ( self ) -> torch . optim . Optimizer : \"\"\"Configuring optimizers. Check the original lightning docs for more info. Returns: Optimizer, and optionally the learning rate scheduler. \"\"\" return torch . optim . Adam ( filter ( lambda p : p . requires_grad , self . parameters ()), lr = self . hparams . optim_cfg . learning_rate , betas = self . hparams . optim_cfg . betas , )","title":"configure_optimizers()"},{"location":"api/Quartznet/module/#thunder.quartznet.module.QuartznetModule.forward","text":"Process the audio tensor to create the predictions. Parameters: Name Type Description Default x Tensor Audio tensor of shape [batch_size, time] required Returns: Type Description Tensor Tuple with the predictions and output lengths. Notice that the ouput lengths are not normalized, they are a long tensor. Source code in thunder/quartznet/module.py def forward ( self , x : Tensor ) -> Tensor : \"\"\"Process the audio tensor to create the predictions. Args: x : Audio tensor of shape [batch_size, time] Returns: Tuple with the predictions and output lengths. Notice that the ouput lengths are not normalized, they are a long tensor. \"\"\" features = self . audio_transform ( x ) encoded = self . encoder ( features ) return self . decoder ( encoded )","title":"forward()"},{"location":"api/Quartznet/module/#thunder.quartznet.module.QuartznetModule.load_from_nemo","text":"Load from the original nemo checkpoint. Parameters: Name Type Description Default checkpoint Union[str, thunder.quartznet.compatibility.QuartznetCheckpoint] Path to local .nemo file or checkpoint to be downloaded locally and lodaded. required save_folder str Path to save the checkpoint when downloading it. Ignored if you pass a .nemo file as the first argument. None Returns: Type Description QuartznetModule The model loaded from the checkpoint Source code in thunder/quartznet/module.py @classmethod def load_from_nemo ( cls , checkpoint : Union [ str , QuartznetCheckpoint ], save_folder : str = None ) -> \"QuartznetModule\" : \"\"\"Load from the original nemo checkpoint. Args: checkpoint : Path to local .nemo file or checkpoint to be downloaded locally and lodaded. save_folder : Path to save the checkpoint when downloading it. Ignored if you pass a .nemo file as the first argument. Returns: The model loaded from the checkpoint \"\"\" if isinstance ( checkpoint , QuartznetCheckpoint ): nemo_filepath = download_checkpoint ( checkpoint , save_folder ) else : nemo_filepath = checkpoint with TemporaryDirectory () as extract_path : extract_archive ( str ( nemo_filepath ), extract_path ) extract_path = Path ( extract_path ) config_path = extract_path / \"model_config.yaml\" encoder_params , initial_vocab , preprocess_params = read_params_from_config ( config_path ) module = cls ( TextTransformConfig ( initial_vocab_tokens = initial_vocab , simple_vocab = True , ), EncoderConfig ( ** encoder_params ), FilterbankConfig ( ** preprocess_params ), ) weights_path = extract_path / \"model_weights.ckpt\" load_quartznet_weights ( module . encoder , module . decoder , str ( weights_path )) # Here we set it in eval mode, so it correctly works during inference # Supposing that the majority of applications will be either (1) load a checkpoint # and directly run inference, or (2) fine-tuning. Either way this will prevent a silent # bug (case 1) or will be ignored (case 2). module . eval () return module","title":"load_from_nemo()"},{"location":"api/Quartznet/module/#thunder.quartznet.module.QuartznetModule.predict","text":"Use this function during inference to predict. Parameters: Name Type Description Default x Tensor Audio tensor of shape [batch_size, time] required Returns: Type Description List[str] A list of strings, each one contains the corresponding transcription to the original batch element. Source code in thunder/quartznet/module.py @torch . jit . export def predict ( self , x : Tensor ) -> List [ str ]: \"\"\"Use this function during inference to predict. Args: x : Audio tensor of shape [batch_size, time] Returns: A list of strings, each one contains the corresponding transcription to the original batch element. \"\"\" pred = self ( x ) return self . text_transform . decode_prediction ( pred . argmax ( 1 ))","title":"predict()"},{"location":"api/Quartznet/module/#thunder.quartznet.module.QuartznetModule.training_step","text":"Training step. Check the original lightning docs for more information. Parameters: Name Type Description Default batch Tuple[torch.Tensor, torch.Tensor, List[str]] Tuple containing the batched audios, normalized lengths and the corresponding text labels. required batch_idx int Batch index required Returns: Type Description Tensor Training loss for that batch Source code in thunder/quartznet/module.py def training_step ( self , batch : Tuple [ torch . Tensor , torch . Tensor , List [ str ]], batch_idx : int ) -> torch . Tensor : \"\"\"Training step. Check the original lightning docs for more information. Args: batch : Tuple containing the batched audios, normalized lengths and the corresponding text labels. batch_idx : Batch index Returns: Training loss for that batch \"\"\" audio , audio_lens , texts = batch y , y_lens = self . text_transform . encode ( texts , device = self . device ) probabilities = self ( audio ) loss = calculate_ctc ( probabilities , y , audio_lens , y_lens , self . text_transform . vocab . blank_idx ) self . log ( \"loss/train_loss\" , loss ) return loss","title":"training_step()"},{"location":"api/Quartznet/module/#thunder.quartznet.module.QuartznetModule.validation_step","text":"Validation step. Check the original lightning docs for more information. Parameters: Name Type Description Default batch Tuple[torch.Tensor, torch.Tensor, List[str]] Tuple containing the batched audios, normalized lengths and the corresponding text labels. required batch_idx int Batch index required Returns: Type Description Tensor Validation loss for that batch Source code in thunder/quartznet/module.py def validation_step ( self , batch : Tuple [ torch . Tensor , torch . Tensor , List [ str ]], batch_idx : int ) -> torch . Tensor : \"\"\"Validation step. Check the original lightning docs for more information. Args: batch : Tuple containing the batched audios, normalized lengths and the corresponding text labels. batch_idx : Batch index Returns: Validation loss for that batch \"\"\" audio , audio_lens , texts = batch y , y_lens = self . text_transform . encode ( texts , device = self . device ) probabilities = self ( audio ) loss = calculate_ctc ( probabilities , y , audio_lens , y_lens , self . text_transform . vocab . blank_idx ) decoded_preds = self . text_transform . decode_prediction ( probabilities . argmax ( 1 )) decoded_targets = self . text_transform . decode_prediction ( y ) self . val_cer ( decoded_preds , decoded_targets ) self . val_wer ( decoded_preds , decoded_targets ) self . log ( \"loss/val_loss\" , loss ) self . log ( \"metrics/cer\" , self . val_cer , on_epoch = True ) self . log ( \"metrics/wer\" , self . val_wer , on_epoch = True ) return loss","title":"validation_step()"},{"location":"api/Quartznet/transform/","text":"Functionality to transform the audio input in the same way that the Quartznet model expects it. DitherAudio __init__ ( self , dither = 1e-05 ) special Add some dithering to the audio tensor. Note From wikipedia: Dither is an intentionally applied form of noise used to randomize quantization error. Parameters: Name Type Description Default dither float Amount of dither to add. 1e-05 Source code in thunder/quartznet/transform.py def __init__ ( self , dither : float = 1e-5 ): \"\"\"Add some dithering to the audio tensor. Note: From wikipedia: Dither is an intentionally applied form of noise used to randomize quantization error. Args: dither : Amount of dither to add. \"\"\" super () . __init__ () self . dither = dither forward ( self , x ) Parameters: Name Type Description Default x Tensor Tensor of shape (batch, time) required Source code in thunder/quartznet/transform.py @torch . no_grad () def forward ( self , x : torch . Tensor ) -> torch . Tensor : \"\"\" Args: x : Tensor of shape (batch, time) \"\"\" if self . training : mask = x > 0.0 return x + mask * ( self . dither * torch . randn_like ( x )) else : return x FeatureBatchNormalizer __init__ ( self ) special Normalize batch at the feature dimension. Source code in thunder/quartznet/transform.py def __init__ ( self ): \"\"\"Normalize batch at the feature dimension.\"\"\" super () . __init__ () self . div_guard = 1e-5 forward ( self , x ) Parameters: Name Type Description Default x Tensor Tensor of shape (batch, features, time) required Source code in thunder/quartznet/transform.py @torch . no_grad () def forward ( self , x : torch . Tensor ) -> torch . Tensor : \"\"\" Args: x : Tensor of shape (batch, features, time) \"\"\" mask = x . abs () > 0.0 num_elements = mask . sum ( dim = 2 , keepdim = True ) . detach () x_mean = x . sum ( dim = 2 , keepdim = True ) . detach () / num_elements numerator = ( x - x_mean ) . pow ( 2 ) . sum ( dim = 2 , keepdim = True ) . detach () x_std = ( numerator / num_elements ) . sqrt () # make sure x_std is not zero x_std += self . div_guard result = ( x - x_mean ) / x_std return torch . masked_fill ( result , ~ mask , 0.0 ) FilterbankConfig dataclass Configuration to create FilterbankFeatures Attributes: Name Type Description sample_rate int Sampling rate of the signal. defaults to 16000. n_window_size int Number of elements in the window size. defaults to 320. n_window_stride int Number of elements in the window stride. defaults to 160. n_fft int Number of fourier features. defaults to 512. preemph float Preemphasis filtering control factor. defaults to 0.97. nfilt int Number of output mel filters to use. defaults to 64. dither float Amount of dither to add. defaults to 1e-5. MelScale __init__ ( self , sample_rate , n_fft , nfilt , log_scale = True ) special Convert a spectrogram to Mel scale, following the default formula of librosa instead of the one used by torchaudio. Also converts to log scale. Parameters: Name Type Description Default sample_rate int Sampling rate of the signal required n_fft int Number of fourier features required nfilt int Number of output mel filters to use required log_scale bool Controls if the output should also be applied a log scale. True Source code in thunder/quartznet/transform.py def __init__ ( self , sample_rate : int , n_fft : int , nfilt : int , log_scale : bool = True ): \"\"\"Convert a spectrogram to Mel scale, following the default formula of librosa instead of the one used by torchaudio. Also converts to log scale. Args: sample_rate : Sampling rate of the signal n_fft : Number of fourier features nfilt : Number of output mel filters to use log_scale : Controls if the output should also be applied a log scale. \"\"\" super () . __init__ () filterbanks = ( create_fb_matrix ( int ( 1 + n_fft // 2 ), n_mels = nfilt , sample_rate = sample_rate , f_min = 0 , f_max = sample_rate / 2 , norm = \"slaney\" , mel_scale = \"slaney\" , ) . transpose ( 0 , 1 ) . unsqueeze ( 0 ) ) self . register_buffer ( \"fb\" , filterbanks ) self . log_scale = log_scale forward ( self , x ) Parameters: Name Type Description Default x Tensor Tensor of shape (batch, features, time) required Source code in thunder/quartznet/transform.py @torch . no_grad () def forward ( self , x : torch . Tensor ) -> torch . Tensor : \"\"\" Args: x : Tensor of shape (batch, features, time) \"\"\" # dot with filterbank energies x = torch . matmul ( self . fb . to ( x . dtype ), x ) # log features # We want to avoid taking the log of zero if self . log_scale : mask = x . abs () > 0.0 x = torch . log ( x + 2 ** - 24 ) x [ ~ mask ] = 0.0 return x PowerSpectrum __init__ ( self , n_window_size = 320 , n_window_stride = 160 , n_fft = None ) special Calculates the power spectrum of the audio signal, following the same method as used in NEMO. Parameters: Name Type Description Default n_window_size int Number of elements in the window size. 320 n_window_stride int Number of elements in the window stride. 160 n_fft Optional[int] Number of fourier features. None Exceptions: Type Description ValueError Raised when incompatible parameters are passed. Source code in thunder/quartznet/transform.py def __init__ ( self , n_window_size : int = 320 , n_window_stride : int = 160 , n_fft : Optional [ int ] = None , ): \"\"\"Calculates the power spectrum of the audio signal, following the same method as used in NEMO. Args: n_window_size : Number of elements in the window size. n_window_stride : Number of elements in the window stride. n_fft : Number of fourier features. Raises: ValueError: Raised when incompatible parameters are passed. \"\"\" super () . __init__ () if n_window_size <= 0 or n_window_stride <= 0 : raise ValueError ( f \" { self } got an invalid value for either n_window_size or \" f \"n_window_stride. Both must be positive ints.\" ) self . win_length = n_window_size self . hop_length = n_window_stride self . n_fft = n_fft or 2 ** math . ceil ( math . log2 ( self . win_length )) window_tensor = torch . hann_window ( self . win_length , periodic = False ) self . register_buffer ( \"window\" , window_tensor ) forward ( self , x ) Parameters: Name Type Description Default x Tensor Tensor of shape (batch, time) required Source code in thunder/quartznet/transform.py @torch . no_grad () def forward ( self , x : torch . Tensor ) -> torch . Tensor : \"\"\" Args: x : Tensor of shape (batch, time) \"\"\" x = torch . stft ( x , n_fft = self . n_fft , hop_length = self . hop_length , win_length = self . win_length , center = True , window = self . window . to ( dtype = torch . float ), return_complex = False , ) # torch returns real, imag; so convert to magnitude x = torch . sqrt ( x . pow ( 2 ) . sum ( - 1 )) # get power spectrum x = x . pow ( 2.0 ) return x PreEmphasisFilter __init__ ( self , preemph = 0.97 ) special Applies preemphasis filtering to the audio signal. This is a classic signal processing function to emphasise the high frequency portion of the content compared to the low frequency. It applies a FIR filter of the form: y[n] = y[n] - preemph * y[n-1] Parameters: Name Type Description Default preemph float Filter control factor. 0.97 Source code in thunder/quartznet/transform.py def __init__ ( self , preemph : float = 0.97 ): \"\"\"Applies preemphasis filtering to the audio signal. This is a classic signal processing function to emphasise the high frequency portion of the content compared to the low frequency. It applies a FIR filter of the form: `y[n] = y[n] - preemph * y[n-1]` Args: preemph : Filter control factor. \"\"\" super () . __init__ () self . preemph = preemph forward ( self , x ) Parameters: Name Type Description Default x Tensor Tensor of shape (batch, time) required Source code in thunder/quartznet/transform.py @torch . no_grad () def forward ( self , x : torch . Tensor ) -> torch . Tensor : \"\"\" Args: x : Tensor of shape (batch, time) \"\"\" return torch . cat ( ( x [:, 0 ] . unsqueeze ( 1 ), x [:, 1 :] - self . preemph * x [:, : - 1 ]), dim = 1 ) FilterbankFeatures ( cfg ) Creates the Filterbank features used in the Quartznet model. Parameters: Name Type Description Default cfg FilterbankConfig required config to create instance required Returns: Type Description Module Module that computes the features based on raw audio tensor. Source code in thunder/quartznet/transform.py def FilterbankFeatures ( cfg : FilterbankConfig ) -> nn . Module : \"\"\"Creates the Filterbank features used in the Quartznet model. Args: cfg: required config to create instance Returns: Module that computes the features based on raw audio tensor. \"\"\" return nn . Sequential ( DitherAudio ( dither = cfg . dither ), PreEmphasisFilter ( preemph = cfg . preemph ), PowerSpectrum ( n_window_size = cfg . n_window_size , n_window_stride = cfg . n_window_stride , n_fft = cfg . n_fft , ), MelScale ( sample_rate = cfg . sample_rate , n_fft = cfg . n_fft , nfilt = cfg . nfilt ), FeatureBatchNormalizer (), )","title":"Transform"},{"location":"api/Quartznet/transform/#thunder.quartznet.transform.DitherAudio","text":"","title":"DitherAudio"},{"location":"api/Quartznet/transform/#thunder.quartznet.transform.DitherAudio.__init__","text":"Add some dithering to the audio tensor. Note From wikipedia: Dither is an intentionally applied form of noise used to randomize quantization error. Parameters: Name Type Description Default dither float Amount of dither to add. 1e-05 Source code in thunder/quartznet/transform.py def __init__ ( self , dither : float = 1e-5 ): \"\"\"Add some dithering to the audio tensor. Note: From wikipedia: Dither is an intentionally applied form of noise used to randomize quantization error. Args: dither : Amount of dither to add. \"\"\" super () . __init__ () self . dither = dither","title":"__init__()"},{"location":"api/Quartznet/transform/#thunder.quartznet.transform.DitherAudio.forward","text":"Parameters: Name Type Description Default x Tensor Tensor of shape (batch, time) required Source code in thunder/quartznet/transform.py @torch . no_grad () def forward ( self , x : torch . Tensor ) -> torch . Tensor : \"\"\" Args: x : Tensor of shape (batch, time) \"\"\" if self . training : mask = x > 0.0 return x + mask * ( self . dither * torch . randn_like ( x )) else : return x","title":"forward()"},{"location":"api/Quartznet/transform/#thunder.quartznet.transform.FeatureBatchNormalizer","text":"","title":"FeatureBatchNormalizer"},{"location":"api/Quartznet/transform/#thunder.quartznet.transform.FeatureBatchNormalizer.__init__","text":"Normalize batch at the feature dimension. Source code in thunder/quartznet/transform.py def __init__ ( self ): \"\"\"Normalize batch at the feature dimension.\"\"\" super () . __init__ () self . div_guard = 1e-5","title":"__init__()"},{"location":"api/Quartznet/transform/#thunder.quartznet.transform.FeatureBatchNormalizer.forward","text":"Parameters: Name Type Description Default x Tensor Tensor of shape (batch, features, time) required Source code in thunder/quartznet/transform.py @torch . no_grad () def forward ( self , x : torch . Tensor ) -> torch . Tensor : \"\"\" Args: x : Tensor of shape (batch, features, time) \"\"\" mask = x . abs () > 0.0 num_elements = mask . sum ( dim = 2 , keepdim = True ) . detach () x_mean = x . sum ( dim = 2 , keepdim = True ) . detach () / num_elements numerator = ( x - x_mean ) . pow ( 2 ) . sum ( dim = 2 , keepdim = True ) . detach () x_std = ( numerator / num_elements ) . sqrt () # make sure x_std is not zero x_std += self . div_guard result = ( x - x_mean ) / x_std return torch . masked_fill ( result , ~ mask , 0.0 )","title":"forward()"},{"location":"api/Quartznet/transform/#thunder.quartznet.transform.FilterbankConfig","text":"Configuration to create FilterbankFeatures Attributes: Name Type Description sample_rate int Sampling rate of the signal. defaults to 16000. n_window_size int Number of elements in the window size. defaults to 320. n_window_stride int Number of elements in the window stride. defaults to 160. n_fft int Number of fourier features. defaults to 512. preemph float Preemphasis filtering control factor. defaults to 0.97. nfilt int Number of output mel filters to use. defaults to 64. dither float Amount of dither to add. defaults to 1e-5.","title":"FilterbankConfig"},{"location":"api/Quartznet/transform/#thunder.quartznet.transform.MelScale","text":"","title":"MelScale"},{"location":"api/Quartznet/transform/#thunder.quartznet.transform.MelScale.__init__","text":"Convert a spectrogram to Mel scale, following the default formula of librosa instead of the one used by torchaudio. Also converts to log scale. Parameters: Name Type Description Default sample_rate int Sampling rate of the signal required n_fft int Number of fourier features required nfilt int Number of output mel filters to use required log_scale bool Controls if the output should also be applied a log scale. True Source code in thunder/quartznet/transform.py def __init__ ( self , sample_rate : int , n_fft : int , nfilt : int , log_scale : bool = True ): \"\"\"Convert a spectrogram to Mel scale, following the default formula of librosa instead of the one used by torchaudio. Also converts to log scale. Args: sample_rate : Sampling rate of the signal n_fft : Number of fourier features nfilt : Number of output mel filters to use log_scale : Controls if the output should also be applied a log scale. \"\"\" super () . __init__ () filterbanks = ( create_fb_matrix ( int ( 1 + n_fft // 2 ), n_mels = nfilt , sample_rate = sample_rate , f_min = 0 , f_max = sample_rate / 2 , norm = \"slaney\" , mel_scale = \"slaney\" , ) . transpose ( 0 , 1 ) . unsqueeze ( 0 ) ) self . register_buffer ( \"fb\" , filterbanks ) self . log_scale = log_scale","title":"__init__()"},{"location":"api/Quartznet/transform/#thunder.quartznet.transform.MelScale.forward","text":"Parameters: Name Type Description Default x Tensor Tensor of shape (batch, features, time) required Source code in thunder/quartznet/transform.py @torch . no_grad () def forward ( self , x : torch . Tensor ) -> torch . Tensor : \"\"\" Args: x : Tensor of shape (batch, features, time) \"\"\" # dot with filterbank energies x = torch . matmul ( self . fb . to ( x . dtype ), x ) # log features # We want to avoid taking the log of zero if self . log_scale : mask = x . abs () > 0.0 x = torch . log ( x + 2 ** - 24 ) x [ ~ mask ] = 0.0 return x","title":"forward()"},{"location":"api/Quartznet/transform/#thunder.quartznet.transform.PowerSpectrum","text":"","title":"PowerSpectrum"},{"location":"api/Quartznet/transform/#thunder.quartznet.transform.PowerSpectrum.__init__","text":"Calculates the power spectrum of the audio signal, following the same method as used in NEMO. Parameters: Name Type Description Default n_window_size int Number of elements in the window size. 320 n_window_stride int Number of elements in the window stride. 160 n_fft Optional[int] Number of fourier features. None Exceptions: Type Description ValueError Raised when incompatible parameters are passed. Source code in thunder/quartznet/transform.py def __init__ ( self , n_window_size : int = 320 , n_window_stride : int = 160 , n_fft : Optional [ int ] = None , ): \"\"\"Calculates the power spectrum of the audio signal, following the same method as used in NEMO. Args: n_window_size : Number of elements in the window size. n_window_stride : Number of elements in the window stride. n_fft : Number of fourier features. Raises: ValueError: Raised when incompatible parameters are passed. \"\"\" super () . __init__ () if n_window_size <= 0 or n_window_stride <= 0 : raise ValueError ( f \" { self } got an invalid value for either n_window_size or \" f \"n_window_stride. Both must be positive ints.\" ) self . win_length = n_window_size self . hop_length = n_window_stride self . n_fft = n_fft or 2 ** math . ceil ( math . log2 ( self . win_length )) window_tensor = torch . hann_window ( self . win_length , periodic = False ) self . register_buffer ( \"window\" , window_tensor )","title":"__init__()"},{"location":"api/Quartznet/transform/#thunder.quartznet.transform.PowerSpectrum.forward","text":"Parameters: Name Type Description Default x Tensor Tensor of shape (batch, time) required Source code in thunder/quartznet/transform.py @torch . no_grad () def forward ( self , x : torch . Tensor ) -> torch . Tensor : \"\"\" Args: x : Tensor of shape (batch, time) \"\"\" x = torch . stft ( x , n_fft = self . n_fft , hop_length = self . hop_length , win_length = self . win_length , center = True , window = self . window . to ( dtype = torch . float ), return_complex = False , ) # torch returns real, imag; so convert to magnitude x = torch . sqrt ( x . pow ( 2 ) . sum ( - 1 )) # get power spectrum x = x . pow ( 2.0 ) return x","title":"forward()"},{"location":"api/Quartznet/transform/#thunder.quartznet.transform.PreEmphasisFilter","text":"","title":"PreEmphasisFilter"},{"location":"api/Quartznet/transform/#thunder.quartznet.transform.PreEmphasisFilter.__init__","text":"Applies preemphasis filtering to the audio signal. This is a classic signal processing function to emphasise the high frequency portion of the content compared to the low frequency. It applies a FIR filter of the form: y[n] = y[n] - preemph * y[n-1] Parameters: Name Type Description Default preemph float Filter control factor. 0.97 Source code in thunder/quartznet/transform.py def __init__ ( self , preemph : float = 0.97 ): \"\"\"Applies preemphasis filtering to the audio signal. This is a classic signal processing function to emphasise the high frequency portion of the content compared to the low frequency. It applies a FIR filter of the form: `y[n] = y[n] - preemph * y[n-1]` Args: preemph : Filter control factor. \"\"\" super () . __init__ () self . preemph = preemph","title":"__init__()"},{"location":"api/Quartznet/transform/#thunder.quartznet.transform.PreEmphasisFilter.forward","text":"Parameters: Name Type Description Default x Tensor Tensor of shape (batch, time) required Source code in thunder/quartznet/transform.py @torch . no_grad () def forward ( self , x : torch . Tensor ) -> torch . Tensor : \"\"\" Args: x : Tensor of shape (batch, time) \"\"\" return torch . cat ( ( x [:, 0 ] . unsqueeze ( 1 ), x [:, 1 :] - self . preemph * x [:, : - 1 ]), dim = 1 )","title":"forward()"},{"location":"api/Quartznet/transform/#thunder.quartznet.transform.FilterbankFeatures","text":"Creates the Filterbank features used in the Quartznet model. Parameters: Name Type Description Default cfg FilterbankConfig required config to create instance required Returns: Type Description Module Module that computes the features based on raw audio tensor. Source code in thunder/quartznet/transform.py def FilterbankFeatures ( cfg : FilterbankConfig ) -> nn . Module : \"\"\"Creates the Filterbank features used in the Quartznet model. Args: cfg: required config to create instance Returns: Module that computes the features based on raw audio tensor. \"\"\" return nn . Sequential ( DitherAudio ( dither = cfg . dither ), PreEmphasisFilter ( preemph = cfg . preemph ), PowerSpectrum ( n_window_size = cfg . n_window_size , n_window_stride = cfg . n_window_stride , n_fft = cfg . n_fft , ), MelScale ( sample_rate = cfg . sample_rate , n_fft = cfg . n_fft , nfilt = cfg . nfilt ), FeatureBatchNormalizer (), )","title":"FilterbankFeatures()"},{"location":"api/Text%20Processing/preprocess/","text":"expand_numbers ( text , language = 'en' ) Expand the numbers present inside the text. That means converting \"42\" into \"forty two\". It also detects if the number is ordinal automatically. Parameters: Name Type Description Default text str Input text required language str Language used to expand the numbers. Defaults to \"en\". 'en' Returns: Type Description str Output text Source code in thunder/text_processing/preprocess.py def expand_numbers ( text : str , language : str = \"en\" ) -> str : \"\"\"Expand the numbers present inside the text. That means converting \"42\" into \"forty two\". It also detects if the number is ordinal automatically. Args: text : Input text language : Language used to expand the numbers. Defaults to \"en\". Returns: Output text \"\"\" number_regex = re . compile ( r \"\\d+\u00ba*\" ) all_numbers = number_regex . findall ( text ) for num in all_numbers : if \"\u00ba\" in num : pure_number = num . replace ( \"\u00ba\" , \"\" ) . strip () expanded = num2words ( int ( pure_number ), lang = language , to = \"ordinal\" ) else : expanded = num2words ( int ( num ), lang = language ) text = text . replace ( num , expanded ) return text lower_text ( text ) Transform all the text to lowercase. Parameters: Name Type Description Default text str Input text required Returns: Type Description str Output text Source code in thunder/text_processing/preprocess.py def lower_text ( text : str ) -> str : \"\"\"Transform all the text to lowercase. Args: text : Input text Returns: Output text \"\"\" return text . lower () normalize_text ( text ) Normalize the text to remove accents and ensure all the characters are valid ascii symbols. Parameters: Name Type Description Default text str Input text required Returns: Type Description str Output text Source code in thunder/text_processing/preprocess.py def normalize_text ( text : str ) -> str : \"\"\"Normalize the text to remove accents and ensure all the characters are valid ascii symbols. Args: text : Input text Returns: Output text \"\"\" nfkd_form = unicodedata . normalize ( \"NFKD\" , text ) only_ascii = nfkd_form . encode ( \"ASCII\" , \"ignore\" ) return only_ascii . decode ()","title":"Preprocess"},{"location":"api/Text%20Processing/preprocess/#thunder.text_processing.preprocess.expand_numbers","text":"Expand the numbers present inside the text. That means converting \"42\" into \"forty two\". It also detects if the number is ordinal automatically. Parameters: Name Type Description Default text str Input text required language str Language used to expand the numbers. Defaults to \"en\". 'en' Returns: Type Description str Output text Source code in thunder/text_processing/preprocess.py def expand_numbers ( text : str , language : str = \"en\" ) -> str : \"\"\"Expand the numbers present inside the text. That means converting \"42\" into \"forty two\". It also detects if the number is ordinal automatically. Args: text : Input text language : Language used to expand the numbers. Defaults to \"en\". Returns: Output text \"\"\" number_regex = re . compile ( r \"\\d+\u00ba*\" ) all_numbers = number_regex . findall ( text ) for num in all_numbers : if \"\u00ba\" in num : pure_number = num . replace ( \"\u00ba\" , \"\" ) . strip () expanded = num2words ( int ( pure_number ), lang = language , to = \"ordinal\" ) else : expanded = num2words ( int ( num ), lang = language ) text = text . replace ( num , expanded ) return text","title":"expand_numbers()"},{"location":"api/Text%20Processing/preprocess/#thunder.text_processing.preprocess.lower_text","text":"Transform all the text to lowercase. Parameters: Name Type Description Default text str Input text required Returns: Type Description str Output text Source code in thunder/text_processing/preprocess.py def lower_text ( text : str ) -> str : \"\"\"Transform all the text to lowercase. Args: text : Input text Returns: Output text \"\"\" return text . lower ()","title":"lower_text()"},{"location":"api/Text%20Processing/preprocess/#thunder.text_processing.preprocess.normalize_text","text":"Normalize the text to remove accents and ensure all the characters are valid ascii symbols. Parameters: Name Type Description Default text str Input text required Returns: Type Description str Output text Source code in thunder/text_processing/preprocess.py def normalize_text ( text : str ) -> str : \"\"\"Normalize the text to remove accents and ensure all the characters are valid ascii symbols. Args: text : Input text Returns: Output text \"\"\" nfkd_form = unicodedata . normalize ( \"NFKD\" , text ) only_ascii = nfkd_form . encode ( \"ASCII\" , \"ignore\" ) return only_ascii . decode ()","title":"normalize_text()"},{"location":"api/Text%20Processing/tokenize/","text":"char_tokenizer ( text ) Tokenize input text splitting into characters Parameters: Name Type Description Default text str Input text required Returns: Type Description List[str] Tokenized text Source code in thunder/text_processing/tokenizer.py def char_tokenizer ( text : str ) -> List [ str ]: \"\"\"Tokenize input text splitting into characters Args: text : Input text Returns: Tokenized text \"\"\" return list ( text ) get_most_frequent_tokens ( corpus , tokenize_function , minimum_frequency = 1 , max_number_of_tokens = None ) Helper function to get the most frequent tokens from a text corpus. Parameters: Name Type Description Default corpus str Text corpus to be used, this is a long string containing all of your text required tokenize_function Callable Same tokenizer function that will be used during training required minimum_frequency int Remove any token with frequency less than that. Defaults to 1. 1 max_number_of_tokens Optional[int] Optionally limit to the K most frequent tokens. Defaults to None. None Returns: Type Description List[str] All of the unique, most frequent tokens, ordered by frequency. Source code in thunder/text_processing/tokenizer.py def get_most_frequent_tokens ( corpus : str , tokenize_function : Callable , minimum_frequency : int = 1 , max_number_of_tokens : Optional [ int ] = None , ) -> List [ str ]: \"\"\"Helper function to get the most frequent tokens from a text corpus. Args: corpus : Text corpus to be used, this is a long string containing all of your text tokenize_function : Same tokenizer function that will be used during training minimum_frequency : Remove any token with frequency less than that. Defaults to 1. max_number_of_tokens : Optionally limit to the K most frequent tokens. Defaults to None. Returns: All of the unique, most frequent tokens, ordered by frequency. \"\"\" tokenized = tokenize_function ( corpus ) token_counter = Counter ( tokenized ) output_tokens = [] for token , count in token_counter . most_common ( max_number_of_tokens ): if count >= minimum_frequency : output_tokens . append ( token ) return output_tokens train_sentencepiece_model ( data_file , vocab_size , output_dir , sample_size =- 1 , do_lower_case = True , tokenizer_type = 'unigram' , character_coverage = 1.0 , train_extremely_large_corpus = False , max_sentencepiece_length =- 1 ) Creates sentence piece tokenizer model from data file. This is a direct port of create_spt_model present on the NEMO toolkit (nemo/collections/common/tokenizers/sentencepiece_tokenizer.py) Parameters: Name Type Description Default data_file str text file containing the sentences that will be used to train the model required vocab_size int maximum vocabulary size required output_dir str folder to save created tokenizer model and vocab required sample_size int maximum number of sentences the trainer loads. -1 means to use all the data. -1 do_lower_case bool if text should be lower cased before tokenizer model is created True tokenizer_type str controls the sentencepiece model type. 'unigram' character_coverage float float value between 0 and 1 (as a percentage). For languages with a vast charset, can be < 1.0, but for all other languages, it should be set as 1.0 1.0 train_extremely_large_corpus bool If training on huge datasets, pass this flag to allow SentencePiece to build the tokenizer. False max_sentencepiece_length int Limits the maximum length of the SentencePiece subword that can be constructed. By default, no limit is placed. -1 Source code in thunder/text_processing/tokenizer.py def train_sentencepiece_model ( data_file : str , vocab_size : int , output_dir : str , sample_size : int = - 1 , do_lower_case : bool = True , tokenizer_type : str = \"unigram\" , character_coverage : float = 1.0 , train_extremely_large_corpus : bool = False , max_sentencepiece_length : int = - 1 , ) -> str : \"\"\" Creates sentence piece tokenizer model from data file. This is a direct port of `create_spt_model` present on the NEMO toolkit (nemo/collections/common/tokenizers/sentencepiece_tokenizer.py) Args: data_file: text file containing the sentences that will be used to train the model vocab_size: maximum vocabulary size output_dir: folder to save created tokenizer model and vocab sample_size: maximum number of sentences the trainer loads. -1 means to use all the data. do_lower_case: if text should be lower cased before tokenizer model is created tokenizer_type: controls the sentencepiece model type. character_coverage: float value between 0 and 1 (as a percentage). For languages with a vast charset, can be < 1.0, but for all other languages, it should be set as 1.0 train_extremely_large_corpus: If training on huge datasets, pass this flag to allow SentencePiece to build the tokenizer. max_sentencepiece_length: Limits the maximum length of the SentencePiece subword that can be constructed. By default, no limit is placed. \"\"\" data_file = Path ( data_file ) if not data_file or not data_file . exists (): raise ValueError ( f \"data_file must be valid file path, but got { data_file } \" ) output_dir = Path ( output_dir ) if ( output_dir / \"tokenizer.model\" ) . exists (): warn ( \"There's already a trained sentencepiece model at the output directory. Skipping train.\" ) return str ( output_dir ) output_dir . mkdir ( exist_ok = True ) cmd = ( f \"--input= { data_file } --model_prefix= { output_dir } /tokenizer \" f \"--vocab_size= { vocab_size } \" f \"--shuffle_input_sentence=true --hard_vocab_limit=false \" f \"--model_type= { tokenizer_type } \" f \"--character_coverage= { character_coverage } \" ) if do_lower_case : cmd += \" --normalization_rule_name=nmt_nfkc_cf\" if sample_size > 0 : cmd += f \" --input_sentence_size= { sample_size } \" if train_extremely_large_corpus : cmd += \" --train_extremely_large_corpus=true\" if max_sentencepiece_length >= 0 : cmd += f \" --max_sentencepiece_length= { max_sentencepiece_length } \" sentencepiece . SentencePieceTrainer . Train ( cmd ) return str ( output_dir ) word_tokenizer ( text ) Tokenize input text splitting into words Parameters: Name Type Description Default text str Input text required Returns: Type Description List[str] Tokenized text Source code in thunder/text_processing/tokenizer.py def word_tokenizer ( text : str ) -> List [ str ]: \"\"\"Tokenize input text splitting into words Args: text : Input text Returns: Tokenized text \"\"\" return text . split ()","title":"Tokenize"},{"location":"api/Text%20Processing/tokenize/#thunder.text_processing.tokenizer.char_tokenizer","text":"Tokenize input text splitting into characters Parameters: Name Type Description Default text str Input text required Returns: Type Description List[str] Tokenized text Source code in thunder/text_processing/tokenizer.py def char_tokenizer ( text : str ) -> List [ str ]: \"\"\"Tokenize input text splitting into characters Args: text : Input text Returns: Tokenized text \"\"\" return list ( text )","title":"char_tokenizer()"},{"location":"api/Text%20Processing/tokenize/#thunder.text_processing.tokenizer.get_most_frequent_tokens","text":"Helper function to get the most frequent tokens from a text corpus. Parameters: Name Type Description Default corpus str Text corpus to be used, this is a long string containing all of your text required tokenize_function Callable Same tokenizer function that will be used during training required minimum_frequency int Remove any token with frequency less than that. Defaults to 1. 1 max_number_of_tokens Optional[int] Optionally limit to the K most frequent tokens. Defaults to None. None Returns: Type Description List[str] All of the unique, most frequent tokens, ordered by frequency. Source code in thunder/text_processing/tokenizer.py def get_most_frequent_tokens ( corpus : str , tokenize_function : Callable , minimum_frequency : int = 1 , max_number_of_tokens : Optional [ int ] = None , ) -> List [ str ]: \"\"\"Helper function to get the most frequent tokens from a text corpus. Args: corpus : Text corpus to be used, this is a long string containing all of your text tokenize_function : Same tokenizer function that will be used during training minimum_frequency : Remove any token with frequency less than that. Defaults to 1. max_number_of_tokens : Optionally limit to the K most frequent tokens. Defaults to None. Returns: All of the unique, most frequent tokens, ordered by frequency. \"\"\" tokenized = tokenize_function ( corpus ) token_counter = Counter ( tokenized ) output_tokens = [] for token , count in token_counter . most_common ( max_number_of_tokens ): if count >= minimum_frequency : output_tokens . append ( token ) return output_tokens","title":"get_most_frequent_tokens()"},{"location":"api/Text%20Processing/tokenize/#thunder.text_processing.tokenizer.train_sentencepiece_model","text":"Creates sentence piece tokenizer model from data file. This is a direct port of create_spt_model present on the NEMO toolkit (nemo/collections/common/tokenizers/sentencepiece_tokenizer.py) Parameters: Name Type Description Default data_file str text file containing the sentences that will be used to train the model required vocab_size int maximum vocabulary size required output_dir str folder to save created tokenizer model and vocab required sample_size int maximum number of sentences the trainer loads. -1 means to use all the data. -1 do_lower_case bool if text should be lower cased before tokenizer model is created True tokenizer_type str controls the sentencepiece model type. 'unigram' character_coverage float float value between 0 and 1 (as a percentage). For languages with a vast charset, can be < 1.0, but for all other languages, it should be set as 1.0 1.0 train_extremely_large_corpus bool If training on huge datasets, pass this flag to allow SentencePiece to build the tokenizer. False max_sentencepiece_length int Limits the maximum length of the SentencePiece subword that can be constructed. By default, no limit is placed. -1 Source code in thunder/text_processing/tokenizer.py def train_sentencepiece_model ( data_file : str , vocab_size : int , output_dir : str , sample_size : int = - 1 , do_lower_case : bool = True , tokenizer_type : str = \"unigram\" , character_coverage : float = 1.0 , train_extremely_large_corpus : bool = False , max_sentencepiece_length : int = - 1 , ) -> str : \"\"\" Creates sentence piece tokenizer model from data file. This is a direct port of `create_spt_model` present on the NEMO toolkit (nemo/collections/common/tokenizers/sentencepiece_tokenizer.py) Args: data_file: text file containing the sentences that will be used to train the model vocab_size: maximum vocabulary size output_dir: folder to save created tokenizer model and vocab sample_size: maximum number of sentences the trainer loads. -1 means to use all the data. do_lower_case: if text should be lower cased before tokenizer model is created tokenizer_type: controls the sentencepiece model type. character_coverage: float value between 0 and 1 (as a percentage). For languages with a vast charset, can be < 1.0, but for all other languages, it should be set as 1.0 train_extremely_large_corpus: If training on huge datasets, pass this flag to allow SentencePiece to build the tokenizer. max_sentencepiece_length: Limits the maximum length of the SentencePiece subword that can be constructed. By default, no limit is placed. \"\"\" data_file = Path ( data_file ) if not data_file or not data_file . exists (): raise ValueError ( f \"data_file must be valid file path, but got { data_file } \" ) output_dir = Path ( output_dir ) if ( output_dir / \"tokenizer.model\" ) . exists (): warn ( \"There's already a trained sentencepiece model at the output directory. Skipping train.\" ) return str ( output_dir ) output_dir . mkdir ( exist_ok = True ) cmd = ( f \"--input= { data_file } --model_prefix= { output_dir } /tokenizer \" f \"--vocab_size= { vocab_size } \" f \"--shuffle_input_sentence=true --hard_vocab_limit=false \" f \"--model_type= { tokenizer_type } \" f \"--character_coverage= { character_coverage } \" ) if do_lower_case : cmd += \" --normalization_rule_name=nmt_nfkc_cf\" if sample_size > 0 : cmd += f \" --input_sentence_size= { sample_size } \" if train_extremely_large_corpus : cmd += \" --train_extremely_large_corpus=true\" if max_sentencepiece_length >= 0 : cmd += f \" --max_sentencepiece_length= { max_sentencepiece_length } \" sentencepiece . SentencePieceTrainer . Train ( cmd ) return str ( output_dir )","title":"train_sentencepiece_model()"},{"location":"api/Text%20Processing/tokenize/#thunder.text_processing.tokenizer.word_tokenizer","text":"Tokenize input text splitting into words Parameters: Name Type Description Default text str Input text required Returns: Type Description List[str] Tokenized text Source code in thunder/text_processing/tokenizer.py def word_tokenizer ( text : str ) -> List [ str ]: \"\"\"Tokenize input text splitting into words Args: text : Input text Returns: Tokenized text \"\"\" return text . split ()","title":"word_tokenizer()"},{"location":"api/Text%20Processing/transform/","text":"BatchTextTransformer __init__ ( self , cfg ) special That class is the glue code that uses all of the text processing functions to encode/decode an entire batch of text at once. Parameters: Name Type Description Default cfg TextTransformConfig required config to create instance required Source code in thunder/text_processing/transform.py def __init__ ( self , cfg : TextTransformConfig ): \"\"\"That class is the glue code that uses all of the text processing functions to encode/decode an entire batch of text at once. Args: cfg: required config to create instance \"\"\" super () . __init__ () self . vocab = ( SimpleVocab ( cfg . initial_vocab_tokens ) if cfg . simple_vocab else Vocab ( cfg . initial_vocab_tokens ) ) self . tokenizer = ( BPETokenizer ( cfg . sentencepiece_model ) if cfg . sentencepiece_model else char_tokenizer ) decode_prediction ( self , predictions , remove_repeated = True ) Parameters: Name Type Description Default predictions Tensor Tensor of shape (batch, time) required remove_repeated bool controls if repeated elements without a blank between them will be removed while decoding True Returns: Type Description List[str] A list of decoded strings, one for each element in the batch. Source code in thunder/text_processing/transform.py @torch . jit . export def decode_prediction ( self , predictions : torch . Tensor , remove_repeated : bool = True ) -> List [ str ]: \"\"\" Args: predictions : Tensor of shape (batch, time) remove_repeated: controls if repeated elements without a blank between them will be removed while decoding Returns: A list of decoded strings, one for each element in the batch. \"\"\" out_list : List [ str ] = [] for element in predictions : # Remove consecutive repeated elements if remove_repeated : element = torch . unique_consecutive ( element ) # Map back to string out = self . vocab . decode_into_text ( element ) # Join prediction into one string out = \"\" . join ( out ) # _ is a special char only present on sentencepiece out = out . replace ( \"\u2581\" , \" \" ) out = self . vocab . remove_special_tokens ( out ) out_list . append ( out ) return out_list TextTransformConfig dataclass Configuration to create BatchTextTransformer Attributes: Name Type Description initial_vocab_tokens List[str] List of tokens to create the vocabulary, special tokens should not be included here. required. simple_vocab bool Controls if the used vocabulary will only have the blank token or more additional special tokens. defaults to False . sentencepiece_model Optional[str] Path to sentencepiece .model file, if applicable. from_sentencepiece ( output_dir ) classmethod Load the data from a folder that contains the tokenizer.vocab and tokenizer.model outputs from sentencepiece. Parameters: Name Type Description Default output_dir str Output directory of the sentencepiece training, that contains the required files. required Returns: Type Description TextTransformConfig Instance of TextTransformConfig with the corresponding data loaded. Source code in thunder/text_processing/transform.py @classmethod def from_sentencepiece ( cls , output_dir : str ) -> \"TextTransformConfig\" : \"\"\"Load the data from a folder that contains the `tokenizer.vocab` and `tokenizer.model` outputs from sentencepiece. Args: output_dir : Output directory of the sentencepiece training, that contains the required files. Returns: Instance of `TextTransformConfig` with the corresponding data loaded. \"\"\" special_tokens = [ \"<s>\" , \"</s>\" , \"<pad>\" , \"<unk>\" ] vocab = [] with open ( f \" { output_dir } /tokenizer.vocab\" , \"r\" ) as f : # Read tokens from each line and parse for vocab for line in f : piece = line . split ( \" \\t \" )[ 0 ] if piece in special_tokens : # skip special tokens continue vocab . append ( piece ) return cls ( initial_vocab_tokens = vocab , sentencepiece_model = f \" { output_dir } /tokenizer.model\" , )","title":"Transform"},{"location":"api/Text%20Processing/transform/#thunder.text_processing.transform.BatchTextTransformer","text":"","title":"BatchTextTransformer"},{"location":"api/Text%20Processing/transform/#thunder.text_processing.transform.BatchTextTransformer.__init__","text":"That class is the glue code that uses all of the text processing functions to encode/decode an entire batch of text at once. Parameters: Name Type Description Default cfg TextTransformConfig required config to create instance required Source code in thunder/text_processing/transform.py def __init__ ( self , cfg : TextTransformConfig ): \"\"\"That class is the glue code that uses all of the text processing functions to encode/decode an entire batch of text at once. Args: cfg: required config to create instance \"\"\" super () . __init__ () self . vocab = ( SimpleVocab ( cfg . initial_vocab_tokens ) if cfg . simple_vocab else Vocab ( cfg . initial_vocab_tokens ) ) self . tokenizer = ( BPETokenizer ( cfg . sentencepiece_model ) if cfg . sentencepiece_model else char_tokenizer )","title":"__init__()"},{"location":"api/Text%20Processing/transform/#thunder.text_processing.transform.BatchTextTransformer.decode_prediction","text":"Parameters: Name Type Description Default predictions Tensor Tensor of shape (batch, time) required remove_repeated bool controls if repeated elements without a blank between them will be removed while decoding True Returns: Type Description List[str] A list of decoded strings, one for each element in the batch. Source code in thunder/text_processing/transform.py @torch . jit . export def decode_prediction ( self , predictions : torch . Tensor , remove_repeated : bool = True ) -> List [ str ]: \"\"\" Args: predictions : Tensor of shape (batch, time) remove_repeated: controls if repeated elements without a blank between them will be removed while decoding Returns: A list of decoded strings, one for each element in the batch. \"\"\" out_list : List [ str ] = [] for element in predictions : # Remove consecutive repeated elements if remove_repeated : element = torch . unique_consecutive ( element ) # Map back to string out = self . vocab . decode_into_text ( element ) # Join prediction into one string out = \"\" . join ( out ) # _ is a special char only present on sentencepiece out = out . replace ( \"\u2581\" , \" \" ) out = self . vocab . remove_special_tokens ( out ) out_list . append ( out ) return out_list","title":"decode_prediction()"},{"location":"api/Text%20Processing/transform/#thunder.text_processing.transform.TextTransformConfig","text":"Configuration to create BatchTextTransformer Attributes: Name Type Description initial_vocab_tokens List[str] List of tokens to create the vocabulary, special tokens should not be included here. required. simple_vocab bool Controls if the used vocabulary will only have the blank token or more additional special tokens. defaults to False . sentencepiece_model Optional[str] Path to sentencepiece .model file, if applicable.","title":"TextTransformConfig"},{"location":"api/Text%20Processing/transform/#thunder.text_processing.transform.TextTransformConfig.from_sentencepiece","text":"Load the data from a folder that contains the tokenizer.vocab and tokenizer.model outputs from sentencepiece. Parameters: Name Type Description Default output_dir str Output directory of the sentencepiece training, that contains the required files. required Returns: Type Description TextTransformConfig Instance of TextTransformConfig with the corresponding data loaded. Source code in thunder/text_processing/transform.py @classmethod def from_sentencepiece ( cls , output_dir : str ) -> \"TextTransformConfig\" : \"\"\"Load the data from a folder that contains the `tokenizer.vocab` and `tokenizer.model` outputs from sentencepiece. Args: output_dir : Output directory of the sentencepiece training, that contains the required files. Returns: Instance of `TextTransformConfig` with the corresponding data loaded. \"\"\" special_tokens = [ \"<s>\" , \"</s>\" , \"<pad>\" , \"<unk>\" ] vocab = [] with open ( f \" { output_dir } /tokenizer.vocab\" , \"r\" ) as f : # Read tokens from each line and parse for vocab for line in f : piece = line . split ( \" \\t \" )[ 0 ] if piece in special_tokens : # skip special tokens continue vocab . append ( piece ) return cls ( initial_vocab_tokens = vocab , sentencepiece_model = f \" { output_dir } /tokenizer.model\" , )","title":"from_sentencepiece()"},{"location":"api/Text%20Processing/vocab/","text":"SimpleVocab __init__ ( self , initial_vocab_tokens , blank_token = '<blank>' ) special Class that represents a vocabulary, with the related methods to numericalize a sequence of tokens into numbers, and do the reverse mapping of numbers back to tokens. This is a simplified vocabulary that only has the ctc blank as special token. Parameters: Name Type Description Default initial_vocab_tokens List[str] Basic list of tokens that will be part of the vocabulary. DO NOT INCLUDE SPECIAL TOKENS THERE. Even the blank is automatically added by the class. Check docs required blank_token str Token that will represent the special ctc blank. '<blank>' Source code in thunder/text_processing/vocab.py def __init__ ( self , initial_vocab_tokens : List [ str ], blank_token : str = \"<blank>\" , ): \"\"\"Class that represents a vocabulary, with the related methods to numericalize a sequence of tokens into numbers, and do the reverse mapping of numbers back to tokens. This is a simplified vocabulary that only has the ctc blank as special token. Args: initial_vocab_tokens : Basic list of tokens that will be part of the vocabulary. DO NOT INCLUDE SPECIAL TOKENS THERE. Even the blank is automatically added by the class. Check [`docs`](https://scart97.github.io/thunder-speech/quick%20reference%20guide/#how-to-get-the-initial_vocab_tokens-from-my-dataset) blank_token : Token that will represent the special ctc blank. \"\"\" # There's no problem if the blank_idx == pad_idx self . blank_token = blank_token self . itos = initial_vocab_tokens + [ blank_token ] self . stoi = { token : i for i , token in enumerate ( self . itos )} self . blank_idx = self . itos . index ( self . blank_token ) self . pad_idx = self . blank_idx add_special_tokens ( self , tokens ) Function to add the special start and end tokens to some tokenized text. Parameters: Name Type Description Default tokens List[str] Tokenized text required Returns: Type Description List[str] Text with the special tokens added. Source code in thunder/text_processing/vocab.py def add_special_tokens ( self , tokens : List [ str ]) -> List [ str ]: \"\"\"Function to add the special start and end tokens to some tokenized text. Args: tokens : Tokenized text Returns: Text with the special tokens added. \"\"\" return tokens decode_into_text ( self , indices ) Function to transform back a list of numbers into the corresponding tokens. Parameters: Name Type Description Default indices Tensor Numeric representation. Usually is the result of the model, after a greedy decoding required Returns: Type Description List[str] Corresponding tokens Source code in thunder/text_processing/vocab.py def decode_into_text ( self , indices : torch . Tensor ) -> List [ str ]: \"\"\"Function to transform back a list of numbers into the corresponding tokens. Args: indices : Numeric representation. Usually is the result of the model, after a greedy decoding Returns: Corresponding tokens \"\"\" return [ self . itos [ it ] for it in indices ] numericalize ( self , tokens ) Function to transform a list of tokens into the corresponding numeric representation. Parameters: Name Type Description Default tokens List[str] A single list of tokens to be transformed required Returns: Type Description Tensor The corresponding numeric representation Source code in thunder/text_processing/vocab.py def numericalize ( self , tokens : List [ str ]) -> torch . Tensor : \"\"\"Function to transform a list of tokens into the corresponding numeric representation. Args: tokens : A single list of tokens to be transformed Returns: The corresponding numeric representation \"\"\" # When in nemo style vocab, there's no unknown token # So we filter out all of the tokens not in the vocab filtered : List [ str ] = [] for t in tokens : if t in self . itos : filtered . append ( t ) # https://github.com/pytorch/pytorch/issues/27504 - alternative to the loop # filtered = [t for t in tokens if t in self.itos] return torch . tensor ([ self . stoi . get ( it ) for it in filtered ], dtype = torch . long ) remove_special_tokens ( self , text ) Function to remove the special tokens from the prediction. Parameters: Name Type Description Default text str Decoded text required Returns: Type Description str Text with the special tokens removed. Source code in thunder/text_processing/vocab.py def remove_special_tokens ( self , text : str ) -> str : \"\"\"Function to remove the special tokens from the prediction. Args: text : Decoded text Returns: Text with the special tokens removed. \"\"\" return text . replace ( self . blank_token , \"\" ) Vocab __init__ ( self , initial_vocab_tokens , pad_token = '<pad>' , unknown_token = '<unk>' , start_token = '<bos>' , end_token = '<eos>' ) special Class that represents a vocabulary, with the related methods to numericalize a sequence of tokens into numbers, and do the reverse mapping of numbers back to tokens. Parameters: Name Type Description Default initial_vocab_tokens List[str] Basic list of tokens that will be part of the vocabulary. DO NOT INCLUDE SPECIAL TOKENS THERE. Even the blank is automatically added by the class. Check docs required pad_token str Token that will represent padding, and also act as the ctc blank. '<pad>' unknown_token str Token that will represent unknown elements. Notice that this is different than the blank used by ctc. '<unk>' start_token str Token that will represent the beginning of the sequence. '<bos>' end_token str Token that will represent the end of the sequence. '<eos>' Source code in thunder/text_processing/vocab.py def __init__ ( self , initial_vocab_tokens : List [ str ], pad_token : str = \"<pad>\" , unknown_token : str = \"<unk>\" , start_token : str = \"<bos>\" , end_token : str = \"<eos>\" , ): \"\"\"Class that represents a vocabulary, with the related methods to numericalize a sequence of tokens into numbers, and do the reverse mapping of numbers back to tokens. Args: initial_vocab_tokens : Basic list of tokens that will be part of the vocabulary. DO NOT INCLUDE SPECIAL TOKENS THERE. Even the blank is automatically added by the class. Check [`docs`](https://scart97.github.io/thunder-speech/quick%20reference%20guide/#how-to-get-the-initial_vocab_tokens-from-my-dataset) pad_token : Token that will represent padding, and also act as the ctc blank. unknown_token : Token that will represent unknown elements. Notice that this is different than the blank used by ctc. start_token : Token that will represent the beginning of the sequence. end_token : Token that will represent the end of the sequence. \"\"\" self . pad_token = pad_token self . unknown_token = unknown_token self . start_token = start_token self . end_token = end_token # There's no problem if the blank_idx == pad_idx self . blank_token = self . pad_token self . itos = initial_vocab_tokens + [ pad_token , unknown_token , start_token , end_token , ] self . stoi = { token : i for i , token in enumerate ( self . itos )} self . pad_idx = self . itos . index ( self . pad_token ) self . unknown_idx = self . itos . index ( self . unknown_token ) self . start_idx = self . itos . index ( self . start_token ) self . end_idx = self . itos . index ( self . end_token ) self . blank_idx = self . itos . index ( self . blank_token ) add_special_tokens ( self , tokens ) Function to add the special start and end tokens to some tokenized text. Parameters: Name Type Description Default tokens List[str] Tokenized text required Returns: Type Description List[str] Text with the special tokens added. Source code in thunder/text_processing/vocab.py def add_special_tokens ( self , tokens : List [ str ]) -> List [ str ]: \"\"\"Function to add the special start and end tokens to some tokenized text. Args: tokens : Tokenized text Returns: Text with the special tokens added. \"\"\" return [ self . start_token ] + tokens + [ self . end_token ] decode_into_text ( self , indices ) Function to transform back a list of numbers into the corresponding tokens. Parameters: Name Type Description Default indices Tensor Numeric representation. Usually is the result of the model, after a greedy decoding required Returns: Type Description List[str] Corresponding tokens Source code in thunder/text_processing/vocab.py def decode_into_text ( self , indices : torch . Tensor ) -> List [ str ]: \"\"\"Function to transform back a list of numbers into the corresponding tokens. Args: indices : Numeric representation. Usually is the result of the model, after a greedy decoding Returns: Corresponding tokens \"\"\" return [ self . itos [ it ] for it in indices ] numericalize ( self , tokens ) Function to transform a list of tokens into the corresponding numeric representation. Parameters: Name Type Description Default tokens List[str] A single list of tokens to be transformed required Returns: Type Description Tensor The corresponding numeric representation Source code in thunder/text_processing/vocab.py def numericalize ( self , tokens : List [ str ]) -> torch . Tensor : \"\"\"Function to transform a list of tokens into the corresponding numeric representation. Args: tokens : A single list of tokens to be transformed Returns: The corresponding numeric representation \"\"\" return torch . tensor ( [ self . stoi . get ( it , self . unknown_idx ) for it in tokens ], dtype = torch . long ) remove_special_tokens ( self , text ) Function to remove the special tokens from the prediction. Parameters: Name Type Description Default text str Decoded text required Returns: Type Description str Text with the special tokens removed. Source code in thunder/text_processing/vocab.py def remove_special_tokens ( self , text : str ) -> str : \"\"\"Function to remove the special tokens from the prediction. Args: text : Decoded text Returns: Text with the special tokens removed. \"\"\" text = text . replace ( self . blank_token , \"\" ) text = text . replace ( self . pad_token , \"\" ) text = text . replace ( self . start_token , \"\" ) text = text . replace ( self . end_token , \"\" ) return text","title":"Vocab"},{"location":"api/Text%20Processing/vocab/#thunder.text_processing.vocab.SimpleVocab","text":"","title":"SimpleVocab"},{"location":"api/Text%20Processing/vocab/#thunder.text_processing.vocab.SimpleVocab.__init__","text":"Class that represents a vocabulary, with the related methods to numericalize a sequence of tokens into numbers, and do the reverse mapping of numbers back to tokens. This is a simplified vocabulary that only has the ctc blank as special token. Parameters: Name Type Description Default initial_vocab_tokens List[str] Basic list of tokens that will be part of the vocabulary. DO NOT INCLUDE SPECIAL TOKENS THERE. Even the blank is automatically added by the class. Check docs required blank_token str Token that will represent the special ctc blank. '<blank>' Source code in thunder/text_processing/vocab.py def __init__ ( self , initial_vocab_tokens : List [ str ], blank_token : str = \"<blank>\" , ): \"\"\"Class that represents a vocabulary, with the related methods to numericalize a sequence of tokens into numbers, and do the reverse mapping of numbers back to tokens. This is a simplified vocabulary that only has the ctc blank as special token. Args: initial_vocab_tokens : Basic list of tokens that will be part of the vocabulary. DO NOT INCLUDE SPECIAL TOKENS THERE. Even the blank is automatically added by the class. Check [`docs`](https://scart97.github.io/thunder-speech/quick%20reference%20guide/#how-to-get-the-initial_vocab_tokens-from-my-dataset) blank_token : Token that will represent the special ctc blank. \"\"\" # There's no problem if the blank_idx == pad_idx self . blank_token = blank_token self . itos = initial_vocab_tokens + [ blank_token ] self . stoi = { token : i for i , token in enumerate ( self . itos )} self . blank_idx = self . itos . index ( self . blank_token ) self . pad_idx = self . blank_idx","title":"__init__()"},{"location":"api/Text%20Processing/vocab/#thunder.text_processing.vocab.SimpleVocab.add_special_tokens","text":"Function to add the special start and end tokens to some tokenized text. Parameters: Name Type Description Default tokens List[str] Tokenized text required Returns: Type Description List[str] Text with the special tokens added. Source code in thunder/text_processing/vocab.py def add_special_tokens ( self , tokens : List [ str ]) -> List [ str ]: \"\"\"Function to add the special start and end tokens to some tokenized text. Args: tokens : Tokenized text Returns: Text with the special tokens added. \"\"\" return tokens","title":"add_special_tokens()"},{"location":"api/Text%20Processing/vocab/#thunder.text_processing.vocab.SimpleVocab.decode_into_text","text":"Function to transform back a list of numbers into the corresponding tokens. Parameters: Name Type Description Default indices Tensor Numeric representation. Usually is the result of the model, after a greedy decoding required Returns: Type Description List[str] Corresponding tokens Source code in thunder/text_processing/vocab.py def decode_into_text ( self , indices : torch . Tensor ) -> List [ str ]: \"\"\"Function to transform back a list of numbers into the corresponding tokens. Args: indices : Numeric representation. Usually is the result of the model, after a greedy decoding Returns: Corresponding tokens \"\"\" return [ self . itos [ it ] for it in indices ]","title":"decode_into_text()"},{"location":"api/Text%20Processing/vocab/#thunder.text_processing.vocab.SimpleVocab.numericalize","text":"Function to transform a list of tokens into the corresponding numeric representation. Parameters: Name Type Description Default tokens List[str] A single list of tokens to be transformed required Returns: Type Description Tensor The corresponding numeric representation Source code in thunder/text_processing/vocab.py def numericalize ( self , tokens : List [ str ]) -> torch . Tensor : \"\"\"Function to transform a list of tokens into the corresponding numeric representation. Args: tokens : A single list of tokens to be transformed Returns: The corresponding numeric representation \"\"\" # When in nemo style vocab, there's no unknown token # So we filter out all of the tokens not in the vocab filtered : List [ str ] = [] for t in tokens : if t in self . itos : filtered . append ( t ) # https://github.com/pytorch/pytorch/issues/27504 - alternative to the loop # filtered = [t for t in tokens if t in self.itos] return torch . tensor ([ self . stoi . get ( it ) for it in filtered ], dtype = torch . long )","title":"numericalize()"},{"location":"api/Text%20Processing/vocab/#thunder.text_processing.vocab.SimpleVocab.remove_special_tokens","text":"Function to remove the special tokens from the prediction. Parameters: Name Type Description Default text str Decoded text required Returns: Type Description str Text with the special tokens removed. Source code in thunder/text_processing/vocab.py def remove_special_tokens ( self , text : str ) -> str : \"\"\"Function to remove the special tokens from the prediction. Args: text : Decoded text Returns: Text with the special tokens removed. \"\"\" return text . replace ( self . blank_token , \"\" )","title":"remove_special_tokens()"},{"location":"api/Text%20Processing/vocab/#thunder.text_processing.vocab.Vocab","text":"","title":"Vocab"},{"location":"api/Text%20Processing/vocab/#thunder.text_processing.vocab.Vocab.__init__","text":"Class that represents a vocabulary, with the related methods to numericalize a sequence of tokens into numbers, and do the reverse mapping of numbers back to tokens. Parameters: Name Type Description Default initial_vocab_tokens List[str] Basic list of tokens that will be part of the vocabulary. DO NOT INCLUDE SPECIAL TOKENS THERE. Even the blank is automatically added by the class. Check docs required pad_token str Token that will represent padding, and also act as the ctc blank. '<pad>' unknown_token str Token that will represent unknown elements. Notice that this is different than the blank used by ctc. '<unk>' start_token str Token that will represent the beginning of the sequence. '<bos>' end_token str Token that will represent the end of the sequence. '<eos>' Source code in thunder/text_processing/vocab.py def __init__ ( self , initial_vocab_tokens : List [ str ], pad_token : str = \"<pad>\" , unknown_token : str = \"<unk>\" , start_token : str = \"<bos>\" , end_token : str = \"<eos>\" , ): \"\"\"Class that represents a vocabulary, with the related methods to numericalize a sequence of tokens into numbers, and do the reverse mapping of numbers back to tokens. Args: initial_vocab_tokens : Basic list of tokens that will be part of the vocabulary. DO NOT INCLUDE SPECIAL TOKENS THERE. Even the blank is automatically added by the class. Check [`docs`](https://scart97.github.io/thunder-speech/quick%20reference%20guide/#how-to-get-the-initial_vocab_tokens-from-my-dataset) pad_token : Token that will represent padding, and also act as the ctc blank. unknown_token : Token that will represent unknown elements. Notice that this is different than the blank used by ctc. start_token : Token that will represent the beginning of the sequence. end_token : Token that will represent the end of the sequence. \"\"\" self . pad_token = pad_token self . unknown_token = unknown_token self . start_token = start_token self . end_token = end_token # There's no problem if the blank_idx == pad_idx self . blank_token = self . pad_token self . itos = initial_vocab_tokens + [ pad_token , unknown_token , start_token , end_token , ] self . stoi = { token : i for i , token in enumerate ( self . itos )} self . pad_idx = self . itos . index ( self . pad_token ) self . unknown_idx = self . itos . index ( self . unknown_token ) self . start_idx = self . itos . index ( self . start_token ) self . end_idx = self . itos . index ( self . end_token ) self . blank_idx = self . itos . index ( self . blank_token )","title":"__init__()"},{"location":"api/Text%20Processing/vocab/#thunder.text_processing.vocab.Vocab.add_special_tokens","text":"Function to add the special start and end tokens to some tokenized text. Parameters: Name Type Description Default tokens List[str] Tokenized text required Returns: Type Description List[str] Text with the special tokens added. Source code in thunder/text_processing/vocab.py def add_special_tokens ( self , tokens : List [ str ]) -> List [ str ]: \"\"\"Function to add the special start and end tokens to some tokenized text. Args: tokens : Tokenized text Returns: Text with the special tokens added. \"\"\" return [ self . start_token ] + tokens + [ self . end_token ]","title":"add_special_tokens()"},{"location":"api/Text%20Processing/vocab/#thunder.text_processing.vocab.Vocab.decode_into_text","text":"Function to transform back a list of numbers into the corresponding tokens. Parameters: Name Type Description Default indices Tensor Numeric representation. Usually is the result of the model, after a greedy decoding required Returns: Type Description List[str] Corresponding tokens Source code in thunder/text_processing/vocab.py def decode_into_text ( self , indices : torch . Tensor ) -> List [ str ]: \"\"\"Function to transform back a list of numbers into the corresponding tokens. Args: indices : Numeric representation. Usually is the result of the model, after a greedy decoding Returns: Corresponding tokens \"\"\" return [ self . itos [ it ] for it in indices ]","title":"decode_into_text()"},{"location":"api/Text%20Processing/vocab/#thunder.text_processing.vocab.Vocab.numericalize","text":"Function to transform a list of tokens into the corresponding numeric representation. Parameters: Name Type Description Default tokens List[str] A single list of tokens to be transformed required Returns: Type Description Tensor The corresponding numeric representation Source code in thunder/text_processing/vocab.py def numericalize ( self , tokens : List [ str ]) -> torch . Tensor : \"\"\"Function to transform a list of tokens into the corresponding numeric representation. Args: tokens : A single list of tokens to be transformed Returns: The corresponding numeric representation \"\"\" return torch . tensor ( [ self . stoi . get ( it , self . unknown_idx ) for it in tokens ], dtype = torch . long )","title":"numericalize()"},{"location":"api/Text%20Processing/vocab/#thunder.text_processing.vocab.Vocab.remove_special_tokens","text":"Function to remove the special tokens from the prediction. Parameters: Name Type Description Default text str Decoded text required Returns: Type Description str Text with the special tokens removed. Source code in thunder/text_processing/vocab.py def remove_special_tokens ( self , text : str ) -> str : \"\"\"Function to remove the special tokens from the prediction. Args: text : Decoded text Returns: Text with the special tokens removed. \"\"\" text = text . replace ( self . blank_token , \"\" ) text = text . replace ( self . pad_token , \"\" ) text = text . replace ( self . start_token , \"\" ) text = text . replace ( self . end_token , \"\" ) return text","title":"remove_special_tokens()"},{"location":"api/Wav2Vec/module/","text":"ModelConfig dataclass Configuration to create the wav2vec 2.0 encoder. Attributes: Name Type Description model_name str Name of the original huggingface checkpoint to load from. defaults to 'facebook/wav2vec2-base' gradient_checkpointing bool Use gradient checkpointing to save memory at the expense of slower backward pass. defaults to False. additional_kwargs Dict[str, Any] Any other option that can be passed to the original Wav2Vec2Model.from_pretrained. defaults to {}. decoder_dropout float Dropout before the final decoding layer. defaults to 0.1. OptimizerConfig dataclass Configuration used by the optimizer Attributes: Name Type Description learning_rate float learning rate. defaults to 3e-4. Wav2Vec2Module __init__ ( self , text_cfg , encoder_cfg = ModelConfig ( model_name = 'facebook/wav2vec2-base' , gradient_checkpointing = False , additional_kwargs = {}, decoder_dropout = 0.1 ), optim_cfg = OptimizerConfig ( learning_rate = 0.0003 )) special Wav2Vec model for fine-tuning. Parameters: Name Type Description Default text_cfg TextTransformConfig Configuration for the text processing pipeline required encoder_cfg ModelConfig Configuration for the wav2vec encoder ModelConfig(model_name='facebook/wav2vec2-base', gradient_checkpointing=False, additional_kwargs={}, decoder_dropout=0.1) optim_cfg OptimizerConfig Configuration for the optimizer used during training OptimizerConfig(learning_rate=0.0003) Source code in thunder/wav2vec/module.py def __init__ ( self , text_cfg : TextTransformConfig , encoder_cfg : ModelConfig = ModelConfig (), optim_cfg : OptimizerConfig = OptimizerConfig (), ): \"\"\"Wav2Vec model for fine-tuning. Args: text_cfg: Configuration for the text processing pipeline encoder_cfg: Configuration for the wav2vec encoder optim_cfg: Configuration for the optimizer used during training \"\"\" super () . __init__ () self . save_hyperparameters () self . audio_transform = Wav2Vec2Preprocess () self . encoder = Wav2Vec2Model . from_pretrained ( encoder_cfg . model_name , gradient_checkpointing = encoder_cfg . gradient_checkpointing , ** encoder_cfg . additional_kwargs , ) self . encoder . feature_extractor . _freeze_parameters () self . text_transform = BatchTextTransformer ( text_cfg ) self . decoder = linear_decoder ( self . encoder . config . hidden_size , len ( self . text_transform . vocab ), encoder_cfg . decoder_dropout , ) # Metrics self . val_cer = CER () self . val_wer = WER () # Example input is one second of fake audio self . example_input_array = torch . randn (( 10 , 16000 )) configure_optimizers ( self ) Configuring optimizers. Check the original lightning docs for more info. Returns: Type Description Optimizer Optimizer, and optionally the learning rate scheduler. Source code in thunder/wav2vec/module.py def configure_optimizers ( self ) -> torch . optim . Optimizer : \"\"\"Configuring optimizers. Check the original lightning docs for more info. Returns: Optimizer, and optionally the learning rate scheduler. \"\"\" return torch . optim . Adam ( filter ( lambda p : p . requires_grad , self . parameters ()), lr = self . hparams . optim_cfg . learning_rate , ) forward ( self , audio ) Process the audio tensor to create the probabilities. Parameters: Name Type Description Default audio Tensor Audio tensor of shape [batch_size, time] required Returns: Type Description Tensor Tensor with the prediction probabilities. Source code in thunder/wav2vec/module.py def forward ( self , audio : Tensor ) -> Tensor : \"\"\"Process the audio tensor to create the probabilities. Args: audio : Audio tensor of shape [batch_size, time] Returns: Tensor with the prediction probabilities. \"\"\" features = self . audio_transform ( audio ) encoded_dict = self . encoder ( features ) return self . decoder ( encoded_dict . last_hidden_state ) predict ( self , x ) Use this function during inference to predict. Parameters: Name Type Description Default x Tensor Audio tensor of shape [batch_size, time] required Returns: Type Description List[str] A list of strings, each one contains the corresponding transcription to the original batch element. Source code in thunder/wav2vec/module.py @torch . jit . export def predict ( self , x : Tensor ) -> List [ str ]: \"\"\"Use this function during inference to predict. Args: x : Audio tensor of shape [batch_size, time] Returns: A list of strings, each one contains the corresponding transcription to the original batch element. \"\"\" pred = self ( x ) return self . text_transform . decode_prediction ( pred . argmax ( 1 )) training_step ( self , batch , batch_idx ) Training step. Check the original lightning docs for more information. Parameters: Name Type Description Default batch Tuple[torch.Tensor, torch.Tensor, List[str]] Tuple containing the batched audios, normalized lengths and the corresponding text labels. required batch_idx int Batch index required Returns: Type Description Tensor Training loss for that batch Source code in thunder/wav2vec/module.py def training_step ( self , batch : Tuple [ torch . Tensor , torch . Tensor , List [ str ]], batch_idx : int ) -> torch . Tensor : \"\"\"Training step. Check the original lightning docs for more information. Args: batch : Tuple containing the batched audios, normalized lengths and the corresponding text labels. batch_idx : Batch index Returns: Training loss for that batch \"\"\" audio , audio_lens , texts = batch y , y_lens = self . text_transform . encode ( texts , device = self . device ) probabilities = self ( audio ) loss = calculate_ctc ( probabilities , y , audio_lens , y_lens , self . text_transform . vocab . blank_idx ) self . log ( \"loss/train_loss\" , loss ) return loss validation_step ( self , batch , batch_idx ) Validation step. Check the original lightning docs for more information. Parameters: Name Type Description Default batch Tuple[torch.Tensor, torch.Tensor, List[str]] Tuple containing the batched audios, normalized lengths and the corresponding text labels. required batch_idx int Batch index required Returns: Type Description Tensor Validation loss for that batch Source code in thunder/wav2vec/module.py def validation_step ( self , batch : Tuple [ torch . Tensor , torch . Tensor , List [ str ]], batch_idx : int ) -> torch . Tensor : \"\"\"Validation step. Check the original lightning docs for more information. Args: batch : Tuple containing the batched audios, normalized lengths and the corresponding text labels. batch_idx : Batch index Returns: Validation loss for that batch \"\"\" audio , audio_lens , texts = batch y , y_lens = self . text_transform . encode ( texts , device = self . device ) probabilities = self ( audio ) loss = calculate_ctc ( probabilities , y , audio_lens , y_lens , self . text_transform . vocab . blank_idx ) decoded_preds = self . text_transform . decode_prediction ( probabilities . argmax ( 1 )) decoded_targets = self . text_transform . decode_prediction ( y ) self . val_cer ( decoded_preds , decoded_targets ) self . val_wer ( decoded_preds , decoded_targets ) self . log ( \"loss/val_loss\" , loss ) self . log ( \"metrics/cer\" , self . val_cer , on_epoch = True ) self . log ( \"metrics/wer\" , self . val_wer , on_epoch = True ) return loss Wav2Vec2Scriptable __init__ ( self , module , quantized = False ) special Wav2vec model ready to be jit scripted and used in inference. This class is necessary because the torchaudio imported model is not a drop in replacement of the transformers implementation, causing some trouble with the jit. Parameters: Name Type Description Default module Wav2Vec2Module The trained Wav2Vec2 module that you want to export required quantized bool Controls if quantization will be applied to the model. False Source code in thunder/wav2vec/module.py def __init__ ( self , module : Wav2Vec2Module , quantized : bool = False ): \"\"\"Wav2vec model ready to be jit scripted and used in inference. This class is necessary because the torchaudio imported model is not a drop in replacement of the transformers implementation, causing some trouble with the jit. Args: module : The trained Wav2Vec2 module that you want to export quantized : Controls if quantization will be applied to the model. \"\"\" super () . __init__ () # Transforming model to torchaudio one encoder_config = _get_config ( module . encoder . config ) encoder_config [ \"encoder_num_out\" ] = len ( module . text_transform . vocab ) imported = _get_model ( ** encoder_config ) imported . feature_extractor . load_state_dict ( module . encoder . feature_extractor . state_dict () ) imported . encoder . feature_projection . load_state_dict ( module . encoder . feature_projection . state_dict () ) imported . encoder . transformer . load_state_dict ( module . encoder . encoder . state_dict () ) imported . encoder . readout . load_state_dict ( module . decoder [ 1 ] . state_dict ()) if quantized : imported . encoder . transformer . pos_conv_embed . __prepare_scriptable__ () imported = torch . quantization . quantize_dynamic ( imported , qconfig_spec = { torch . nn . Linear }, dtype = torch . qint8 ) self . model = imported self . audio_transform = module . audio_transform self . text_transform = module . text_transform forward ( self , audio ) Process the audio tensor to create the probabilities. Parameters: Name Type Description Default audio Tensor Audio tensor of shape [batch_size, time] required Returns: Type Description Tensor Tensor with the prediction probabilities. Source code in thunder/wav2vec/module.py def forward ( self , audio : Tensor ) -> Tensor : \"\"\"Process the audio tensor to create the probabilities. Args: audio : Audio tensor of shape [batch_size, time] Returns: Tensor with the prediction probabilities. \"\"\" features = self . audio_transform ( audio ) outputs = self . model ( features ) probs = outputs [ 0 ] # Change from (batch, time, #vocab) to (batch, #vocab, time) # that is expected by the rest of the library return probs . permute ( 0 , 2 , 1 ) predict ( self , x ) Use this function during inference to predict. Parameters: Name Type Description Default x Tensor Audio tensor of shape [batch_size, time] required Returns: Type Description List[str] A list of strings, each one contains the corresponding transcription to the original batch element. Source code in thunder/wav2vec/module.py @torch . jit . export def predict ( self , x : Tensor ) -> List [ str ]: \"\"\"Use this function during inference to predict. Args: x : Audio tensor of shape [batch_size, time] Returns: A list of strings, each one contains the corresponding transcription to the original batch element. \"\"\" pred = self ( x ) return self . text_transform . decode_prediction ( pred . argmax ( 1 ))","title":"Module"},{"location":"api/Wav2Vec/module/#thunder.wav2vec.module.ModelConfig","text":"Configuration to create the wav2vec 2.0 encoder. Attributes: Name Type Description model_name str Name of the original huggingface checkpoint to load from. defaults to 'facebook/wav2vec2-base' gradient_checkpointing bool Use gradient checkpointing to save memory at the expense of slower backward pass. defaults to False. additional_kwargs Dict[str, Any] Any other option that can be passed to the original Wav2Vec2Model.from_pretrained. defaults to {}. decoder_dropout float Dropout before the final decoding layer. defaults to 0.1.","title":"ModelConfig"},{"location":"api/Wav2Vec/module/#thunder.wav2vec.module.OptimizerConfig","text":"Configuration used by the optimizer Attributes: Name Type Description learning_rate float learning rate. defaults to 3e-4.","title":"OptimizerConfig"},{"location":"api/Wav2Vec/module/#thunder.wav2vec.module.Wav2Vec2Module","text":"","title":"Wav2Vec2Module"},{"location":"api/Wav2Vec/module/#thunder.wav2vec.module.Wav2Vec2Module.__init__","text":"Wav2Vec model for fine-tuning. Parameters: Name Type Description Default text_cfg TextTransformConfig Configuration for the text processing pipeline required encoder_cfg ModelConfig Configuration for the wav2vec encoder ModelConfig(model_name='facebook/wav2vec2-base', gradient_checkpointing=False, additional_kwargs={}, decoder_dropout=0.1) optim_cfg OptimizerConfig Configuration for the optimizer used during training OptimizerConfig(learning_rate=0.0003) Source code in thunder/wav2vec/module.py def __init__ ( self , text_cfg : TextTransformConfig , encoder_cfg : ModelConfig = ModelConfig (), optim_cfg : OptimizerConfig = OptimizerConfig (), ): \"\"\"Wav2Vec model for fine-tuning. Args: text_cfg: Configuration for the text processing pipeline encoder_cfg: Configuration for the wav2vec encoder optim_cfg: Configuration for the optimizer used during training \"\"\" super () . __init__ () self . save_hyperparameters () self . audio_transform = Wav2Vec2Preprocess () self . encoder = Wav2Vec2Model . from_pretrained ( encoder_cfg . model_name , gradient_checkpointing = encoder_cfg . gradient_checkpointing , ** encoder_cfg . additional_kwargs , ) self . encoder . feature_extractor . _freeze_parameters () self . text_transform = BatchTextTransformer ( text_cfg ) self . decoder = linear_decoder ( self . encoder . config . hidden_size , len ( self . text_transform . vocab ), encoder_cfg . decoder_dropout , ) # Metrics self . val_cer = CER () self . val_wer = WER () # Example input is one second of fake audio self . example_input_array = torch . randn (( 10 , 16000 ))","title":"__init__()"},{"location":"api/Wav2Vec/module/#thunder.wav2vec.module.Wav2Vec2Module.configure_optimizers","text":"Configuring optimizers. Check the original lightning docs for more info. Returns: Type Description Optimizer Optimizer, and optionally the learning rate scheduler. Source code in thunder/wav2vec/module.py def configure_optimizers ( self ) -> torch . optim . Optimizer : \"\"\"Configuring optimizers. Check the original lightning docs for more info. Returns: Optimizer, and optionally the learning rate scheduler. \"\"\" return torch . optim . Adam ( filter ( lambda p : p . requires_grad , self . parameters ()), lr = self . hparams . optim_cfg . learning_rate , )","title":"configure_optimizers()"},{"location":"api/Wav2Vec/module/#thunder.wav2vec.module.Wav2Vec2Module.forward","text":"Process the audio tensor to create the probabilities. Parameters: Name Type Description Default audio Tensor Audio tensor of shape [batch_size, time] required Returns: Type Description Tensor Tensor with the prediction probabilities. Source code in thunder/wav2vec/module.py def forward ( self , audio : Tensor ) -> Tensor : \"\"\"Process the audio tensor to create the probabilities. Args: audio : Audio tensor of shape [batch_size, time] Returns: Tensor with the prediction probabilities. \"\"\" features = self . audio_transform ( audio ) encoded_dict = self . encoder ( features ) return self . decoder ( encoded_dict . last_hidden_state )","title":"forward()"},{"location":"api/Wav2Vec/module/#thunder.wav2vec.module.Wav2Vec2Module.predict","text":"Use this function during inference to predict. Parameters: Name Type Description Default x Tensor Audio tensor of shape [batch_size, time] required Returns: Type Description List[str] A list of strings, each one contains the corresponding transcription to the original batch element. Source code in thunder/wav2vec/module.py @torch . jit . export def predict ( self , x : Tensor ) -> List [ str ]: \"\"\"Use this function during inference to predict. Args: x : Audio tensor of shape [batch_size, time] Returns: A list of strings, each one contains the corresponding transcription to the original batch element. \"\"\" pred = self ( x ) return self . text_transform . decode_prediction ( pred . argmax ( 1 ))","title":"predict()"},{"location":"api/Wav2Vec/module/#thunder.wav2vec.module.Wav2Vec2Module.training_step","text":"Training step. Check the original lightning docs for more information. Parameters: Name Type Description Default batch Tuple[torch.Tensor, torch.Tensor, List[str]] Tuple containing the batched audios, normalized lengths and the corresponding text labels. required batch_idx int Batch index required Returns: Type Description Tensor Training loss for that batch Source code in thunder/wav2vec/module.py def training_step ( self , batch : Tuple [ torch . Tensor , torch . Tensor , List [ str ]], batch_idx : int ) -> torch . Tensor : \"\"\"Training step. Check the original lightning docs for more information. Args: batch : Tuple containing the batched audios, normalized lengths and the corresponding text labels. batch_idx : Batch index Returns: Training loss for that batch \"\"\" audio , audio_lens , texts = batch y , y_lens = self . text_transform . encode ( texts , device = self . device ) probabilities = self ( audio ) loss = calculate_ctc ( probabilities , y , audio_lens , y_lens , self . text_transform . vocab . blank_idx ) self . log ( \"loss/train_loss\" , loss ) return loss","title":"training_step()"},{"location":"api/Wav2Vec/module/#thunder.wav2vec.module.Wav2Vec2Module.validation_step","text":"Validation step. Check the original lightning docs for more information. Parameters: Name Type Description Default batch Tuple[torch.Tensor, torch.Tensor, List[str]] Tuple containing the batched audios, normalized lengths and the corresponding text labels. required batch_idx int Batch index required Returns: Type Description Tensor Validation loss for that batch Source code in thunder/wav2vec/module.py def validation_step ( self , batch : Tuple [ torch . Tensor , torch . Tensor , List [ str ]], batch_idx : int ) -> torch . Tensor : \"\"\"Validation step. Check the original lightning docs for more information. Args: batch : Tuple containing the batched audios, normalized lengths and the corresponding text labels. batch_idx : Batch index Returns: Validation loss for that batch \"\"\" audio , audio_lens , texts = batch y , y_lens = self . text_transform . encode ( texts , device = self . device ) probabilities = self ( audio ) loss = calculate_ctc ( probabilities , y , audio_lens , y_lens , self . text_transform . vocab . blank_idx ) decoded_preds = self . text_transform . decode_prediction ( probabilities . argmax ( 1 )) decoded_targets = self . text_transform . decode_prediction ( y ) self . val_cer ( decoded_preds , decoded_targets ) self . val_wer ( decoded_preds , decoded_targets ) self . log ( \"loss/val_loss\" , loss ) self . log ( \"metrics/cer\" , self . val_cer , on_epoch = True ) self . log ( \"metrics/wer\" , self . val_wer , on_epoch = True ) return loss","title":"validation_step()"},{"location":"api/Wav2Vec/module/#thunder.wav2vec.module.Wav2Vec2Scriptable","text":"","title":"Wav2Vec2Scriptable"},{"location":"api/Wav2Vec/module/#thunder.wav2vec.module.Wav2Vec2Scriptable.__init__","text":"Wav2vec model ready to be jit scripted and used in inference. This class is necessary because the torchaudio imported model is not a drop in replacement of the transformers implementation, causing some trouble with the jit. Parameters: Name Type Description Default module Wav2Vec2Module The trained Wav2Vec2 module that you want to export required quantized bool Controls if quantization will be applied to the model. False Source code in thunder/wav2vec/module.py def __init__ ( self , module : Wav2Vec2Module , quantized : bool = False ): \"\"\"Wav2vec model ready to be jit scripted and used in inference. This class is necessary because the torchaudio imported model is not a drop in replacement of the transformers implementation, causing some trouble with the jit. Args: module : The trained Wav2Vec2 module that you want to export quantized : Controls if quantization will be applied to the model. \"\"\" super () . __init__ () # Transforming model to torchaudio one encoder_config = _get_config ( module . encoder . config ) encoder_config [ \"encoder_num_out\" ] = len ( module . text_transform . vocab ) imported = _get_model ( ** encoder_config ) imported . feature_extractor . load_state_dict ( module . encoder . feature_extractor . state_dict () ) imported . encoder . feature_projection . load_state_dict ( module . encoder . feature_projection . state_dict () ) imported . encoder . transformer . load_state_dict ( module . encoder . encoder . state_dict () ) imported . encoder . readout . load_state_dict ( module . decoder [ 1 ] . state_dict ()) if quantized : imported . encoder . transformer . pos_conv_embed . __prepare_scriptable__ () imported = torch . quantization . quantize_dynamic ( imported , qconfig_spec = { torch . nn . Linear }, dtype = torch . qint8 ) self . model = imported self . audio_transform = module . audio_transform self . text_transform = module . text_transform","title":"__init__()"},{"location":"api/Wav2Vec/module/#thunder.wav2vec.module.Wav2Vec2Scriptable.forward","text":"Process the audio tensor to create the probabilities. Parameters: Name Type Description Default audio Tensor Audio tensor of shape [batch_size, time] required Returns: Type Description Tensor Tensor with the prediction probabilities. Source code in thunder/wav2vec/module.py def forward ( self , audio : Tensor ) -> Tensor : \"\"\"Process the audio tensor to create the probabilities. Args: audio : Audio tensor of shape [batch_size, time] Returns: Tensor with the prediction probabilities. \"\"\" features = self . audio_transform ( audio ) outputs = self . model ( features ) probs = outputs [ 0 ] # Change from (batch, time, #vocab) to (batch, #vocab, time) # that is expected by the rest of the library return probs . permute ( 0 , 2 , 1 )","title":"forward()"},{"location":"api/Wav2Vec/module/#thunder.wav2vec.module.Wav2Vec2Scriptable.predict","text":"Use this function during inference to predict. Parameters: Name Type Description Default x Tensor Audio tensor of shape [batch_size, time] required Returns: Type Description List[str] A list of strings, each one contains the corresponding transcription to the original batch element. Source code in thunder/wav2vec/module.py @torch . jit . export def predict ( self , x : Tensor ) -> List [ str ]: \"\"\"Use this function during inference to predict. Args: x : Audio tensor of shape [batch_size, time] Returns: A list of strings, each one contains the corresponding transcription to the original batch element. \"\"\" pred = self ( x ) return self . text_transform . decode_prediction ( pred . argmax ( 1 ))","title":"predict()"},{"location":"api/Wav2Vec/transform/","text":"Wav2Vec2Preprocess __init__ ( self , div_guard = 1e-05 ) special Wav2Vec model preprocessing. It only consists of normalizing the audio. Parameters: Name Type Description Default div_guard float Guard value to prevent division by zero. 1e-05 Source code in thunder/wav2vec/transform.py def __init__ ( self , div_guard : float = 1e-5 ): \"\"\"Wav2Vec model preprocessing. It only consists of normalizing the audio. Args: div_guard : Guard value to prevent division by zero. \"\"\" super () . __init__ () self . div_guard = div_guard forward ( self , audio ) Applies the normalization Parameters: Name Type Description Default audio Tensor Audio tensor of shape [batch_size, time] required Returns: Type Description Tensor Normalized audio tensor with same shape as input Source code in thunder/wav2vec/transform.py def forward ( self , audio : torch . Tensor ) -> torch . Tensor : \"\"\"Applies the normalization Args: audio : Audio tensor of shape [batch_size, time] Returns: Normalized audio tensor with same shape as input \"\"\" mean = audio . mean ( 1 , keepdim = True ) . detach () std = ( audio . var ( 1 , keepdim = True ) . detach () + self . div_guard ) . sqrt () return ( audio - mean ) / std","title":"Transform"},{"location":"api/Wav2Vec/transform/#thunder.wav2vec.transform.Wav2Vec2Preprocess","text":"","title":"Wav2Vec2Preprocess"},{"location":"api/Wav2Vec/transform/#thunder.wav2vec.transform.Wav2Vec2Preprocess.__init__","text":"Wav2Vec model preprocessing. It only consists of normalizing the audio. Parameters: Name Type Description Default div_guard float Guard value to prevent division by zero. 1e-05 Source code in thunder/wav2vec/transform.py def __init__ ( self , div_guard : float = 1e-5 ): \"\"\"Wav2Vec model preprocessing. It only consists of normalizing the audio. Args: div_guard : Guard value to prevent division by zero. \"\"\" super () . __init__ () self . div_guard = div_guard","title":"__init__()"},{"location":"api/Wav2Vec/transform/#thunder.wav2vec.transform.Wav2Vec2Preprocess.forward","text":"Applies the normalization Parameters: Name Type Description Default audio Tensor Audio tensor of shape [batch_size, time] required Returns: Type Description Tensor Normalized audio tensor with same shape as input Source code in thunder/wav2vec/transform.py def forward ( self , audio : torch . Tensor ) -> torch . Tensor : \"\"\"Applies the normalization Args: audio : Audio tensor of shape [batch_size, time] Returns: Normalized audio tensor with same shape as input \"\"\" mean = audio . mean ( 1 , keepdim = True ) . detach () std = ( audio . var ( 1 , keepdim = True ) . detach () + self . div_guard ) . sqrt () return ( audio - mean ) / std","title":"forward()"}]}