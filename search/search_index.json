{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Thunder speech A Hackable speech recognition library. Note This project has been set up using PyScaffold 3.3. For details and usage information on PyScaffold see https://pyscaffold.org/.","title":"Home"},{"location":"#thunder-speech","text":"A Hackable speech recognition library.","title":"Thunder speech"},{"location":"#note","text":"This project has been set up using PyScaffold 3.3. For details and usage information on PyScaffold see https://pyscaffold.org/.","title":"Note"},{"location":"quick%20reference%20guide/","text":"Quick reference guide How to export a Quartznet .nemo file to a pure pytorch model? from thunder.quartznet.module import QuartznetModule module = QuartznetModule . load_from_nemo ( nemo_filepath = \"/path/to/checkpoint.nemo\" ) module . to_torchscript ( \"model_ready_for_inference.pt\" ) How to run inference on that exported file? import torch import torchaudio model = torch . jit . load ( \"model_ready_for_inference.pt\" ) audio , sr = torchaudio . load ( \"audio_file.wav\" ) # Optionally resample if sr is different from original model sample rate # tfm = torchaudio.transforms.Resample(orig_freq=sr, new_freq=16000) # audio = tfm(audio) transcriptions = model . predict ( audio ) # transcriptions is a list of strings with the captions. Note The exported model only depends on pytorch and torchaudio, and the later is only used to open the audio file into a tensor. If torchaudio.load could be compiled inside the model in the future, similar to what already happens with torchvision, the dependency can be removed and only the base pytorch will be necessary to run inferece! What if I want the probabilities instead of the captions? Instead of model.predict(audio) , use just model(audio) import torch import torchaudio model = torch . jit . load ( \"model_ready_for_inference.pt\" ) audio , sr = torchaudio . load ( audio_name ) probs = model ( audio ) # If you also want the transcriptions: transcriptions = model . text_pipeline . decode_prediction ( probs . argmax ( 1 )) How to finetune a model if I already have the nemo manifests prepared? import pytorch_lightning as pl from thunder.data.datamodule import ManifestDatamodule from thunder.quartznet.module import QuartznetModule dm = ManifestDatamodule ( train_manifest = \"/path/to/train_manifest.json\" , val_manifest = \"/path/to/val_manifest.json\" , test_manifest = \"/path/to/test_manifest.json\" , ) model = QuartznetModule . load_from_nemo ( checkpoint_name = \"QuartzNet5x5LS-En\" ) trainer = pl . Trainer ( gpus =- 1 , # Use all gpus max_epochs = 10 , ) trainer . fit ( model = model , datamodule = dm ) Danger This will probably have a subpar result right now, as I'm still working on properly fine tuning (freeze encoder at the start, learning rate scheduling, better defaults)","title":"Quick reference guide"},{"location":"quick%20reference%20guide/#quick-reference-guide","text":"","title":"Quick reference guide"},{"location":"quick%20reference%20guide/#how-to-export-a-quartznet-nemo-file-to-a-pure-pytorch-model","text":"from thunder.quartznet.module import QuartznetModule module = QuartznetModule . load_from_nemo ( nemo_filepath = \"/path/to/checkpoint.nemo\" ) module . to_torchscript ( \"model_ready_for_inference.pt\" )","title":"How to export a Quartznet .nemo file to a pure pytorch model?"},{"location":"quick%20reference%20guide/#how-to-run-inference-on-that-exported-file","text":"import torch import torchaudio model = torch . jit . load ( \"model_ready_for_inference.pt\" ) audio , sr = torchaudio . load ( \"audio_file.wav\" ) # Optionally resample if sr is different from original model sample rate # tfm = torchaudio.transforms.Resample(orig_freq=sr, new_freq=16000) # audio = tfm(audio) transcriptions = model . predict ( audio ) # transcriptions is a list of strings with the captions. Note The exported model only depends on pytorch and torchaudio, and the later is only used to open the audio file into a tensor. If torchaudio.load could be compiled inside the model in the future, similar to what already happens with torchvision, the dependency can be removed and only the base pytorch will be necessary to run inferece!","title":"How to run inference on that exported file?"},{"location":"quick%20reference%20guide/#what-if-i-want-the-probabilities-instead-of-the-captions","text":"Instead of model.predict(audio) , use just model(audio) import torch import torchaudio model = torch . jit . load ( \"model_ready_for_inference.pt\" ) audio , sr = torchaudio . load ( audio_name ) probs = model ( audio ) # If you also want the transcriptions: transcriptions = model . text_pipeline . decode_prediction ( probs . argmax ( 1 ))","title":"What if I want the probabilities instead of the captions?"},{"location":"quick%20reference%20guide/#how-to-finetune-a-model-if-i-already-have-the-nemo-manifests-prepared","text":"import pytorch_lightning as pl from thunder.data.datamodule import ManifestDatamodule from thunder.quartznet.module import QuartznetModule dm = ManifestDatamodule ( train_manifest = \"/path/to/train_manifest.json\" , val_manifest = \"/path/to/val_manifest.json\" , test_manifest = \"/path/to/test_manifest.json\" , ) model = QuartznetModule . load_from_nemo ( checkpoint_name = \"QuartzNet5x5LS-En\" ) trainer = pl . Trainer ( gpus =- 1 , # Use all gpus max_epochs = 10 , ) trainer . fit ( model = model , datamodule = dm ) Danger This will probably have a subpar result right now, as I'm still working on properly fine tuning (freeze encoder at the start, learning rate scheduling, better defaults)","title":"How to finetune a model if I already have the nemo manifests prepared?"},{"location":"api/librosa%20compatibility/","text":"create_fb_matrix ( n_freqs , f_min , f_max , n_mels , sample_rate , norm = None , htk = True ) Create a frequency bin conversion matrix. This is a direct modification of torchaudio.functional.create_fb_matrix to also create the frequencies using the same formula as librosa Parameters: Name Type Description Default n_freqs int Number of frequencies to highlight/apply required f_min float Minimum frequency (Hz) required f_max float Maximum frequency (Hz) required n_mels int Number of mel filterbanks required sample_rate int Sample rate of the audio waveform required norm Optional[str] If 'slaney', divide the triangular mel weights by the width of the mel band (area normalization). None htk bool Use htk formula for mel scale or not. True Returns: Type Description Tensor Triangular filter banks (fb matrix) of size ( n_freqs , n_mels ) meaning number of frequencies to highlight/apply to x the number of filterbanks. Each column is a filterbank so that assuming there is a matrix A of size (..., n_freqs ), the applied result would be A * create_fb_matrix(A.size(-1), ...) . Source code in thunder/librosa_compat.py def create_fb_matrix ( n_freqs : int , f_min : float , f_max : float , n_mels : int , sample_rate : int , norm : Optional [ str ] = None , htk : bool = True , ) -> Tensor : \"\"\"Create a frequency bin conversion matrix. This is a direct modification of torchaudio.functional.create_fb_matrix to also create the frequencies using the same formula as librosa Args: n_freqs : Number of frequencies to highlight/apply f_min : Minimum frequency (Hz) f_max : Maximum frequency (Hz) n_mels : Number of mel filterbanks sample_rate : Sample rate of the audio waveform norm : If 'slaney', divide the triangular mel weights by the width of the mel band (area normalization). htk : Use htk formula for mel scale or not. Returns: Triangular filter banks (fb matrix) of size (``n_freqs``, ``n_mels``) meaning number of frequencies to highlight/apply to x the number of filterbanks. Each column is a filterbank so that assuming there is a matrix A of size (..., ``n_freqs``), the applied result would be ``A * create_fb_matrix(A.size(-1), ...)``. \"\"\" if norm is not None and norm != \"slaney\" : raise ValueError ( \"norm must be one of None or 'slaney'\" ) # freq bins # Equivalent filterbank construction by Librosa all_freqs = torch . linspace ( 0 , sample_rate // 2 , n_freqs ) f_pts = mel_frequencies ( f_min , f_max , n_mels , htk = htk ) # calculate the difference between each mel point and each stft freq point in hertz f_diff = f_pts [ 1 :] - f_pts [: - 1 ] # (n_mels + 1) slopes = f_pts . unsqueeze ( 0 ) - all_freqs . unsqueeze ( 1 ) # (n_freqs, n_mels + 2) # create overlapping triangles zero = torch . zeros ( 1 ) down_slopes = ( - 1.0 * slopes [:, : - 2 ]) / f_diff [: - 1 ] # (n_freqs, n_mels) up_slopes = slopes [:, 2 :] / f_diff [ 1 :] # (n_freqs, n_mels) fb = torch . max ( zero , torch . min ( down_slopes , up_slopes )) if norm is not None and norm == \"slaney\" : # Slaney-style mel is scaled to be approx constant energy per channel enorm = 2.0 / ( f_pts [ 2 : n_mels + 2 ] - f_pts [: n_mels ]) fb *= enorm . unsqueeze ( 0 ) if ( fb . max ( dim = 0 ) . values == 0.0 ) . any (): warnings . warn ( \"At least one mel filterbank has all zero values. \" f \"The value for `n_mels` ( { n_mels } ) may be set too high. \" f \"Or, the value for `n_freqs` ( { n_freqs } ) may be set too low.\" ) return fb hz_to_mel ( frequencies , htk = False ) This is a direct port of librosa.core.conver.hz_to_mel to work with torchaudio. Parameters: Name Type Description Default frequencies int Frequencies to convert required htk bool Use htk formula for conversion or not. Defaults to False. False Returns: Type Description int Frequencies in mel scale. Source code in thunder/librosa_compat.py def hz_to_mel ( frequencies : int , htk : bool = False ) -> int : \"\"\"This is a direct port of librosa.core.conver.hz_to_mel to work with torchaudio. Args: frequencies : Frequencies to convert htk : Use htk formula for conversion or not. Defaults to False. Returns: Frequencies in mel scale. \"\"\" if htk : return 2595.0 * math . log10 ( 1.0 + frequencies / 700.0 ) # Fill in the linear part f_min = 0.0 f_sp = 200.0 / 3 mels = ( frequencies - f_min ) / f_sp # Fill in the log-scale part min_log_hz = 1000.0 # beginning of log region (Hz) min_log_mel = ( min_log_hz - f_min ) / f_sp # same (Mels) logstep = math . log ( 6.4 ) / 27.0 # step size for log region if frequencies >= min_log_hz : # If we have scalar data, heck directly mels = min_log_mel + math . log ( frequencies / min_log_hz ) / logstep return mels mel_frequencies ( f_min , f_max , n_mels , htk ) Calculates the frequencies to create the mel scale filterbanks. This is a direct port of librosa.filters.mel_frequencies to work with pytorch. Parameters: Name Type Description Default f_min int Minimum frequency required f_max int Maximum frequency required n_mels int Number of mels required htk bool Use htk formula for mel scale or not required Returns: Type Description Tensor Tensor containing the corresponding frequencies. Source code in thunder/librosa_compat.py def mel_frequencies ( f_min : int , f_max : int , n_mels : int , htk : bool ) -> Tensor : \"\"\"Calculates the frequencies to create the mel scale filterbanks. This is a direct port of librosa.filters.mel_frequencies to work with pytorch. Args: f_min : Minimum frequency f_max : Maximum frequency n_mels : Number of mels htk : Use htk formula for mel scale or not Returns: Tensor containing the corresponding frequencies. \"\"\" m_min = hz_to_mel ( f_min , htk = htk ) m_max = hz_to_mel ( f_max , htk = htk ) m_pts = torch . linspace ( m_min , m_max , n_mels + 2 ) f_pts = mel_to_hz ( m_pts , htk = htk ) return f_pts mel_to_hz ( mels , htk = False ) This is a direct port of librosa.core.conver.mel_to_hz to work with torchaudio. Parameters: Name Type Description Default mels Tensor Frequencies to convert, in mel scale required htk bool Use htk formula for conversion or not. Defaults to False. False Returns: Type Description Tensor Frequencies in hertz. Source code in thunder/librosa_compat.py def mel_to_hz ( mels : Tensor , htk : bool = False ) -> Tensor : \"\"\"This is a direct port of librosa.core.conver.mel_to_hz to work with torchaudio. Args: mels : Frequencies to convert, in mel scale htk : Use htk formula for conversion or not. Defaults to False. Returns: Frequencies in hertz. \"\"\" if htk : return 700.0 * ( 10.0 ** ( mels / 2595.0 ) - 1.0 ) # Fill in the linear scale f_min = 0.0 f_sp = 200.0 / 3 freqs = f_min + f_sp * mels # And now the nonlinear scale min_log_hz = 1000.0 # beginning of log region (Hz) min_log_mel = ( min_log_hz - f_min ) / f_sp # same (Mels) logstep = math . log ( 6.4 ) / 27.0 # step size for log region log_t = mels >= min_log_mel freqs [ log_t ] = min_log_hz * ( logstep * ( mels [ log_t ] - min_log_mel )) . exp () return freqs","title":"Librosa compatibility"},{"location":"api/librosa%20compatibility/#thunder.librosa_compat.create_fb_matrix","text":"Create a frequency bin conversion matrix. This is a direct modification of torchaudio.functional.create_fb_matrix to also create the frequencies using the same formula as librosa Parameters: Name Type Description Default n_freqs int Number of frequencies to highlight/apply required f_min float Minimum frequency (Hz) required f_max float Maximum frequency (Hz) required n_mels int Number of mel filterbanks required sample_rate int Sample rate of the audio waveform required norm Optional[str] If 'slaney', divide the triangular mel weights by the width of the mel band (area normalization). None htk bool Use htk formula for mel scale or not. True Returns: Type Description Tensor Triangular filter banks (fb matrix) of size ( n_freqs , n_mels ) meaning number of frequencies to highlight/apply to x the number of filterbanks. Each column is a filterbank so that assuming there is a matrix A of size (..., n_freqs ), the applied result would be A * create_fb_matrix(A.size(-1), ...) . Source code in thunder/librosa_compat.py def create_fb_matrix ( n_freqs : int , f_min : float , f_max : float , n_mels : int , sample_rate : int , norm : Optional [ str ] = None , htk : bool = True , ) -> Tensor : \"\"\"Create a frequency bin conversion matrix. This is a direct modification of torchaudio.functional.create_fb_matrix to also create the frequencies using the same formula as librosa Args: n_freqs : Number of frequencies to highlight/apply f_min : Minimum frequency (Hz) f_max : Maximum frequency (Hz) n_mels : Number of mel filterbanks sample_rate : Sample rate of the audio waveform norm : If 'slaney', divide the triangular mel weights by the width of the mel band (area normalization). htk : Use htk formula for mel scale or not. Returns: Triangular filter banks (fb matrix) of size (``n_freqs``, ``n_mels``) meaning number of frequencies to highlight/apply to x the number of filterbanks. Each column is a filterbank so that assuming there is a matrix A of size (..., ``n_freqs``), the applied result would be ``A * create_fb_matrix(A.size(-1), ...)``. \"\"\" if norm is not None and norm != \"slaney\" : raise ValueError ( \"norm must be one of None or 'slaney'\" ) # freq bins # Equivalent filterbank construction by Librosa all_freqs = torch . linspace ( 0 , sample_rate // 2 , n_freqs ) f_pts = mel_frequencies ( f_min , f_max , n_mels , htk = htk ) # calculate the difference between each mel point and each stft freq point in hertz f_diff = f_pts [ 1 :] - f_pts [: - 1 ] # (n_mels + 1) slopes = f_pts . unsqueeze ( 0 ) - all_freqs . unsqueeze ( 1 ) # (n_freqs, n_mels + 2) # create overlapping triangles zero = torch . zeros ( 1 ) down_slopes = ( - 1.0 * slopes [:, : - 2 ]) / f_diff [: - 1 ] # (n_freqs, n_mels) up_slopes = slopes [:, 2 :] / f_diff [ 1 :] # (n_freqs, n_mels) fb = torch . max ( zero , torch . min ( down_slopes , up_slopes )) if norm is not None and norm == \"slaney\" : # Slaney-style mel is scaled to be approx constant energy per channel enorm = 2.0 / ( f_pts [ 2 : n_mels + 2 ] - f_pts [: n_mels ]) fb *= enorm . unsqueeze ( 0 ) if ( fb . max ( dim = 0 ) . values == 0.0 ) . any (): warnings . warn ( \"At least one mel filterbank has all zero values. \" f \"The value for `n_mels` ( { n_mels } ) may be set too high. \" f \"Or, the value for `n_freqs` ( { n_freqs } ) may be set too low.\" ) return fb","title":"create_fb_matrix()"},{"location":"api/librosa%20compatibility/#thunder.librosa_compat.hz_to_mel","text":"This is a direct port of librosa.core.conver.hz_to_mel to work with torchaudio. Parameters: Name Type Description Default frequencies int Frequencies to convert required htk bool Use htk formula for conversion or not. Defaults to False. False Returns: Type Description int Frequencies in mel scale. Source code in thunder/librosa_compat.py def hz_to_mel ( frequencies : int , htk : bool = False ) -> int : \"\"\"This is a direct port of librosa.core.conver.hz_to_mel to work with torchaudio. Args: frequencies : Frequencies to convert htk : Use htk formula for conversion or not. Defaults to False. Returns: Frequencies in mel scale. \"\"\" if htk : return 2595.0 * math . log10 ( 1.0 + frequencies / 700.0 ) # Fill in the linear part f_min = 0.0 f_sp = 200.0 / 3 mels = ( frequencies - f_min ) / f_sp # Fill in the log-scale part min_log_hz = 1000.0 # beginning of log region (Hz) min_log_mel = ( min_log_hz - f_min ) / f_sp # same (Mels) logstep = math . log ( 6.4 ) / 27.0 # step size for log region if frequencies >= min_log_hz : # If we have scalar data, heck directly mels = min_log_mel + math . log ( frequencies / min_log_hz ) / logstep return mels","title":"hz_to_mel()"},{"location":"api/librosa%20compatibility/#thunder.librosa_compat.mel_frequencies","text":"Calculates the frequencies to create the mel scale filterbanks. This is a direct port of librosa.filters.mel_frequencies to work with pytorch. Parameters: Name Type Description Default f_min int Minimum frequency required f_max int Maximum frequency required n_mels int Number of mels required htk bool Use htk formula for mel scale or not required Returns: Type Description Tensor Tensor containing the corresponding frequencies. Source code in thunder/librosa_compat.py def mel_frequencies ( f_min : int , f_max : int , n_mels : int , htk : bool ) -> Tensor : \"\"\"Calculates the frequencies to create the mel scale filterbanks. This is a direct port of librosa.filters.mel_frequencies to work with pytorch. Args: f_min : Minimum frequency f_max : Maximum frequency n_mels : Number of mels htk : Use htk formula for mel scale or not Returns: Tensor containing the corresponding frequencies. \"\"\" m_min = hz_to_mel ( f_min , htk = htk ) m_max = hz_to_mel ( f_max , htk = htk ) m_pts = torch . linspace ( m_min , m_max , n_mels + 2 ) f_pts = mel_to_hz ( m_pts , htk = htk ) return f_pts","title":"mel_frequencies()"},{"location":"api/librosa%20compatibility/#thunder.librosa_compat.mel_to_hz","text":"This is a direct port of librosa.core.conver.mel_to_hz to work with torchaudio. Parameters: Name Type Description Default mels Tensor Frequencies to convert, in mel scale required htk bool Use htk formula for conversion or not. Defaults to False. False Returns: Type Description Tensor Frequencies in hertz. Source code in thunder/librosa_compat.py def mel_to_hz ( mels : Tensor , htk : bool = False ) -> Tensor : \"\"\"This is a direct port of librosa.core.conver.mel_to_hz to work with torchaudio. Args: mels : Frequencies to convert, in mel scale htk : Use htk formula for conversion or not. Defaults to False. Returns: Frequencies in hertz. \"\"\" if htk : return 700.0 * ( 10.0 ** ( mels / 2595.0 ) - 1.0 ) # Fill in the linear scale f_min = 0.0 f_sp = 200.0 / 3 freqs = f_min + f_sp * mels # And now the nonlinear scale min_log_hz = 1000.0 # beginning of log region (Hz) min_log_mel = ( min_log_hz - f_min ) / f_sp # same (Mels) logstep = math . log ( 6.4 ) / 27.0 # step size for log region log_t = mels >= min_log_mel freqs [ log_t ] = min_log_hz * ( logstep * ( mels [ log_t ] - min_log_mel )) . exp () return freqs","title":"mel_to_hz()"},{"location":"api/metrics/","text":"CER Metric to compute the character error rate of predictions during the training loop. Accepts lists of predictions and references, correctly accumulating the metrics and computing the final value when requested. Check EditBaseMetric for more details on the possible methods. update_func ( self , predicted , reference ) Compute the statistics used to compare two strings using character error rate. Parameters: Name Type Description Default predicted str Single model prediction required reference str Corresponding reference required Returns: Type Description Tuple[int, int] Tuple containing the pure edit distance between predicted and reference, and the normalizing factor. Source code in thunder/metrics.py def update_func ( self , predicted : str , reference : str ) -> Tuple [ int , int ]: \"\"\"Compute the statistics used to compare two strings using character error rate. Args: predicted : Single model prediction reference : Corresponding reference Returns: Tuple containing the pure edit distance between predicted and reference, and the normalizing factor. \"\"\" return _cer_update ( predicted , reference ) EditBaseMetric __init__ ( self , compute_on_step = True , dist_sync_on_step = False , process_group = None , dist_sync_fn = None ) special Base metric for computations based on edit distance. Parameters: Name Type Description Default compute_on_step bool Forward only calls update() and returns None if this is set to False. True dist_sync_on_step bool Synchronize metric state across processes at each forward() before returning the value at the step. False process_group Optional[Any] Specify the process group on which synchronization is called. default: None (which selects the entire world) None dist_sync_fn Callable Callback that performs the allgather operation on the metric state. When None, DDP will be used to perform the allgather. None Source code in thunder/metrics.py def __init__ ( self , compute_on_step : bool = True , dist_sync_on_step : bool = False , process_group : Optional [ Any ] = None , dist_sync_fn : Callable = None , ): \"\"\"Base metric for computations based on edit distance. Args: compute_on_step: Forward only calls update() and returns None if this is set to False. dist_sync_on_step: Synchronize metric state across processes at each forward() before returning the value at the step. process_group: Specify the process group on which synchronization is called. default: None (which selects the entire world) dist_sync_fn: Callback that performs the allgather operation on the metric state. When None, DDP will be used to perform the allgather. \"\"\" super () . __init__ ( compute_on_step = compute_on_step , dist_sync_on_step = dist_sync_on_step , process_group = process_group , dist_sync_fn = dist_sync_fn , ) self . add_state ( \"distance\" , default = tensor ( 0 ), dist_reduce_fx = \"sum\" ) self . add_state ( \"total\" , default = tensor ( 0 ), dist_reduce_fx = \"sum\" ) compute ( self ) Uses the aggregated counters to calculate the final metric value. Returns: Type Description Tensor Float tensor between 0.0 and 1.0 representing the error rate. Source code in thunder/metrics.py def compute ( self ) -> Tensor : \"\"\"Uses the aggregated counters to calculate the final metric value. Returns: Float tensor between 0.0 and 1.0 representing the error rate. \"\"\" return tensor ( _edit_compute ( self . distance , self . total )) update ( self , preds , target ) Method used to update the internal counters of the metric at every batch. Subclasses should leave this function untouched and implement update_func instead. Parameters: Name Type Description Default preds List[str] List of predictions of the model, already decoded into string form. required target List[str] List of corresponding references. required Source code in thunder/metrics.py def update ( self , preds : List [ str ], target : List [ str ]): \"\"\"Method used to update the internal counters of the metric at every batch. Subclasses should leave this function untouched and implement update_func instead. Args: preds : List of predictions of the model, already decoded into string form. target : List of corresponding references. \"\"\" # fmt: off assert len ( target ) > 0 , \"You need to pass at least one pair\" assert len ( preds ) == len ( target ), \"The number of predictions and targets must be the same\" # fmt: on for predicted , reference in zip ( preds , target ): distance , total = self . update_func ( predicted , reference ) self . distance += distance self . total += total update_func ( self , predicted , reference ) Function to calculate the statistics from one pair of elements. This function should take the two strings, split them according to characters/words/phonemes based on the metric implemented, calculate the edit distance between the two splitted strings and return also the normalizing factor, that is the number of elements in the splitted reference. Parameters: Name Type Description Default predicted str Single model prediction required reference str Corresponding reference required Returns: Type Description Tuple[int, int] Tuple containing the pure edit distance between predicted and reference, and the normalizing factor. Source code in thunder/metrics.py def update_func ( self , predicted : str , reference : str ) -> Tuple [ int , int ]: \"\"\"Function to calculate the statistics from one pair of elements. This function should take the two strings, split them according to characters/words/phonemes based on the metric implemented, calculate the edit distance between the two splitted strings and return also the normalizing factor, that is the number of elements in the splitted reference. Args: predicted : Single model prediction reference : Corresponding reference Returns: Tuple containing the pure edit distance between predicted and reference, and the normalizing factor. \"\"\" pass WER Metric to compute the word error rate of predictions during the training loop. Accepts lists of predictions and references, correctly accumulating the metrics and computing the final value when requested. Check EditBaseMetric for more details on the possible methods. update_func ( self , predicted , reference ) Compute the statistics used to compare two strings using word error rate. Parameters: Name Type Description Default predicted str Single model prediction required reference str Corresponding reference required Returns: Type Description Tuple[int, int] Tuple containing the pure edit distance between predicted and reference, and the normalizing factor. Source code in thunder/metrics.py def update_func ( self , predicted : str , reference : str ) -> Tuple [ int , int ]: \"\"\"Compute the statistics used to compare two strings using word error rate. Args: predicted : Single model prediction reference : Corresponding reference Returns: Tuple containing the pure edit distance between predicted and reference, and the normalizing factor. \"\"\" return _wer_update ( predicted , reference ) single_cer ( predicted , reference ) Computes the character error rate between one prediction and the corresponding reference. This is the functional form, and it's recommended to use inside the training loop the class CER instead. Parameters: Name Type Description Default predicted str Model prediction after decoding back to string required reference str Reference text required Returns: Type Description float Value between 0.0 and 1.0 that measures the error rate. Source code in thunder/metrics.py def single_cer ( predicted : str , reference : str ) -> float : \"\"\"Computes the character error rate between one prediction and the corresponding reference. This is the functional form, and it's recommended to use inside the training loop the class [`CER`][thunder.metrics.CER] instead. Args: predicted : Model prediction after decoding back to string reference : Reference text Returns: Value between 0.0 and 1.0 that measures the error rate. \"\"\" distance , total = _cer_update ( predicted , reference ) return _edit_compute ( distance , total ) single_wer ( predicted , reference ) Computes the word error rate between one prediction and the corresponding reference. This is the functional form, and it's recommended to use inside the training loop the class WER instead. Parameters: Name Type Description Default predicted str Model prediction after decoding back to string required reference str Reference text required Returns: Type Description float Value between 0.0 and 1.0 that measures the error rate. Source code in thunder/metrics.py def single_wer ( predicted : str , reference : str ) -> float : \"\"\"Computes the word error rate between one prediction and the corresponding reference. This is the functional form, and it's recommended to use inside the training loop the class [`WER`][thunder.metrics.WER] instead. Args: predicted : Model prediction after decoding back to string reference : Reference text Returns: Value between 0.0 and 1.0 that measures the error rate. \"\"\" distance , total = _wer_update ( predicted , reference ) return _edit_compute ( distance , total )","title":"Metrics"},{"location":"api/metrics/#thunder.metrics.CER","text":"Metric to compute the character error rate of predictions during the training loop. Accepts lists of predictions and references, correctly accumulating the metrics and computing the final value when requested. Check EditBaseMetric for more details on the possible methods.","title":"CER"},{"location":"api/metrics/#thunder.metrics.CER.update_func","text":"Compute the statistics used to compare two strings using character error rate. Parameters: Name Type Description Default predicted str Single model prediction required reference str Corresponding reference required Returns: Type Description Tuple[int, int] Tuple containing the pure edit distance between predicted and reference, and the normalizing factor. Source code in thunder/metrics.py def update_func ( self , predicted : str , reference : str ) -> Tuple [ int , int ]: \"\"\"Compute the statistics used to compare two strings using character error rate. Args: predicted : Single model prediction reference : Corresponding reference Returns: Tuple containing the pure edit distance between predicted and reference, and the normalizing factor. \"\"\" return _cer_update ( predicted , reference )","title":"update_func()"},{"location":"api/metrics/#thunder.metrics.EditBaseMetric","text":"","title":"EditBaseMetric"},{"location":"api/metrics/#thunder.metrics.EditBaseMetric.__init__","text":"Base metric for computations based on edit distance. Parameters: Name Type Description Default compute_on_step bool Forward only calls update() and returns None if this is set to False. True dist_sync_on_step bool Synchronize metric state across processes at each forward() before returning the value at the step. False process_group Optional[Any] Specify the process group on which synchronization is called. default: None (which selects the entire world) None dist_sync_fn Callable Callback that performs the allgather operation on the metric state. When None, DDP will be used to perform the allgather. None Source code in thunder/metrics.py def __init__ ( self , compute_on_step : bool = True , dist_sync_on_step : bool = False , process_group : Optional [ Any ] = None , dist_sync_fn : Callable = None , ): \"\"\"Base metric for computations based on edit distance. Args: compute_on_step: Forward only calls update() and returns None if this is set to False. dist_sync_on_step: Synchronize metric state across processes at each forward() before returning the value at the step. process_group: Specify the process group on which synchronization is called. default: None (which selects the entire world) dist_sync_fn: Callback that performs the allgather operation on the metric state. When None, DDP will be used to perform the allgather. \"\"\" super () . __init__ ( compute_on_step = compute_on_step , dist_sync_on_step = dist_sync_on_step , process_group = process_group , dist_sync_fn = dist_sync_fn , ) self . add_state ( \"distance\" , default = tensor ( 0 ), dist_reduce_fx = \"sum\" ) self . add_state ( \"total\" , default = tensor ( 0 ), dist_reduce_fx = \"sum\" )","title":"__init__()"},{"location":"api/metrics/#thunder.metrics.EditBaseMetric.compute","text":"Uses the aggregated counters to calculate the final metric value. Returns: Type Description Tensor Float tensor between 0.0 and 1.0 representing the error rate. Source code in thunder/metrics.py def compute ( self ) -> Tensor : \"\"\"Uses the aggregated counters to calculate the final metric value. Returns: Float tensor between 0.0 and 1.0 representing the error rate. \"\"\" return tensor ( _edit_compute ( self . distance , self . total ))","title":"compute()"},{"location":"api/metrics/#thunder.metrics.EditBaseMetric.update","text":"Method used to update the internal counters of the metric at every batch. Subclasses should leave this function untouched and implement update_func instead. Parameters: Name Type Description Default preds List[str] List of predictions of the model, already decoded into string form. required target List[str] List of corresponding references. required Source code in thunder/metrics.py def update ( self , preds : List [ str ], target : List [ str ]): \"\"\"Method used to update the internal counters of the metric at every batch. Subclasses should leave this function untouched and implement update_func instead. Args: preds : List of predictions of the model, already decoded into string form. target : List of corresponding references. \"\"\" # fmt: off assert len ( target ) > 0 , \"You need to pass at least one pair\" assert len ( preds ) == len ( target ), \"The number of predictions and targets must be the same\" # fmt: on for predicted , reference in zip ( preds , target ): distance , total = self . update_func ( predicted , reference ) self . distance += distance self . total += total","title":"update()"},{"location":"api/metrics/#thunder.metrics.EditBaseMetric.update_func","text":"Function to calculate the statistics from one pair of elements. This function should take the two strings, split them according to characters/words/phonemes based on the metric implemented, calculate the edit distance between the two splitted strings and return also the normalizing factor, that is the number of elements in the splitted reference. Parameters: Name Type Description Default predicted str Single model prediction required reference str Corresponding reference required Returns: Type Description Tuple[int, int] Tuple containing the pure edit distance between predicted and reference, and the normalizing factor. Source code in thunder/metrics.py def update_func ( self , predicted : str , reference : str ) -> Tuple [ int , int ]: \"\"\"Function to calculate the statistics from one pair of elements. This function should take the two strings, split them according to characters/words/phonemes based on the metric implemented, calculate the edit distance between the two splitted strings and return also the normalizing factor, that is the number of elements in the splitted reference. Args: predicted : Single model prediction reference : Corresponding reference Returns: Tuple containing the pure edit distance between predicted and reference, and the normalizing factor. \"\"\" pass","title":"update_func()"},{"location":"api/metrics/#thunder.metrics.WER","text":"Metric to compute the word error rate of predictions during the training loop. Accepts lists of predictions and references, correctly accumulating the metrics and computing the final value when requested. Check EditBaseMetric for more details on the possible methods.","title":"WER"},{"location":"api/metrics/#thunder.metrics.WER.update_func","text":"Compute the statistics used to compare two strings using word error rate. Parameters: Name Type Description Default predicted str Single model prediction required reference str Corresponding reference required Returns: Type Description Tuple[int, int] Tuple containing the pure edit distance between predicted and reference, and the normalizing factor. Source code in thunder/metrics.py def update_func ( self , predicted : str , reference : str ) -> Tuple [ int , int ]: \"\"\"Compute the statistics used to compare two strings using word error rate. Args: predicted : Single model prediction reference : Corresponding reference Returns: Tuple containing the pure edit distance between predicted and reference, and the normalizing factor. \"\"\" return _wer_update ( predicted , reference )","title":"update_func()"},{"location":"api/metrics/#thunder.metrics.single_cer","text":"Computes the character error rate between one prediction and the corresponding reference. This is the functional form, and it's recommended to use inside the training loop the class CER instead. Parameters: Name Type Description Default predicted str Model prediction after decoding back to string required reference str Reference text required Returns: Type Description float Value between 0.0 and 1.0 that measures the error rate. Source code in thunder/metrics.py def single_cer ( predicted : str , reference : str ) -> float : \"\"\"Computes the character error rate between one prediction and the corresponding reference. This is the functional form, and it's recommended to use inside the training loop the class [`CER`][thunder.metrics.CER] instead. Args: predicted : Model prediction after decoding back to string reference : Reference text Returns: Value between 0.0 and 1.0 that measures the error rate. \"\"\" distance , total = _cer_update ( predicted , reference ) return _edit_compute ( distance , total )","title":"single_cer()"},{"location":"api/metrics/#thunder.metrics.single_wer","text":"Computes the word error rate between one prediction and the corresponding reference. This is the functional form, and it's recommended to use inside the training loop the class WER instead. Parameters: Name Type Description Default predicted str Model prediction after decoding back to string required reference str Reference text required Returns: Type Description float Value between 0.0 and 1.0 that measures the error rate. Source code in thunder/metrics.py def single_wer ( predicted : str , reference : str ) -> float : \"\"\"Computes the word error rate between one prediction and the corresponding reference. This is the functional form, and it's recommended to use inside the training loop the class [`WER`][thunder.metrics.WER] instead. Args: predicted : Model prediction after decoding back to string reference : Reference text Returns: Value between 0.0 and 1.0 that measures the error rate. \"\"\" distance , total = _wer_update ( predicted , reference ) return _edit_compute ( distance , total )","title":"single_wer()"},{"location":"api/utils/","text":"audio_len ( item ) Returns the length of the audio file Parameters: Name Type Description Default item Union[pathlib.Path, str] Audio path required Returns: Type Description float Lenght in seconds of the audio Source code in thunder/utils.py def audio_len ( item : Union [ Path , str ]) -> float : \"\"\"Returns the length of the audio file Args: item : Audio path Returns: Lenght in seconds of the audio \"\"\" metadata = torchaudio . info ( item ) return metadata . num_frames / metadata . sample_rate chain_calls ( * funcs ) Chain multiple functions that take only one argument, producing a new function that is the result of calling the individual functions in sequence. Examples: f1 = lambda x : 2 * x f2 = lambda x : 3 * x f3 = lambda x : 4 * x g = chain_calls ( f1 , f2 , f3 ) assert g ( 1 ) == 24 Returns: Type Description Callable Single chained function Source code in thunder/utils.py def chain_calls ( * funcs : List [ Callable ]) -> Callable : \"\"\"Chain multiple functions that take only one argument, producing a new function that is the result of calling the individual functions in sequence. Example: ```python f1 = lambda x: 2 * x f2 = lambda x: 3 * x f3 = lambda x: 4 * x g = chain_calls(f1, f2, f3) assert g(1) == 24 ``` Returns: Single chained function \"\"\" def call ( x , f ): return f ( x ) def _inner ( arg ): return functools . reduce ( call , funcs , arg ) return _inner get_default_cache_folder () Get the default folder where the cached stuff will be saved. Returns: Type Description Path Path of the cache folder. Source code in thunder/utils.py def get_default_cache_folder () -> Path : \"\"\"Get the default folder where the cached stuff will be saved. Returns: Path of the cache folder. \"\"\" folder = Path . home () / \".thunder\" folder . mkdir ( exist_ok = True ) return folder get_files ( directory , extension ) Find all files in directory with extension. Parameters: Name Type Description Default directory Union[str, pathlib.Path] Directory to recursively find the files required extension str File extension to search for required Returns: Type Description List[pathlib.Path] List of all the files that match the extension Source code in thunder/utils.py def get_files ( directory : Union [ str , Path ], extension : str ) -> List [ Path ]: \"\"\"Find all files in directory with extension. Args: directory : Directory to recursively find the files extension : File extension to search for Returns: List of all the files that match the extension \"\"\" files_found = [] for root , _ , files in os . walk ( directory , followlinks = True ): files_found += [ Path ( root ) / f for f in files if f . endswith ( extension )] return files_found","title":"Utils"},{"location":"api/utils/#thunder.utils.audio_len","text":"Returns the length of the audio file Parameters: Name Type Description Default item Union[pathlib.Path, str] Audio path required Returns: Type Description float Lenght in seconds of the audio Source code in thunder/utils.py def audio_len ( item : Union [ Path , str ]) -> float : \"\"\"Returns the length of the audio file Args: item : Audio path Returns: Lenght in seconds of the audio \"\"\" metadata = torchaudio . info ( item ) return metadata . num_frames / metadata . sample_rate","title":"audio_len()"},{"location":"api/utils/#thunder.utils.chain_calls","text":"Chain multiple functions that take only one argument, producing a new function that is the result of calling the individual functions in sequence. Examples: f1 = lambda x : 2 * x f2 = lambda x : 3 * x f3 = lambda x : 4 * x g = chain_calls ( f1 , f2 , f3 ) assert g ( 1 ) == 24 Returns: Type Description Callable Single chained function Source code in thunder/utils.py def chain_calls ( * funcs : List [ Callable ]) -> Callable : \"\"\"Chain multiple functions that take only one argument, producing a new function that is the result of calling the individual functions in sequence. Example: ```python f1 = lambda x: 2 * x f2 = lambda x: 3 * x f3 = lambda x: 4 * x g = chain_calls(f1, f2, f3) assert g(1) == 24 ``` Returns: Single chained function \"\"\" def call ( x , f ): return f ( x ) def _inner ( arg ): return functools . reduce ( call , funcs , arg ) return _inner","title":"chain_calls()"},{"location":"api/utils/#thunder.utils.get_default_cache_folder","text":"Get the default folder where the cached stuff will be saved. Returns: Type Description Path Path of the cache folder. Source code in thunder/utils.py def get_default_cache_folder () -> Path : \"\"\"Get the default folder where the cached stuff will be saved. Returns: Path of the cache folder. \"\"\" folder = Path . home () / \".thunder\" folder . mkdir ( exist_ok = True ) return folder","title":"get_default_cache_folder()"},{"location":"api/utils/#thunder.utils.get_files","text":"Find all files in directory with extension. Parameters: Name Type Description Default directory Union[str, pathlib.Path] Directory to recursively find the files required extension str File extension to search for required Returns: Type Description List[pathlib.Path] List of all the files that match the extension Source code in thunder/utils.py def get_files ( directory : Union [ str , Path ], extension : str ) -> List [ Path ]: \"\"\"Find all files in directory with extension. Args: directory : Directory to recursively find the files extension : File extension to search for Returns: List of all the files that match the extension \"\"\" files_found = [] for root , _ , files in os . walk ( directory , followlinks = True ): files_found += [ Path ( root ) / f for f in files if f . endswith ( extension )] return files_found","title":"get_files()"},{"location":"api/Data/dataloader%20utils/","text":"Helper functions used by the speech dataloaders. BucketingSampler __init__ ( self , data_source , batch_size = 1 ) special Samples batches assuming they are in order of size to batch similarly sized samples together Parameters: Name Type Description Default data_source Iterable Source of elements already sorted by length. required batch_size int Number of elements to batch. 1 Source code in thunder/data/dataloader_utils.py def __init__ ( self , data_source : Iterable , batch_size : int = 1 ): \"\"\"Samples batches assuming they are in order of size to batch similarly sized samples together Args: data_source : Source of elements already sorted by length. batch_size : Number of elements to batch. \"\"\" super () . __init__ ( data_source ) self . data_source = data_source ids = list ( range ( 0 , len ( data_source ))) self . bins = [ ids [ i : i + batch_size ] for i in range ( 0 , len ( ids ), batch_size )] asr_collate ( samples ) Function that collect samples and adds padding. Parameters: Name Type Description Default samples List[Tuple[torch.Tensor, str]] Samples produced by dataloader required Returns: Type Description Tuple[torch.Tensor, torch.Tensor, List[str]] Tuple containing padded audios, audio lengths (normalized to 0.0 <-> 1.0 range) and the list of corresponding transcriptions in that order. Source code in thunder/data/dataloader_utils.py def asr_collate ( samples : List [ Tuple [ Tensor , str ]]) -> Tuple [ Tensor , Tensor , List [ str ]]: \"\"\"Function that collect samples and adds padding. Args: samples : Samples produced by dataloader Returns: Tuple containing padded audios, audio lengths (normalized to 0.0 <-> 1.0 range) and the list of corresponding transcriptions in that order. \"\"\" samples = sorted ( samples , key = lambda sample : sample [ 0 ] . size ( - 1 ), reverse = True ) padded_audios = pad_sequence ([ s [ 0 ] . squeeze () for s in samples ], batch_first = True ) audio_lengths = Tensor ([ s [ 0 ] . size ( - 1 ) for s in samples ]) audio_lengths = audio_lengths / audio_lengths . max () # Normalize by max length texts = [ s [ 1 ] for s in samples ] return ( padded_audios , audio_lengths , texts )","title":"Dataloader utils"},{"location":"api/Data/dataloader%20utils/#thunder.data.dataloader_utils.BucketingSampler","text":"","title":"BucketingSampler"},{"location":"api/Data/dataloader%20utils/#thunder.data.dataloader_utils.BucketingSampler.__init__","text":"Samples batches assuming they are in order of size to batch similarly sized samples together Parameters: Name Type Description Default data_source Iterable Source of elements already sorted by length. required batch_size int Number of elements to batch. 1 Source code in thunder/data/dataloader_utils.py def __init__ ( self , data_source : Iterable , batch_size : int = 1 ): \"\"\"Samples batches assuming they are in order of size to batch similarly sized samples together Args: data_source : Source of elements already sorted by length. batch_size : Number of elements to batch. \"\"\" super () . __init__ ( data_source ) self . data_source = data_source ids = list ( range ( 0 , len ( data_source ))) self . bins = [ ids [ i : i + batch_size ] for i in range ( 0 , len ( ids ), batch_size )]","title":"__init__()"},{"location":"api/Data/dataloader%20utils/#thunder.data.dataloader_utils.asr_collate","text":"Function that collect samples and adds padding. Parameters: Name Type Description Default samples List[Tuple[torch.Tensor, str]] Samples produced by dataloader required Returns: Type Description Tuple[torch.Tensor, torch.Tensor, List[str]] Tuple containing padded audios, audio lengths (normalized to 0.0 <-> 1.0 range) and the list of corresponding transcriptions in that order. Source code in thunder/data/dataloader_utils.py def asr_collate ( samples : List [ Tuple [ Tensor , str ]]) -> Tuple [ Tensor , Tensor , List [ str ]]: \"\"\"Function that collect samples and adds padding. Args: samples : Samples produced by dataloader Returns: Tuple containing padded audios, audio lengths (normalized to 0.0 <-> 1.0 range) and the list of corresponding transcriptions in that order. \"\"\" samples = sorted ( samples , key = lambda sample : sample [ 0 ] . size ( - 1 ), reverse = True ) padded_audios = pad_sequence ([ s [ 0 ] . squeeze () for s in samples ], batch_first = True ) audio_lengths = Tensor ([ s [ 0 ] . size ( - 1 ) for s in samples ]) audio_lengths = audio_lengths / audio_lengths . max () # Normalize by max length texts = [ s [ 1 ] for s in samples ] return ( padded_audios , audio_lengths , texts )","title":"asr_collate()"},{"location":"api/Data/datamodule/","text":"BaseDataModule steps_per_epoch : int property readonly Number of steps for each training epoch. Used for learning rate scheduling. Returns: Type Description int Number of steps get_dataset ( self , split ) Function to get the corresponding dataset to the specified split. This should be implemented by subclasses. Parameters: Name Type Description Default split str One of \"train\", \"valid\" or \"test\". required Returns: Type Description BaseSpeechDataset The corresponding dataset. Source code in thunder/data/datamodule.py def get_dataset ( self , split : str ) -> BaseSpeechDataset : \"\"\"Function to get the corresponding dataset to the specified split. This should be implemented by subclasses. Args: split : One of \"train\", \"valid\" or \"test\". Returns: The corresponding dataset. \"\"\" raise NotImplementedError () test_dataloader ( self ) Implement one or multiple PyTorch DataLoaders for testing. The dataloader you return will not be called every epoch unless you set :paramref: ~pytorch_lightning.trainer.Trainer.reload_dataloaders_every_epoch to True . For data processing use the following pattern: - download in :meth:`prepare_data` - process and split in :meth:`setup` However, the above are only necessary for distributed processing. .. warning:: do not assign state in prepare_data :meth: ~pytorch_lightning.trainer.Trainer.fit ... :meth: prepare_data :meth: setup :meth: train_dataloader :meth: val_dataloader :meth: test_dataloader Note Lightning adds the correct sampler for distributed and arbitrary hardware. There is no need to set it yourself. Returns: Type Description DataLoader Single or multiple PyTorch DataLoaders. Example:: def test_dataloader(self): transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (1.0,))]) dataset = MNIST(root='/path/to/mnist/', train=False, transform=transform, download=True) loader = torch.utils.data.DataLoader( dataset=dataset, batch_size=self.batch_size, shuffle=False ) return loader # can also return multiple dataloaders def test_dataloader(self): return [loader_a, loader_b, ..., loader_n] Note If you don't need a test dataset and a :meth: test_step , you don't need to implement this method. Note In the case where you return multiple test dataloaders, the :meth: test_step will have an argument dataloader_idx which matches the order here. Source code in thunder/data/datamodule.py def test_dataloader ( self ) -> DataLoader : return DataLoader ( self . test_dataset , batch_size = self . bs , shuffle = False , collate_fn = asr_collate , num_workers = self . num_workers , ) train_dataloader ( self ) Implement one or more PyTorch DataLoaders for training. Returns: Type Description DataLoader Either a single PyTorch :class: ~torch.utils.data.DataLoader or a collection of these (list, dict, nested lists and dicts). In the case of multiple dataloaders, please see this :ref: page <multiple-training-dataloaders> The dataloader you return will not be called every epoch unless you set :paramref: ~pytorch_lightning.trainer.Trainer.reload_dataloaders_every_epoch to True . For data processing use the following pattern: - download in :meth:`prepare_data` - process and split in :meth:`setup` However, the above are only necessary for distributed processing. .. warning:: do not assign state in prepare_data :meth: ~pytorch_lightning.trainer.Trainer.fit ... :meth: prepare_data :meth: setup :meth: train_dataloader Note Lightning adds the correct sampler for distributed and arbitrary hardware. There is no need to set it yourself. Example:: # single dataloader def train_dataloader(self): transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (1.0,))]) dataset = MNIST(root='/path/to/mnist/', train=True, transform=transform, download=True) loader = torch.utils.data.DataLoader( dataset=dataset, batch_size=self.batch_size, shuffle=True ) return loader # multiple dataloaders, return as list def train_dataloader(self): mnist = MNIST(...) cifar = CIFAR(...) mnist_loader = torch.utils.data.DataLoader( dataset=mnist, batch_size=self.batch_size, shuffle=True ) cifar_loader = torch.utils.data.DataLoader( dataset=cifar, batch_size=self.batch_size, shuffle=True ) # each batch will be a list of tensors: [batch_mnist, batch_cifar] return [mnist_loader, cifar_loader] # multiple dataloader, return as dict def train_dataloader(self): mnist = MNIST(...) cifar = CIFAR(...) mnist_loader = torch.utils.data.DataLoader( dataset=mnist, batch_size=self.batch_size, shuffle=True ) cifar_loader = torch.utils.data.DataLoader( dataset=cifar, batch_size=self.batch_size, shuffle=True ) # each batch will be a dict of tensors: {'mnist': batch_mnist, 'cifar': batch_cifar} return {'mnist': mnist_loader, 'cifar': cifar_loader} Source code in thunder/data/datamodule.py def train_dataloader ( self ) -> DataLoader : return DataLoader ( self . train_dataset , batch_sampler = self . sampler , collate_fn = asr_collate , num_workers = self . num_workers , ) val_dataloader ( self ) Implement one or multiple PyTorch DataLoaders for validation. The dataloader you return will not be called every epoch unless you set :paramref: ~pytorch_lightning.trainer.Trainer.reload_dataloaders_every_epoch to True . It's recommended that all data downloads and preparation happen in :meth: prepare_data . :meth: ~pytorch_lightning.trainer.Trainer.fit ... :meth: prepare_data :meth: train_dataloader :meth: val_dataloader :meth: test_dataloader Note Lightning adds the correct sampler for distributed and arbitrary hardware There is no need to set it yourself. Returns: Type Description DataLoader Single or multiple PyTorch DataLoaders. Examples:: def val_dataloader(self): transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (1.0,))]) dataset = MNIST(root='/path/to/mnist/', train=False, transform=transform, download=True) loader = torch.utils.data.DataLoader( dataset=dataset, batch_size=self.batch_size, shuffle=False ) return loader # can also return multiple dataloaders def val_dataloader(self): return [loader_a, loader_b, ..., loader_n] Note If you don't need a validation dataset and a :meth: validation_step , you don't need to implement this method. Note In the case where you return multiple validation dataloaders, the :meth: validation_step will have an argument dataloader_idx which matches the order here. Source code in thunder/data/datamodule.py def val_dataloader ( self ) -> DataLoader : return DataLoader ( self . val_dataset , batch_size = self . bs , shuffle = False , collate_fn = asr_collate , num_workers = self . num_workers , ) ManifestDatamodule get_dataset ( self , split ) Function to get the corresponding dataset to the specified split. This should be implemented by subclasses. Parameters: Name Type Description Default split str One of \"train\", \"valid\" or \"test\". required Returns: Type Description The corresponding dataset. Source code in thunder/data/datamodule.py def get_dataset ( self , split : str ): file = Path ( self . manifest_mapping [ split ]) return ManifestSpeechDataset ( file , self . force_mono , self . sr )","title":"Datamodule"},{"location":"api/Data/datamodule/#thunder.data.datamodule.BaseDataModule","text":"","title":"BaseDataModule"},{"location":"api/Data/datamodule/#thunder.data.datamodule.BaseDataModule.steps_per_epoch","text":"Number of steps for each training epoch. Used for learning rate scheduling. Returns: Type Description int Number of steps","title":"steps_per_epoch"},{"location":"api/Data/datamodule/#thunder.data.datamodule.BaseDataModule.get_dataset","text":"Function to get the corresponding dataset to the specified split. This should be implemented by subclasses. Parameters: Name Type Description Default split str One of \"train\", \"valid\" or \"test\". required Returns: Type Description BaseSpeechDataset The corresponding dataset. Source code in thunder/data/datamodule.py def get_dataset ( self , split : str ) -> BaseSpeechDataset : \"\"\"Function to get the corresponding dataset to the specified split. This should be implemented by subclasses. Args: split : One of \"train\", \"valid\" or \"test\". Returns: The corresponding dataset. \"\"\" raise NotImplementedError ()","title":"get_dataset()"},{"location":"api/Data/datamodule/#thunder.data.datamodule.BaseDataModule.test_dataloader","text":"Implement one or multiple PyTorch DataLoaders for testing. The dataloader you return will not be called every epoch unless you set :paramref: ~pytorch_lightning.trainer.Trainer.reload_dataloaders_every_epoch to True . For data processing use the following pattern: - download in :meth:`prepare_data` - process and split in :meth:`setup` However, the above are only necessary for distributed processing. .. warning:: do not assign state in prepare_data :meth: ~pytorch_lightning.trainer.Trainer.fit ... :meth: prepare_data :meth: setup :meth: train_dataloader :meth: val_dataloader :meth: test_dataloader Note Lightning adds the correct sampler for distributed and arbitrary hardware. There is no need to set it yourself. Returns: Type Description DataLoader Single or multiple PyTorch DataLoaders. Example:: def test_dataloader(self): transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (1.0,))]) dataset = MNIST(root='/path/to/mnist/', train=False, transform=transform, download=True) loader = torch.utils.data.DataLoader( dataset=dataset, batch_size=self.batch_size, shuffle=False ) return loader # can also return multiple dataloaders def test_dataloader(self): return [loader_a, loader_b, ..., loader_n] Note If you don't need a test dataset and a :meth: test_step , you don't need to implement this method. Note In the case where you return multiple test dataloaders, the :meth: test_step will have an argument dataloader_idx which matches the order here. Source code in thunder/data/datamodule.py def test_dataloader ( self ) -> DataLoader : return DataLoader ( self . test_dataset , batch_size = self . bs , shuffle = False , collate_fn = asr_collate , num_workers = self . num_workers , )","title":"test_dataloader()"},{"location":"api/Data/datamodule/#thunder.data.datamodule.BaseDataModule.train_dataloader","text":"Implement one or more PyTorch DataLoaders for training. Returns: Type Description DataLoader Either a single PyTorch :class: ~torch.utils.data.DataLoader or a collection of these (list, dict, nested lists and dicts). In the case of multiple dataloaders, please see this :ref: page <multiple-training-dataloaders> The dataloader you return will not be called every epoch unless you set :paramref: ~pytorch_lightning.trainer.Trainer.reload_dataloaders_every_epoch to True . For data processing use the following pattern: - download in :meth:`prepare_data` - process and split in :meth:`setup` However, the above are only necessary for distributed processing. .. warning:: do not assign state in prepare_data :meth: ~pytorch_lightning.trainer.Trainer.fit ... :meth: prepare_data :meth: setup :meth: train_dataloader Note Lightning adds the correct sampler for distributed and arbitrary hardware. There is no need to set it yourself. Example:: # single dataloader def train_dataloader(self): transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (1.0,))]) dataset = MNIST(root='/path/to/mnist/', train=True, transform=transform, download=True) loader = torch.utils.data.DataLoader( dataset=dataset, batch_size=self.batch_size, shuffle=True ) return loader # multiple dataloaders, return as list def train_dataloader(self): mnist = MNIST(...) cifar = CIFAR(...) mnist_loader = torch.utils.data.DataLoader( dataset=mnist, batch_size=self.batch_size, shuffle=True ) cifar_loader = torch.utils.data.DataLoader( dataset=cifar, batch_size=self.batch_size, shuffle=True ) # each batch will be a list of tensors: [batch_mnist, batch_cifar] return [mnist_loader, cifar_loader] # multiple dataloader, return as dict def train_dataloader(self): mnist = MNIST(...) cifar = CIFAR(...) mnist_loader = torch.utils.data.DataLoader( dataset=mnist, batch_size=self.batch_size, shuffle=True ) cifar_loader = torch.utils.data.DataLoader( dataset=cifar, batch_size=self.batch_size, shuffle=True ) # each batch will be a dict of tensors: {'mnist': batch_mnist, 'cifar': batch_cifar} return {'mnist': mnist_loader, 'cifar': cifar_loader} Source code in thunder/data/datamodule.py def train_dataloader ( self ) -> DataLoader : return DataLoader ( self . train_dataset , batch_sampler = self . sampler , collate_fn = asr_collate , num_workers = self . num_workers , )","title":"train_dataloader()"},{"location":"api/Data/datamodule/#thunder.data.datamodule.BaseDataModule.val_dataloader","text":"Implement one or multiple PyTorch DataLoaders for validation. The dataloader you return will not be called every epoch unless you set :paramref: ~pytorch_lightning.trainer.Trainer.reload_dataloaders_every_epoch to True . It's recommended that all data downloads and preparation happen in :meth: prepare_data . :meth: ~pytorch_lightning.trainer.Trainer.fit ... :meth: prepare_data :meth: train_dataloader :meth: val_dataloader :meth: test_dataloader Note Lightning adds the correct sampler for distributed and arbitrary hardware There is no need to set it yourself. Returns: Type Description DataLoader Single or multiple PyTorch DataLoaders. Examples:: def val_dataloader(self): transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (1.0,))]) dataset = MNIST(root='/path/to/mnist/', train=False, transform=transform, download=True) loader = torch.utils.data.DataLoader( dataset=dataset, batch_size=self.batch_size, shuffle=False ) return loader # can also return multiple dataloaders def val_dataloader(self): return [loader_a, loader_b, ..., loader_n] Note If you don't need a validation dataset and a :meth: validation_step , you don't need to implement this method. Note In the case where you return multiple validation dataloaders, the :meth: validation_step will have an argument dataloader_idx which matches the order here. Source code in thunder/data/datamodule.py def val_dataloader ( self ) -> DataLoader : return DataLoader ( self . val_dataset , batch_size = self . bs , shuffle = False , collate_fn = asr_collate , num_workers = self . num_workers , )","title":"val_dataloader()"},{"location":"api/Data/datamodule/#thunder.data.datamodule.ManifestDatamodule","text":"","title":"ManifestDatamodule"},{"location":"api/Data/datamodule/#thunder.data.datamodule.ManifestDatamodule.get_dataset","text":"Function to get the corresponding dataset to the specified split. This should be implemented by subclasses. Parameters: Name Type Description Default split str One of \"train\", \"valid\" or \"test\". required Returns: Type Description The corresponding dataset. Source code in thunder/data/datamodule.py def get_dataset ( self , split : str ): file = Path ( self . manifest_mapping [ split ]) return ManifestSpeechDataset ( file , self . force_mono , self . sr )","title":"get_dataset()"},{"location":"api/Data/dataset/","text":"BaseSpeechDataset __init__ ( self , items , force_mono = True , sr = 16000 ) special This is the base class that implements the minimal functionality to have a compatible speech dataset, in a way that can be easily customized by subclassing. Parameters: Name Type Description Default items Iterable Source of items in the dataset, sorted by audio duration. This can be a list of files, a pandas dataframe or any other iterable structure where you record your data. required force_mono bool If true, convert all the loaded samples to mono. True sr int Sample rate used by the dataset. All of the samples that have different rate will be resampled. 16000 Source code in thunder/data/dataset.py def __init__ ( self , items : Iterable , force_mono : bool = True , sr : int = 16000 ): \"\"\"This is the base class that implements the minimal functionality to have a compatible speech dataset, in a way that can be easily customized by subclassing. Args: items : Source of items in the dataset, sorted by audio duration. This can be a list of files, a pandas dataframe or any other iterable structure where you record your data. force_mono : If true, convert all the loaded samples to mono. sr : Sample rate used by the dataset. All of the samples that have different rate will be resampled. \"\"\" super () . __init__ () self . items = items self . sr = sr self . force_mono = force_mono correct_audio ( self , audio , sr ) Apply some base transforms to the audio, that fix silent problems. It transforms all the audios to mono (depending on class creation parameter) and resamples the audios to a common sample rate. Parameters: Name Type Description Default audio Tensor Audio tensor required sr int Sample rate required Returns: Type Description Tensor Audio tensor after the transforms. Source code in thunder/data/dataset.py def correct_audio ( self , audio : Tensor , sr : int ) -> Tensor : \"\"\"Apply some base transforms to the audio, that fix silent problems. It transforms all the audios to mono (depending on class creation parameter) and resamples the audios to a common sample rate. Args: audio : Audio tensor sr : Sample rate Returns: Audio tensor after the transforms. \"\"\" if self . force_mono and ( audio . shape [ 0 ] > 1 ): audio = audio . mean ( 0 , keepdim = True ) if self . sr != sr : tfm = torchaudio . transforms . Resample ( orig_freq = sr , new_freq = self . sr ) audio = tfm ( audio ) return audio get_item ( self , index ) Get the item source specified by the index. Parameters: Name Type Description Default index int Indicates what item it needs to return information about. required Returns: Type Description Any Whatever data necessary to open the audio and text corresponding to this index. Source code in thunder/data/dataset.py def get_item ( self , index : int ) -> Any : \"\"\"Get the item source specified by the index. Args: index : Indicates what item it needs to return information about. Returns: Whatever data necessary to open the audio and text corresponding to this index. \"\"\" return self . items [ index ] open_audio ( self , item ) Uses the data returned by get_item to open the audio Parameters: Name Type Description Default item Any Data returned by get_item(index) required Returns: Type Description Tuple[torch.Tensor, int] Tuple containing the audio tensor with shape (channels, time), and the corresponding sample rate. Source code in thunder/data/dataset.py def open_audio ( self , item : Any ) -> Tuple [ Tensor , int ]: \"\"\"Uses the data returned by get_item to open the audio Args: item : Data returned by get_item(index) Returns: Tuple containing the audio tensor with shape (channels, time), and the corresponding sample rate. \"\"\" return torchaudio . load ( item ) open_text ( self , item ) Opens the transcription based on the data returned by get_item(index) Parameters: Name Type Description Default item Any The data returned by get_item. required Returns: Type Description str The transcription corresponding to the item. Source code in thunder/data/dataset.py def open_text ( self , item : Any ) -> str : \"\"\"Opens the transcription based on the data returned by get_item(index) Args: item : The data returned by get_item. Returns: The transcription corresponding to the item. \"\"\" raise NotImplementedError () ManifestSpeechDataset __init__ ( self , file , force_mono , sr ) special Dataset that loads from nemo manifest files. Parameters: Name Type Description Default file Path Nemo manifest file. required force_mono bool If true, convert all the loaded samples to mono. required sr int Sample rate used by the dataset. All of the samples that have different rate will be resampled. required Source code in thunder/data/dataset.py def __init__ ( self , file : Path , force_mono : bool , sr : int ): \"\"\"Dataset that loads from nemo manifest files. Args: file : Nemo manifest file. force_mono : If true, convert all the loaded samples to mono. sr : Sample rate used by the dataset. All of the samples that have different rate will be resampled. \"\"\" # Reading from the manifest file items = [ json . loads ( line ) for line in file . read_text () . strip () . splitlines ()] # Sort items for efficient batching later. sorted_items = list ( sorted ( items , key = lambda x : x [ \"duration\" ])) super () . __init__ ( sorted_items , force_mono = force_mono , sr = sr ) open_audio ( self , item ) Uses the data returned by get_item to open the audio Parameters: Name Type Description Default item dict Data returned by get_item(index) required Returns: Type Description Tuple[torch.Tensor, int] Tuple containing the audio tensor with shape (channels, time), and the corresponding sample rate. Source code in thunder/data/dataset.py def open_audio ( self , item : dict ) -> Tuple [ Tensor , int ]: return torchaudio . load ( item [ \"audio_filepath\" ]) open_text ( self , item ) Opens the transcription based on the data returned by get_item(index) Parameters: Name Type Description Default item dict The data returned by get_item. required Returns: Type Description str The transcription corresponding to the item. Source code in thunder/data/dataset.py def open_text ( self , item : dict ) -> str : return item [ \"text\" ]","title":"Dataset"},{"location":"api/Data/dataset/#thunder.data.dataset.BaseSpeechDataset","text":"","title":"BaseSpeechDataset"},{"location":"api/Data/dataset/#thunder.data.dataset.BaseSpeechDataset.__init__","text":"This is the base class that implements the minimal functionality to have a compatible speech dataset, in a way that can be easily customized by subclassing. Parameters: Name Type Description Default items Iterable Source of items in the dataset, sorted by audio duration. This can be a list of files, a pandas dataframe or any other iterable structure where you record your data. required force_mono bool If true, convert all the loaded samples to mono. True sr int Sample rate used by the dataset. All of the samples that have different rate will be resampled. 16000 Source code in thunder/data/dataset.py def __init__ ( self , items : Iterable , force_mono : bool = True , sr : int = 16000 ): \"\"\"This is the base class that implements the minimal functionality to have a compatible speech dataset, in a way that can be easily customized by subclassing. Args: items : Source of items in the dataset, sorted by audio duration. This can be a list of files, a pandas dataframe or any other iterable structure where you record your data. force_mono : If true, convert all the loaded samples to mono. sr : Sample rate used by the dataset. All of the samples that have different rate will be resampled. \"\"\" super () . __init__ () self . items = items self . sr = sr self . force_mono = force_mono","title":"__init__()"},{"location":"api/Data/dataset/#thunder.data.dataset.BaseSpeechDataset.correct_audio","text":"Apply some base transforms to the audio, that fix silent problems. It transforms all the audios to mono (depending on class creation parameter) and resamples the audios to a common sample rate. Parameters: Name Type Description Default audio Tensor Audio tensor required sr int Sample rate required Returns: Type Description Tensor Audio tensor after the transforms. Source code in thunder/data/dataset.py def correct_audio ( self , audio : Tensor , sr : int ) -> Tensor : \"\"\"Apply some base transforms to the audio, that fix silent problems. It transforms all the audios to mono (depending on class creation parameter) and resamples the audios to a common sample rate. Args: audio : Audio tensor sr : Sample rate Returns: Audio tensor after the transforms. \"\"\" if self . force_mono and ( audio . shape [ 0 ] > 1 ): audio = audio . mean ( 0 , keepdim = True ) if self . sr != sr : tfm = torchaudio . transforms . Resample ( orig_freq = sr , new_freq = self . sr ) audio = tfm ( audio ) return audio","title":"correct_audio()"},{"location":"api/Data/dataset/#thunder.data.dataset.BaseSpeechDataset.get_item","text":"Get the item source specified by the index. Parameters: Name Type Description Default index int Indicates what item it needs to return information about. required Returns: Type Description Any Whatever data necessary to open the audio and text corresponding to this index. Source code in thunder/data/dataset.py def get_item ( self , index : int ) -> Any : \"\"\"Get the item source specified by the index. Args: index : Indicates what item it needs to return information about. Returns: Whatever data necessary to open the audio and text corresponding to this index. \"\"\" return self . items [ index ]","title":"get_item()"},{"location":"api/Data/dataset/#thunder.data.dataset.BaseSpeechDataset.open_audio","text":"Uses the data returned by get_item to open the audio Parameters: Name Type Description Default item Any Data returned by get_item(index) required Returns: Type Description Tuple[torch.Tensor, int] Tuple containing the audio tensor with shape (channels, time), and the corresponding sample rate. Source code in thunder/data/dataset.py def open_audio ( self , item : Any ) -> Tuple [ Tensor , int ]: \"\"\"Uses the data returned by get_item to open the audio Args: item : Data returned by get_item(index) Returns: Tuple containing the audio tensor with shape (channels, time), and the corresponding sample rate. \"\"\" return torchaudio . load ( item )","title":"open_audio()"},{"location":"api/Data/dataset/#thunder.data.dataset.BaseSpeechDataset.open_text","text":"Opens the transcription based on the data returned by get_item(index) Parameters: Name Type Description Default item Any The data returned by get_item. required Returns: Type Description str The transcription corresponding to the item. Source code in thunder/data/dataset.py def open_text ( self , item : Any ) -> str : \"\"\"Opens the transcription based on the data returned by get_item(index) Args: item : The data returned by get_item. Returns: The transcription corresponding to the item. \"\"\" raise NotImplementedError ()","title":"open_text()"},{"location":"api/Data/dataset/#thunder.data.dataset.ManifestSpeechDataset","text":"","title":"ManifestSpeechDataset"},{"location":"api/Data/dataset/#thunder.data.dataset.ManifestSpeechDataset.__init__","text":"Dataset that loads from nemo manifest files. Parameters: Name Type Description Default file Path Nemo manifest file. required force_mono bool If true, convert all the loaded samples to mono. required sr int Sample rate used by the dataset. All of the samples that have different rate will be resampled. required Source code in thunder/data/dataset.py def __init__ ( self , file : Path , force_mono : bool , sr : int ): \"\"\"Dataset that loads from nemo manifest files. Args: file : Nemo manifest file. force_mono : If true, convert all the loaded samples to mono. sr : Sample rate used by the dataset. All of the samples that have different rate will be resampled. \"\"\" # Reading from the manifest file items = [ json . loads ( line ) for line in file . read_text () . strip () . splitlines ()] # Sort items for efficient batching later. sorted_items = list ( sorted ( items , key = lambda x : x [ \"duration\" ])) super () . __init__ ( sorted_items , force_mono = force_mono , sr = sr )","title":"__init__()"},{"location":"api/Data/dataset/#thunder.data.dataset.ManifestSpeechDataset.open_audio","text":"Uses the data returned by get_item to open the audio Parameters: Name Type Description Default item dict Data returned by get_item(index) required Returns: Type Description Tuple[torch.Tensor, int] Tuple containing the audio tensor with shape (channels, time), and the corresponding sample rate. Source code in thunder/data/dataset.py def open_audio ( self , item : dict ) -> Tuple [ Tensor , int ]: return torchaudio . load ( item [ \"audio_filepath\" ])","title":"open_audio()"},{"location":"api/Data/dataset/#thunder.data.dataset.ManifestSpeechDataset.open_text","text":"Opens the transcription based on the data returned by get_item(index) Parameters: Name Type Description Default item dict The data returned by get_item. required Returns: Type Description str The transcription corresponding to the item. Source code in thunder/data/dataset.py def open_text ( self , item : dict ) -> str : return item [ \"text\" ]","title":"open_text()"},{"location":"api/Quartznet/blocks/","text":"Basic building blocks to create the Quartznet model InitMode Weight init methods. Used by init_weights . Note Possible values are xavier_uniform , xavier_normal , kaiming_uniform and kaiming_normal QuartznetBlock __init__ ( self , inplanes , planes , repeat = 5 , kernel_size = [ 11 ], stride = [ 1 ], dilation = [ 1 ], dropout = 0.0 , residual = True , separable = False ) special Quartznet block. This is a refactoring of the Jasperblock present on the NeMo toolkit, but simplified to only support the new quartznet model. Biggest change is that dense residual used on Jasper is not supported here, and masked convolutions were also removed. Parameters: Name Type Description Default inplanes int Number of input planes required planes int Number of output planes required repeat int Repetitions inside block. 5 kernel_size List[int] Kernel size. [11] stride List[int] Stride of each repetition. [1] dilation List[int] Dilation of each repetition. [1] dropout float Dropout used before each activation. 0.0 residual bool Controls the use of residual connection. True separable bool Controls the use of separable convolutions. False Source code in thunder/quartznet/blocks.py def __init__ ( self , inplanes : int , planes : int , repeat : int = 5 , kernel_size : List [ int ] = [ 11 ], stride : List [ int ] = [ 1 ], dilation : List [ int ] = [ 1 ], dropout : float = 0.0 , residual : bool = True , separable : bool = False , ): \"\"\"Quartznet block. This is a refactoring of the Jasperblock present on the NeMo toolkit, but simplified to only support the new quartznet model. Biggest change is that dense residual used on Jasper is not supported here, and masked convolutions were also removed. Args: inplanes : Number of input planes planes : Number of output planes repeat : Repetitions inside block. kernel_size : Kernel size. stride : Stride of each repetition. dilation : Dilation of each repetition. dropout : Dropout used before each activation. residual : Controls the use of residual connection. separable : Controls the use of separable convolutions. \"\"\" super () . __init__ () padding_val = get_same_padding ( kernel_size [ 0 ], stride [ 0 ], dilation [ 0 ]) inplanes_loop = inplanes conv = [] for _ in range ( repeat - 1 ): conv . extend ( self . _get_conv_bn_layer ( inplanes_loop , planes , kernel_size = kernel_size , stride = stride , dilation = dilation , padding = padding_val , separable = separable , bias = False , ) ) conv . extend ( self . _get_act_dropout_layer ( drop_prob = dropout )) inplanes_loop = planes conv . extend ( self . _get_conv_bn_layer ( inplanes_loop , planes , kernel_size = kernel_size , stride = stride , dilation = dilation , padding = padding_val , separable = separable , bias = False , ) ) self . mconv = nn . Sequential ( * conv ) if residual : stride_residual = stride if stride [ 0 ] == 1 else stride [ 0 ] ** repeat self . res = nn . Sequential ( * self . _get_conv_bn_layer ( inplanes , planes , kernel_size = 1 , stride = stride_residual , bias = False , ) ) else : self . res = None self . mout = nn . Sequential ( * self . _get_act_dropout_layer ( drop_prob = dropout )) forward ( self , x ) Parameters: Name Type Description Default x Tensor Tensor of shape (batch, features, time) where #features == inplanes required Returns: Type Description Tensor Result of applying the block on the input Source code in thunder/quartznet/blocks.py def forward ( self , x : torch . Tensor ) -> torch . Tensor : \"\"\" Args: x : Tensor of shape (batch, features, time) where #features == inplanes Returns: Result of applying the block on the input \"\"\" # compute forward convolutions out = self . mconv ( x ) # compute the residuals if self . res is not None : res_out = self . res ( x ) out = out + res_out # compute the output out = self . mout ( out ) return out body ( filters , kernel_size , repeat_blocks = 1 ) Creates the body of the Quartznet model. That is the middle part. Parameters: Name Type Description Default filters List[int] List of filters inside each block in the body. required kernel_size List[int] Corresponding list of kernel sizes for each block. Should have the same length as the first argument. required repeat_blocks int Number of repetitions of each block inside the body. 1 Returns: Type Description List[thunder.quartznet.blocks.QuartznetBlock] List of layers that form the body of the network. Source code in thunder/quartznet/blocks.py def body ( filters : List [ int ], kernel_size : List [ int ], repeat_blocks : int = 1 ) -> List [ QuartznetBlock ]: \"\"\"Creates the body of the Quartznet model. That is the middle part. Args: filters : List of filters inside each block in the body. kernel_size : Corresponding list of kernel sizes for each block. Should have the same length as the first argument. repeat_blocks : Number of repetitions of each block inside the body. Returns: List of layers that form the body of the network. \"\"\" layers = [] f_in = 256 for f , k in zip ( filters , kernel_size ): for _ in range ( repeat_blocks ): layers . append ( QuartznetBlock ( f_in , f , kernel_size = [ k ], separable = True )) f_in = f layers . extend ( [ QuartznetBlock ( f_in , 512 , repeat = 1 , dilation = [ 2 ], kernel_size = [ 87 ], residual = False , separable = True , ), QuartznetBlock ( 512 , 1024 , repeat = 1 , kernel_size = [ 1 ], residual = False , separable = False ), ] ) return layers get_same_padding ( kernel_size , stride , dilation ) Calculates the padding size to obtain same padding. Same padding means that the output will have the shape input_shape / stride. That means, for stride = 1 the output shape is the same as the input, and stride = 2 gives an output that is half of the input shape. Parameters: Name Type Description Default kernel_size int convolution kernel size. Only tested to be correct with odd values. required stride int convolution stride required dilation int convolution dilation required Exceptions: Type Description ValueError Only stride or dilation may be greater than 1 Returns: Type Description int padding value to obtain same padding. Source code in thunder/quartznet/blocks.py def get_same_padding ( kernel_size : int , stride : int , dilation : int ) -> int : \"\"\"Calculates the padding size to obtain same padding. Same padding means that the output will have the shape input_shape / stride. That means, for stride = 1 the output shape is the same as the input, and stride = 2 gives an output that is half of the input shape. Args: kernel_size : convolution kernel size. Only tested to be correct with odd values. stride : convolution stride dilation : convolution dilation Raises: ValueError: Only stride or dilation may be greater than 1 Returns: padding value to obtain same padding. \"\"\" if stride > 1 and dilation > 1 : raise ValueError ( \"Only stride OR dilation may be greater than 1\" ) if dilation > 1 : return ( dilation * ( kernel_size - 1 ) + 1 ) // 2 return kernel_size // 2 init_weights ( m , mode =< InitMode . xavier_uniform : 'xavier_uniform' > ) Initialize Linear, MaskedConv1d/Conv1d or BatchNorm1d weights. There's no return, the operation occurs inplace. Parameters: Name Type Description Default m Module The layer to be initialized required mode InitMode Weight initialization mode. Only applicable to linear and conv layers. <InitMode.xavier_uniform: 'xavier_uniform'> Exceptions: Type Description ValueError Raised when the initial mode is not one of the possible options. Source code in thunder/quartznet/blocks.py def init_weights ( m : nn . Module , mode : InitMode = InitMode . xavier_uniform ): \"\"\"Initialize Linear, MaskedConv1d/Conv1d or BatchNorm1d weights. There's no return, the operation occurs inplace. Args: m: The layer to be initialized mode: Weight initialization mode. Only applicable to linear and conv layers. Raises: ValueError: Raised when the initial mode is not one of the possible options. \"\"\" if isinstance ( m , ( nn . Conv1d , nn . Linear )): if mode == InitMode . xavier_uniform : nn . init . xavier_uniform_ ( m . weight , gain = 1.0 ) elif mode == InitMode . xavier_normal : nn . init . xavier_normal_ ( m . weight , gain = 1.0 ) elif mode == InitMode . kaiming_uniform : nn . init . kaiming_uniform_ ( m . weight , nonlinearity = \"relu\" ) elif mode == InitMode . kaiming_normal : nn . init . kaiming_normal_ ( m . weight , nonlinearity = \"relu\" ) else : raise ValueError ( \"Unknown Initialization mode: {0} \" . format ( mode )) elif isinstance ( m , nn . BatchNorm1d ): if m . track_running_stats : m . running_mean . zero_ () m . running_var . fill_ ( 1 ) m . num_batches_tracked . zero_ () if m . affine : nn . init . ones_ ( m . weight ) nn . init . zeros_ ( m . bias ) stem ( feat_in ) Creates the Quartznet stem. That is the first block of the model, that process the input directly. Parameters: Name Type Description Default feat_in int Number of input features required Returns: Type Description QuartznetBlock Quartznet stem block Source code in thunder/quartznet/blocks.py def stem ( feat_in : int ) -> QuartznetBlock : \"\"\"Creates the Quartznet stem. That is the first block of the model, that process the input directly. Args: feat_in : Number of input features Returns: Quartznet stem block \"\"\" return QuartznetBlock ( feat_in , 256 , repeat = 1 , stride = [ 2 ], kernel_size = [ 33 ], residual = False , separable = True , )","title":"Blocks"},{"location":"api/Quartznet/blocks/#thunder.quartznet.blocks.InitMode","text":"Weight init methods. Used by init_weights . Note Possible values are xavier_uniform , xavier_normal , kaiming_uniform and kaiming_normal","title":"InitMode"},{"location":"api/Quartznet/blocks/#thunder.quartznet.blocks.QuartznetBlock","text":"","title":"QuartznetBlock"},{"location":"api/Quartznet/blocks/#thunder.quartznet.blocks.QuartznetBlock.__init__","text":"Quartznet block. This is a refactoring of the Jasperblock present on the NeMo toolkit, but simplified to only support the new quartznet model. Biggest change is that dense residual used on Jasper is not supported here, and masked convolutions were also removed. Parameters: Name Type Description Default inplanes int Number of input planes required planes int Number of output planes required repeat int Repetitions inside block. 5 kernel_size List[int] Kernel size. [11] stride List[int] Stride of each repetition. [1] dilation List[int] Dilation of each repetition. [1] dropout float Dropout used before each activation. 0.0 residual bool Controls the use of residual connection. True separable bool Controls the use of separable convolutions. False Source code in thunder/quartznet/blocks.py def __init__ ( self , inplanes : int , planes : int , repeat : int = 5 , kernel_size : List [ int ] = [ 11 ], stride : List [ int ] = [ 1 ], dilation : List [ int ] = [ 1 ], dropout : float = 0.0 , residual : bool = True , separable : bool = False , ): \"\"\"Quartznet block. This is a refactoring of the Jasperblock present on the NeMo toolkit, but simplified to only support the new quartznet model. Biggest change is that dense residual used on Jasper is not supported here, and masked convolutions were also removed. Args: inplanes : Number of input planes planes : Number of output planes repeat : Repetitions inside block. kernel_size : Kernel size. stride : Stride of each repetition. dilation : Dilation of each repetition. dropout : Dropout used before each activation. residual : Controls the use of residual connection. separable : Controls the use of separable convolutions. \"\"\" super () . __init__ () padding_val = get_same_padding ( kernel_size [ 0 ], stride [ 0 ], dilation [ 0 ]) inplanes_loop = inplanes conv = [] for _ in range ( repeat - 1 ): conv . extend ( self . _get_conv_bn_layer ( inplanes_loop , planes , kernel_size = kernel_size , stride = stride , dilation = dilation , padding = padding_val , separable = separable , bias = False , ) ) conv . extend ( self . _get_act_dropout_layer ( drop_prob = dropout )) inplanes_loop = planes conv . extend ( self . _get_conv_bn_layer ( inplanes_loop , planes , kernel_size = kernel_size , stride = stride , dilation = dilation , padding = padding_val , separable = separable , bias = False , ) ) self . mconv = nn . Sequential ( * conv ) if residual : stride_residual = stride if stride [ 0 ] == 1 else stride [ 0 ] ** repeat self . res = nn . Sequential ( * self . _get_conv_bn_layer ( inplanes , planes , kernel_size = 1 , stride = stride_residual , bias = False , ) ) else : self . res = None self . mout = nn . Sequential ( * self . _get_act_dropout_layer ( drop_prob = dropout ))","title":"__init__()"},{"location":"api/Quartznet/blocks/#thunder.quartznet.blocks.QuartznetBlock.forward","text":"Parameters: Name Type Description Default x Tensor Tensor of shape (batch, features, time) where #features == inplanes required Returns: Type Description Tensor Result of applying the block on the input Source code in thunder/quartznet/blocks.py def forward ( self , x : torch . Tensor ) -> torch . Tensor : \"\"\" Args: x : Tensor of shape (batch, features, time) where #features == inplanes Returns: Result of applying the block on the input \"\"\" # compute forward convolutions out = self . mconv ( x ) # compute the residuals if self . res is not None : res_out = self . res ( x ) out = out + res_out # compute the output out = self . mout ( out ) return out","title":"forward()"},{"location":"api/Quartznet/blocks/#thunder.quartznet.blocks.body","text":"Creates the body of the Quartznet model. That is the middle part. Parameters: Name Type Description Default filters List[int] List of filters inside each block in the body. required kernel_size List[int] Corresponding list of kernel sizes for each block. Should have the same length as the first argument. required repeat_blocks int Number of repetitions of each block inside the body. 1 Returns: Type Description List[thunder.quartznet.blocks.QuartznetBlock] List of layers that form the body of the network. Source code in thunder/quartznet/blocks.py def body ( filters : List [ int ], kernel_size : List [ int ], repeat_blocks : int = 1 ) -> List [ QuartznetBlock ]: \"\"\"Creates the body of the Quartznet model. That is the middle part. Args: filters : List of filters inside each block in the body. kernel_size : Corresponding list of kernel sizes for each block. Should have the same length as the first argument. repeat_blocks : Number of repetitions of each block inside the body. Returns: List of layers that form the body of the network. \"\"\" layers = [] f_in = 256 for f , k in zip ( filters , kernel_size ): for _ in range ( repeat_blocks ): layers . append ( QuartznetBlock ( f_in , f , kernel_size = [ k ], separable = True )) f_in = f layers . extend ( [ QuartznetBlock ( f_in , 512 , repeat = 1 , dilation = [ 2 ], kernel_size = [ 87 ], residual = False , separable = True , ), QuartznetBlock ( 512 , 1024 , repeat = 1 , kernel_size = [ 1 ], residual = False , separable = False ), ] ) return layers","title":"body()"},{"location":"api/Quartznet/blocks/#thunder.quartznet.blocks.get_same_padding","text":"Calculates the padding size to obtain same padding. Same padding means that the output will have the shape input_shape / stride. That means, for stride = 1 the output shape is the same as the input, and stride = 2 gives an output that is half of the input shape. Parameters: Name Type Description Default kernel_size int convolution kernel size. Only tested to be correct with odd values. required stride int convolution stride required dilation int convolution dilation required Exceptions: Type Description ValueError Only stride or dilation may be greater than 1 Returns: Type Description int padding value to obtain same padding. Source code in thunder/quartznet/blocks.py def get_same_padding ( kernel_size : int , stride : int , dilation : int ) -> int : \"\"\"Calculates the padding size to obtain same padding. Same padding means that the output will have the shape input_shape / stride. That means, for stride = 1 the output shape is the same as the input, and stride = 2 gives an output that is half of the input shape. Args: kernel_size : convolution kernel size. Only tested to be correct with odd values. stride : convolution stride dilation : convolution dilation Raises: ValueError: Only stride or dilation may be greater than 1 Returns: padding value to obtain same padding. \"\"\" if stride > 1 and dilation > 1 : raise ValueError ( \"Only stride OR dilation may be greater than 1\" ) if dilation > 1 : return ( dilation * ( kernel_size - 1 ) + 1 ) // 2 return kernel_size // 2","title":"get_same_padding()"},{"location":"api/Quartznet/blocks/#thunder.quartznet.blocks.init_weights","text":"Initialize Linear, MaskedConv1d/Conv1d or BatchNorm1d weights. There's no return, the operation occurs inplace. Parameters: Name Type Description Default m Module The layer to be initialized required mode InitMode Weight initialization mode. Only applicable to linear and conv layers. <InitMode.xavier_uniform: 'xavier_uniform'> Exceptions: Type Description ValueError Raised when the initial mode is not one of the possible options. Source code in thunder/quartznet/blocks.py def init_weights ( m : nn . Module , mode : InitMode = InitMode . xavier_uniform ): \"\"\"Initialize Linear, MaskedConv1d/Conv1d or BatchNorm1d weights. There's no return, the operation occurs inplace. Args: m: The layer to be initialized mode: Weight initialization mode. Only applicable to linear and conv layers. Raises: ValueError: Raised when the initial mode is not one of the possible options. \"\"\" if isinstance ( m , ( nn . Conv1d , nn . Linear )): if mode == InitMode . xavier_uniform : nn . init . xavier_uniform_ ( m . weight , gain = 1.0 ) elif mode == InitMode . xavier_normal : nn . init . xavier_normal_ ( m . weight , gain = 1.0 ) elif mode == InitMode . kaiming_uniform : nn . init . kaiming_uniform_ ( m . weight , nonlinearity = \"relu\" ) elif mode == InitMode . kaiming_normal : nn . init . kaiming_normal_ ( m . weight , nonlinearity = \"relu\" ) else : raise ValueError ( \"Unknown Initialization mode: {0} \" . format ( mode )) elif isinstance ( m , nn . BatchNorm1d ): if m . track_running_stats : m . running_mean . zero_ () m . running_var . fill_ ( 1 ) m . num_batches_tracked . zero_ () if m . affine : nn . init . ones_ ( m . weight ) nn . init . zeros_ ( m . bias )","title":"init_weights()"},{"location":"api/Quartznet/blocks/#thunder.quartznet.blocks.stem","text":"Creates the Quartznet stem. That is the first block of the model, that process the input directly. Parameters: Name Type Description Default feat_in int Number of input features required Returns: Type Description QuartznetBlock Quartznet stem block Source code in thunder/quartznet/blocks.py def stem ( feat_in : int ) -> QuartznetBlock : \"\"\"Creates the Quartznet stem. That is the first block of the model, that process the input directly. Args: feat_in : Number of input features Returns: Quartznet stem block \"\"\" return QuartznetBlock ( feat_in , 256 , repeat = 1 , stride = [ 2 ], kernel_size = [ 33 ], residual = False , separable = True , )","title":"stem()"},{"location":"api/Quartznet/compatibility/","text":"Helper functions to load the Quartznet model from original Nemo released checkpoint files. download_checkpoint ( name , checkpoint_folder = None ) Download quartznet checkpoint by identifier. Parameters: Name Type Description Default name str Model identifier. Check checkpoint_archives.keys() required checkpoint_folder str Folder where the checkpoint will be saved to. None Returns: Type Description Path Path to the saved checkpoint file. Source code in thunder/quartznet/compatibility.py def download_checkpoint ( name : str , checkpoint_folder : str = None ) -> Path : \"\"\"Download quartznet checkpoint by identifier. Args: name: Model identifier. Check checkpoint_archives.keys() checkpoint_folder: Folder where the checkpoint will be saved to. Returns: Path to the saved checkpoint file. \"\"\" if checkpoint_folder is None : checkpoint_folder = get_default_cache_folder () url = checkpoint_archives [ name ] filename = url . split ( \"/\" )[ - 1 ] checkpoint_path = Path ( checkpoint_folder ) / filename if not checkpoint_path . exists (): wget . download ( url , out = str ( checkpoint_path )) return checkpoint_path load_quartznet_weights ( encoder , decoder , weights_path ) Load Quartznet model weights from data present inside .nemo file Parameters: Name Type Description Default encoder Module Encoder module to load the weights into required decoder Module Decoder module to load the weights into required weights_path str Path to the pytorch weights checkpoint required Source code in thunder/quartznet/compatibility.py def load_quartznet_weights ( encoder : nn . Module , decoder : nn . Module , weights_path : str ): \"\"\"Load Quartznet model weights from data present inside .nemo file Args: encoder: Encoder module to load the weights into decoder: Decoder module to load the weights into weights_path : Path to the pytorch weights checkpoint \"\"\" weights = torch . load ( weights_path ) # We remove the 'encoder.' and 'decoder.' prefix from the weights to enable # compatibility to load with plain nn.Modules created by reading the config encoder_weights = { k . replace ( \"encoder.\" , \"\" ) . replace ( \".conv\" , \"\" ) . replace ( \".res.0\" , \".res\" ): v for k , v in weights . items () if \"encoder\" in k } encoder . load_state_dict ( encoder_weights , strict = True ) decoder_weights = { k . replace ( \"decoder.decoder_layers.0.\" , \"\" ): v for k , v in weights . items () if \"decoder\" in k } decoder . load_state_dict ( decoder_weights , strict = True ) read_params_from_config ( config_path ) Read the important parameters from the config stored inside the .nemo checkpoint. Parameters: Name Type Description Default config_path str Path to the .yaml file, usually called model_config.yaml required Returns: Type Description Tuple[Dict, List[str], Dict] A tuple containing, in this order, the encoder hyperparameters, the vocabulary, and the preprocessing hyperparameters Source code in thunder/quartznet/compatibility.py def read_params_from_config ( config_path : str ) -> Tuple [ Dict , List [ str ], Dict ]: \"\"\"Read the important parameters from the config stored inside the .nemo checkpoint. Args: config_path : Path to the .yaml file, usually called model_config.yaml Returns: A tuple containing, in this order, the encoder hyperparameters, the vocabulary, and the preprocessing hyperparameters \"\"\" conf = OmegaConf . load ( config_path ) encoder_params = conf [ \"encoder\" ][ \"params\" ] quartznet_conf = OmegaConf . to_container ( encoder_params [ \"jasper\" ]) body_config = quartznet_conf [ 1 : - 2 ] filters = [ cfg [ \"filters\" ] for cfg in body_config ] kernel_sizes = [ cfg [ \"kernel\" ][ 0 ] for cfg in body_config ] encoder_cfg = { \"filters\" : filters , \"kernel_sizes\" : kernel_sizes , } preprocess = conf [ \"preprocessor\" ][ \"params\" ] preprocess_cfg = { \"sample_rate\" : preprocess [ \"sample_rate\" ], \"n_window_size\" : int ( preprocess [ \"window_size\" ] * preprocess [ \"sample_rate\" ]), \"n_window_stride\" : int ( preprocess [ \"window_stride\" ] * preprocess [ \"sample_rate\" ]), \"n_fft\" : preprocess [ \"n_fft\" ], \"nfilt\" : preprocess [ \"features\" ], \"dither\" : preprocess [ \"dither\" ], } return ( encoder_cfg , OmegaConf . to_container ( conf [ \"labels\" ]), preprocess_cfg , )","title":"Compatibility"},{"location":"api/Quartznet/compatibility/#thunder.quartznet.compatibility.download_checkpoint","text":"Download quartznet checkpoint by identifier. Parameters: Name Type Description Default name str Model identifier. Check checkpoint_archives.keys() required checkpoint_folder str Folder where the checkpoint will be saved to. None Returns: Type Description Path Path to the saved checkpoint file. Source code in thunder/quartznet/compatibility.py def download_checkpoint ( name : str , checkpoint_folder : str = None ) -> Path : \"\"\"Download quartznet checkpoint by identifier. Args: name: Model identifier. Check checkpoint_archives.keys() checkpoint_folder: Folder where the checkpoint will be saved to. Returns: Path to the saved checkpoint file. \"\"\" if checkpoint_folder is None : checkpoint_folder = get_default_cache_folder () url = checkpoint_archives [ name ] filename = url . split ( \"/\" )[ - 1 ] checkpoint_path = Path ( checkpoint_folder ) / filename if not checkpoint_path . exists (): wget . download ( url , out = str ( checkpoint_path )) return checkpoint_path","title":"download_checkpoint()"},{"location":"api/Quartznet/compatibility/#thunder.quartznet.compatibility.load_quartznet_weights","text":"Load Quartznet model weights from data present inside .nemo file Parameters: Name Type Description Default encoder Module Encoder module to load the weights into required decoder Module Decoder module to load the weights into required weights_path str Path to the pytorch weights checkpoint required Source code in thunder/quartznet/compatibility.py def load_quartznet_weights ( encoder : nn . Module , decoder : nn . Module , weights_path : str ): \"\"\"Load Quartznet model weights from data present inside .nemo file Args: encoder: Encoder module to load the weights into decoder: Decoder module to load the weights into weights_path : Path to the pytorch weights checkpoint \"\"\" weights = torch . load ( weights_path ) # We remove the 'encoder.' and 'decoder.' prefix from the weights to enable # compatibility to load with plain nn.Modules created by reading the config encoder_weights = { k . replace ( \"encoder.\" , \"\" ) . replace ( \".conv\" , \"\" ) . replace ( \".res.0\" , \".res\" ): v for k , v in weights . items () if \"encoder\" in k } encoder . load_state_dict ( encoder_weights , strict = True ) decoder_weights = { k . replace ( \"decoder.decoder_layers.0.\" , \"\" ): v for k , v in weights . items () if \"decoder\" in k } decoder . load_state_dict ( decoder_weights , strict = True )","title":"load_quartznet_weights()"},{"location":"api/Quartznet/compatibility/#thunder.quartznet.compatibility.read_params_from_config","text":"Read the important parameters from the config stored inside the .nemo checkpoint. Parameters: Name Type Description Default config_path str Path to the .yaml file, usually called model_config.yaml required Returns: Type Description Tuple[Dict, List[str], Dict] A tuple containing, in this order, the encoder hyperparameters, the vocabulary, and the preprocessing hyperparameters Source code in thunder/quartznet/compatibility.py def read_params_from_config ( config_path : str ) -> Tuple [ Dict , List [ str ], Dict ]: \"\"\"Read the important parameters from the config stored inside the .nemo checkpoint. Args: config_path : Path to the .yaml file, usually called model_config.yaml Returns: A tuple containing, in this order, the encoder hyperparameters, the vocabulary, and the preprocessing hyperparameters \"\"\" conf = OmegaConf . load ( config_path ) encoder_params = conf [ \"encoder\" ][ \"params\" ] quartznet_conf = OmegaConf . to_container ( encoder_params [ \"jasper\" ]) body_config = quartznet_conf [ 1 : - 2 ] filters = [ cfg [ \"filters\" ] for cfg in body_config ] kernel_sizes = [ cfg [ \"kernel\" ][ 0 ] for cfg in body_config ] encoder_cfg = { \"filters\" : filters , \"kernel_sizes\" : kernel_sizes , } preprocess = conf [ \"preprocessor\" ][ \"params\" ] preprocess_cfg = { \"sample_rate\" : preprocess [ \"sample_rate\" ], \"n_window_size\" : int ( preprocess [ \"window_size\" ] * preprocess [ \"sample_rate\" ]), \"n_window_stride\" : int ( preprocess [ \"window_stride\" ] * preprocess [ \"sample_rate\" ]), \"n_fft\" : preprocess [ \"n_fft\" ], \"nfilt\" : preprocess [ \"features\" ], \"dither\" : preprocess [ \"dither\" ], } return ( encoder_cfg , OmegaConf . to_container ( conf [ \"labels\" ]), preprocess_cfg , )","title":"read_params_from_config()"},{"location":"api/Quartznet/model/","text":"Functionality to quickly create a new quartznet model. Quartznet15x5_encoder ( feat_in = 64 ) Build encoder corresponding to the Quartznet15x5 model. Parameters: Name Type Description Default feat_in int Number of input features to the model. 64 Returns: Type Description Module Pytorch model of the encoder Source code in thunder/quartznet/model.py def Quartznet15x5_encoder ( feat_in : int = 64 ) -> nn . Module : \"\"\"Build encoder corresponding to the Quartznet15x5 model. Args: feat_in : Number of input features to the model. Returns: Pytorch model of the encoder \"\"\" return Quartznet5 ( feat_in , repeat_blocks = 3 ) Quartznet5 ( feat_in , filters = [ 256 , 256 , 512 , 512 , 512 ], kernel_sizes = [ 33 , 39 , 51 , 63 , 75 ], repeat_blocks = 1 ) Basic Quartznet encoder setup. Can be used to build either Quartznet5x5 (repeat_blocks=1) or Quartznet15x5 (repeat_blocks=3) Parameters: Name Type Description Default feat_in int Number of input features to the model. required repeat_blocks int Number of repetitions of each block. 1 Returns: Type Description Module Pytorch model corresponding to the encoder. Source code in thunder/quartznet/model.py def Quartznet5 ( feat_in : int , filters : List [ int ] = [ 256 , 256 , 512 , 512 , 512 ], kernel_sizes : List [ int ] = [ 33 , 39 , 51 , 63 , 75 ], repeat_blocks : int = 1 , ) -> nn . Module : \"\"\"Basic Quartznet encoder setup. Can be used to build either Quartznet5x5 (repeat_blocks=1) or Quartznet15x5 (repeat_blocks=3) Args: feat_in : Number of input features to the model. repeat_blocks : Number of repetitions of each block. Returns: Pytorch model corresponding to the encoder. \"\"\" return nn . Sequential ( stem ( feat_in ), * body ( filters , kernel_sizes , repeat_blocks ), ) Quartznet5x5_encoder ( feat_in = 64 ) Build encoder corresponding to the Quartznet5x5 model. Parameters: Name Type Description Default feat_in int Number of input features to the model. 64 Returns: Type Description Module Pytorch model of the encoder Source code in thunder/quartznet/model.py def Quartznet5x5_encoder ( feat_in : int = 64 ) -> nn . Module : \"\"\"Build encoder corresponding to the Quartznet5x5 model. Args: feat_in : Number of input features to the model. Returns: Pytorch model of the encoder \"\"\" return Quartznet5 ( feat_in ) Quartznet_decoder ( num_classes , input_channels = 1024 ) Build the Quartznet decoder. Parameters: Name Type Description Default num_classes int Number of output classes of the model. It's the size of the vocabulary, excluding the blank symbol. required Returns: Type Description Module Pytorch model of the decoder Source code in thunder/quartznet/model.py def Quartznet_decoder ( num_classes : int , input_channels : int = 1024 ) -> nn . Module : \"\"\"Build the Quartznet decoder. Args: num_classes : Number of output classes of the model. It's the size of the vocabulary, excluding the blank symbol. Returns: Pytorch model of the decoder \"\"\" decoder = nn . Conv1d ( input_channels , num_classes , kernel_size = 1 , bias = True , ) decoder . apply ( init_weights ) return decoder","title":"Model"},{"location":"api/Quartznet/model/#thunder.quartznet.model.Quartznet15x5_encoder","text":"Build encoder corresponding to the Quartznet15x5 model. Parameters: Name Type Description Default feat_in int Number of input features to the model. 64 Returns: Type Description Module Pytorch model of the encoder Source code in thunder/quartznet/model.py def Quartznet15x5_encoder ( feat_in : int = 64 ) -> nn . Module : \"\"\"Build encoder corresponding to the Quartznet15x5 model. Args: feat_in : Number of input features to the model. Returns: Pytorch model of the encoder \"\"\" return Quartznet5 ( feat_in , repeat_blocks = 3 )","title":"Quartznet15x5_encoder()"},{"location":"api/Quartznet/model/#thunder.quartznet.model.Quartznet5","text":"Basic Quartznet encoder setup. Can be used to build either Quartznet5x5 (repeat_blocks=1) or Quartznet15x5 (repeat_blocks=3) Parameters: Name Type Description Default feat_in int Number of input features to the model. required repeat_blocks int Number of repetitions of each block. 1 Returns: Type Description Module Pytorch model corresponding to the encoder. Source code in thunder/quartznet/model.py def Quartznet5 ( feat_in : int , filters : List [ int ] = [ 256 , 256 , 512 , 512 , 512 ], kernel_sizes : List [ int ] = [ 33 , 39 , 51 , 63 , 75 ], repeat_blocks : int = 1 , ) -> nn . Module : \"\"\"Basic Quartznet encoder setup. Can be used to build either Quartznet5x5 (repeat_blocks=1) or Quartznet15x5 (repeat_blocks=3) Args: feat_in : Number of input features to the model. repeat_blocks : Number of repetitions of each block. Returns: Pytorch model corresponding to the encoder. \"\"\" return nn . Sequential ( stem ( feat_in ), * body ( filters , kernel_sizes , repeat_blocks ), )","title":"Quartznet5()"},{"location":"api/Quartznet/model/#thunder.quartznet.model.Quartznet5x5_encoder","text":"Build encoder corresponding to the Quartznet5x5 model. Parameters: Name Type Description Default feat_in int Number of input features to the model. 64 Returns: Type Description Module Pytorch model of the encoder Source code in thunder/quartznet/model.py def Quartznet5x5_encoder ( feat_in : int = 64 ) -> nn . Module : \"\"\"Build encoder corresponding to the Quartznet5x5 model. Args: feat_in : Number of input features to the model. Returns: Pytorch model of the encoder \"\"\" return Quartznet5 ( feat_in )","title":"Quartznet5x5_encoder()"},{"location":"api/Quartznet/model/#thunder.quartznet.model.Quartznet_decoder","text":"Build the Quartznet decoder. Parameters: Name Type Description Default num_classes int Number of output classes of the model. It's the size of the vocabulary, excluding the blank symbol. required Returns: Type Description Module Pytorch model of the decoder Source code in thunder/quartznet/model.py def Quartznet_decoder ( num_classes : int , input_channels : int = 1024 ) -> nn . Module : \"\"\"Build the Quartznet decoder. Args: num_classes : Number of output classes of the model. It's the size of the vocabulary, excluding the blank symbol. Returns: Pytorch model of the decoder \"\"\" decoder = nn . Conv1d ( input_channels , num_classes , kernel_size = 1 , bias = True , ) decoder . apply ( init_weights ) return decoder","title":"Quartznet_decoder()"},{"location":"api/Quartznet/module/","text":"QuartznetModule configure_optimizers ( self ) Choose what optimizers and learning-rate schedulers to use in your optimization. Normally you'd need one. But in the case of GANs or similar you might have multiple. Returns: Type Description Any of these 6 options. Single optimizer. List or Tuple - List of optimizers. Two lists - The first list has multiple optimizers, the second a list of LR schedulers (or lr_dict). Dictionary, with an 'optimizer' key, and (optionally) a 'lr_scheduler' key whose value is a single LR scheduler or lr_dict. Tuple of dictionaries as described, with an optional 'frequency' key. None - Fit will run without any optimizer. Note The 'frequency' value is an int corresponding to the number of sequential batches optimized with the specific optimizer. It should be given to none or to all of the optimizers. There is a difference between passing multiple optimizers in a list, and passing multiple optimizers in dictionaries with a frequency of 1: In the former case, all optimizers will operate on the given batch in each optimization step. In the latter, only one optimizer will operate on the given batch at every step. The lr_dict is a dictionary which contains the scheduler and its associated configuration. The default configuration is shown below. .. code-block:: python { 'scheduler': lr_scheduler, # The LR scheduler instance (required) 'interval': 'epoch', # The unit of the scheduler's step size 'frequency': 1, # The frequency of the scheduler 'reduce_on_plateau': False, # For ReduceLROnPlateau scheduler 'monitor': 'val_loss', # Metric for ReduceLROnPlateau to monitor 'strict': True, # Whether to crash the training if `monitor` is not found 'name': None, # Custom name for LearningRateMonitor to use } Only the scheduler key is required, the rest will be set to the defaults above. Examples:: # most cases def configure_optimizers(self): opt = Adam(self.parameters(), lr=1e-3) return opt # multiple optimizer case (e.g.: GAN) def configure_optimizers(self): generator_opt = Adam(self.model_gen.parameters(), lr=0.01) disriminator_opt = Adam(self.model_disc.parameters(), lr=0.02) return generator_opt, disriminator_opt # example with learning rate schedulers def configure_optimizers(self): generator_opt = Adam(self.model_gen.parameters(), lr=0.01) disriminator_opt = Adam(self.model_disc.parameters(), lr=0.02) discriminator_sched = CosineAnnealing(discriminator_opt, T_max=10) return [generator_opt, disriminator_opt], [discriminator_sched] # example with step-based learning rate schedulers def configure_optimizers(self): gen_opt = Adam(self.model_gen.parameters(), lr=0.01) dis_opt = Adam(self.model_disc.parameters(), lr=0.02) gen_sched = {'scheduler': ExponentialLR(gen_opt, 0.99), 'interval': 'step'} # called after each training step dis_sched = CosineAnnealing(discriminator_opt, T_max=10) # called every epoch return [gen_opt, dis_opt], [gen_sched, dis_sched] # example with optimizer frequencies # see training procedure in `Improved Training of Wasserstein GANs`, Algorithm 1 # https://arxiv.org/abs/1704.00028 def configure_optimizers(self): gen_opt = Adam(self.model_gen.parameters(), lr=0.01) dis_opt = Adam(self.model_disc.parameters(), lr=0.02) n_critic = 5 return ( {'optimizer': dis_opt, 'frequency': n_critic}, {'optimizer': gen_opt, 'frequency': 1} ) Note: Some things to know: - Lightning calls ``.backward()`` and ``.step()`` on each optimizer and learning rate scheduler as needed. - If you use 16-bit precision (``precision=16``), Lightning will automatically handle the optimizers for you. - If you use multiple optimizers, :meth:`training_step` will have an additional ``optimizer_idx`` parameter. - If you use LBFGS Lightning handles the closure function automatically for you. - If you use multiple optimizers, gradients will be calculated only for the parameters of current optimizer at each training step. - If you need to control how often those optimizers step or override the default ``.step()`` schedule, override the :meth:`optimizer_step` hook. - If you only want to call a learning rate scheduler every ``x`` step or epoch, or want to monitor a custom metric, you can specify these in a lr_dict: .. code-block:: python { 'scheduler': lr_scheduler, 'interval': 'step', # or 'epoch' 'monitor': 'val_f1', 'frequency': x, } Source code in thunder/quartznet/module.py def configure_optimizers ( self ): return torch . optim . Adam ( filter ( lambda p : p . requires_grad , self . parameters ()), lr = self . hparams . learning_rate , ) forward ( self , x ) Same as :meth: torch.nn.Module.forward() , however in Lightning you want this to define the operations you want to use for prediction (i.e.: on a server or as a feature extractor). Normally you'd call self() from your :meth: training_step method. This makes it easy to write a complex system for training with the outputs you'd want in a prediction setting. You may also find the :func: ~pytorch_lightning.core.decorators.auto_move_data decorator useful when using the module outside Lightning in a production setting. Parameters: Name Type Description Default *args Whatever you decide to pass into the forward method. required **kwargs Keyword arguments are also possible. required Returns: Type Description Tensor Predicted output Examples:: # example if we were using this model as a feature extractor def forward(self, x): feature_maps = self.convnet(x) return feature_maps def training_step(self, batch, batch_idx): x, y = batch feature_maps = self(x) logits = self.classifier(feature_maps) # ... return loss # splitting it this way allows model to be used a feature extractor model = MyModelAbove() inputs = server.get_request() results = model(inputs) server.write_results(results) # ------------- # This is in stark contrast to torch.nn.Module where normally you would have this: def forward(self, batch): x, y = batch feature_maps = self.convnet(x) logits = self.classifier(feature_maps) return logits Source code in thunder/quartznet/module.py def forward ( self , x : torch . Tensor ) -> torch . Tensor : features = self . features ( x ) encoded = self . encoder ( features ) return self . decoder ( encoded ) predict ( self , x ) Use this function with trainer.predict(...). Override if you need to add any processing logic. Source code in thunder/quartznet/module.py @torch . jit . export def predict ( self , x : torch . Tensor ) -> List [ str ]: pred = self ( x ) return self . text_pipeline . decode_prediction ( pred . argmax ( 1 )) training_step ( self , batch , batch_idx ) Here you compute and return the training loss and some additional metrics for e.g. the progress bar or logger. Parameters: Name Type Description Default batch class: ~torch.Tensor | (:class: ~torch.Tensor , ...) | [:class: ~torch.Tensor , ...]): The output of your :class: ~torch.utils.data.DataLoader . A tensor, tuple or list. required batch_idx int Integer displaying index of this batch required optimizer_idx int When using multiple optimizers, this argument will also be present. required hiddens( class: ~torch.Tensor ): Passed in if :paramref: ~pytorch_lightning.trainer.trainer.Trainer.truncated_bptt_steps > 0. required Returns: Type Description Any of. - class: ~torch.Tensor - The loss tensor - dict - A dictionary. Can include any keys, but must include the key 'loss' - None - Training will skip to the next batch Note Returning None is currently not supported for multi-GPU or TPU, or with 16-bit precision enabled. In this step you'd normally do the forward pass and calculate the loss for a batch. You can also do fancier things like multiple forward passes or something model specific. Example:: def training_step(self, batch, batch_idx): x, y, z = batch out = self.encoder(x) loss = self.loss(out, x) return loss If you define multiple optimizers, this step will be called with an additional optimizer_idx parameter. .. code-block:: python # Multiple optimizers (e.g.: GANs) def training_step(self, batch, batch_idx, optimizer_idx): if optimizer_idx == 0: # do training_step with encoder if optimizer_idx == 1: # do training_step with decoder If you add truncated back propagation through time you will also get an additional argument with the hidden states of the previous step. .. code-block:: python # Truncated back-propagation through time def training_step(self, batch, batch_idx, hiddens): # hiddens are the hidden states from the previous truncated backprop step ... out, hiddens = self.lstm(data, hiddens) ... return {'loss': loss, 'hiddens': hiddens} Note The loss value shown in the progress bar is smoothed (averaged) over the last values, so it differs from the actual loss returned in train/validation step. Source code in thunder/quartznet/module.py def training_step ( self , batch , batch_idx ): audio , audio_lens , texts = batch y , y_lens = self . text_pipeline . encode ( texts , device = self . device ) probabilities = self ( audio ) loss = self . calculate_loss ( probabilities , y , audio_lens , y_lens ) self . log ( \"loss/train_loss\" , loss ) return loss validation_step ( self , batch , batch_idx ) Operates on a single batch of data from the validation set. In this step you'd might generate examples or calculate anything of interest like accuracy. .. code-block:: python # the pseudocode for these calls val_outs = [] for val_batch in val_data: out = validation_step(val_batch) val_outs.append(out) validation_epoch_end(val_outs) Parameters: Name Type Description Default batch class: ~torch.Tensor | (:class: ~torch.Tensor , ...) | [:class: ~torch.Tensor , ...]): The output of your :class: ~torch.utils.data.DataLoader . A tensor, tuple or list. required batch_idx int The index of this batch required dataloader_idx int The index of the dataloader that produced this batch (only if multiple val dataloaders used) required Returns: Type Description Any of. Any object or value None - Validation will skip to the next batch .. code-block:: python # pseudocode of order val_outs = [] for val_batch in val_data: out = validation_step(val_batch) if defined('validation_step_end'): out = validation_step_end(out) val_outs.append(out) val_outs = validation_epoch_end(val_outs) .. code-block:: python # if you have one val dataloader: def validation_step(self, batch, batch_idx) # if you have multiple val dataloaders: def validation_step(self, batch, batch_idx, dataloader_idx) Examples:: # CASE 1: A single validation dataset def validation_step(self, batch, batch_idx): x, y = batch # implement your own out = self(x) loss = self.loss(out, y) # log 6 example images # or generated text... or whatever sample_imgs = x[:6] grid = torchvision.utils.make_grid(sample_imgs) self.logger.experiment.add_image('example_images', grid, 0) # calculate acc labels_hat = torch.argmax(out, dim=1) val_acc = torch.sum(y == labels_hat).item() / (len(y) * 1.0) # log the outputs! self.log_dict({'val_loss': loss, 'val_acc': val_acc}) If you pass in multiple val dataloaders, :meth: validation_step will have an additional argument. .. code-block:: python # CASE 2: multiple validation dataloaders def validation_step(self, batch, batch_idx, dataloader_idx): # dataloader_idx tells you which dataset this is. Note If you don't need to validate you don't need to implement this method. Note When the :meth: validation_step is called, the model has been put in eval mode and PyTorch gradients have been disabled. At the end of validation, the model goes back to training mode and gradients are enabled. Source code in thunder/quartznet/module.py def validation_step ( self , batch , batch_idx ): audio , audio_lens , texts = batch y , y_lens = self . text_pipeline . encode ( texts , device = self . device ) probabilities = self ( audio ) loss = self . calculate_loss ( probabilities , y , audio_lens , y_lens ) decoded_preds = self . text_pipeline . decode_prediction ( probabilities . argmax ( 1 )) decoded_targets = self . text_pipeline . decode_prediction ( y ) self . val_cer ( decoded_preds , decoded_targets ) self . val_wer ( decoded_preds , decoded_targets ) self . log ( \"loss/val_loss\" , loss ) self . log ( \"metrics/cer\" , self . val_cer , on_epoch = True ) self . log ( \"metrics/wer\" , self . val_wer , on_epoch = True ) return loss","title":"Module"},{"location":"api/Quartznet/module/#thunder.quartznet.module.QuartznetModule","text":"","title":"QuartznetModule"},{"location":"api/Quartznet/module/#thunder.quartznet.module.QuartznetModule.configure_optimizers","text":"Choose what optimizers and learning-rate schedulers to use in your optimization. Normally you'd need one. But in the case of GANs or similar you might have multiple. Returns: Type Description Any of these 6 options. Single optimizer. List or Tuple - List of optimizers. Two lists - The first list has multiple optimizers, the second a list of LR schedulers (or lr_dict). Dictionary, with an 'optimizer' key, and (optionally) a 'lr_scheduler' key whose value is a single LR scheduler or lr_dict. Tuple of dictionaries as described, with an optional 'frequency' key. None - Fit will run without any optimizer. Note The 'frequency' value is an int corresponding to the number of sequential batches optimized with the specific optimizer. It should be given to none or to all of the optimizers. There is a difference between passing multiple optimizers in a list, and passing multiple optimizers in dictionaries with a frequency of 1: In the former case, all optimizers will operate on the given batch in each optimization step. In the latter, only one optimizer will operate on the given batch at every step. The lr_dict is a dictionary which contains the scheduler and its associated configuration. The default configuration is shown below. .. code-block:: python { 'scheduler': lr_scheduler, # The LR scheduler instance (required) 'interval': 'epoch', # The unit of the scheduler's step size 'frequency': 1, # The frequency of the scheduler 'reduce_on_plateau': False, # For ReduceLROnPlateau scheduler 'monitor': 'val_loss', # Metric for ReduceLROnPlateau to monitor 'strict': True, # Whether to crash the training if `monitor` is not found 'name': None, # Custom name for LearningRateMonitor to use } Only the scheduler key is required, the rest will be set to the defaults above. Examples:: # most cases def configure_optimizers(self): opt = Adam(self.parameters(), lr=1e-3) return opt # multiple optimizer case (e.g.: GAN) def configure_optimizers(self): generator_opt = Adam(self.model_gen.parameters(), lr=0.01) disriminator_opt = Adam(self.model_disc.parameters(), lr=0.02) return generator_opt, disriminator_opt # example with learning rate schedulers def configure_optimizers(self): generator_opt = Adam(self.model_gen.parameters(), lr=0.01) disriminator_opt = Adam(self.model_disc.parameters(), lr=0.02) discriminator_sched = CosineAnnealing(discriminator_opt, T_max=10) return [generator_opt, disriminator_opt], [discriminator_sched] # example with step-based learning rate schedulers def configure_optimizers(self): gen_opt = Adam(self.model_gen.parameters(), lr=0.01) dis_opt = Adam(self.model_disc.parameters(), lr=0.02) gen_sched = {'scheduler': ExponentialLR(gen_opt, 0.99), 'interval': 'step'} # called after each training step dis_sched = CosineAnnealing(discriminator_opt, T_max=10) # called every epoch return [gen_opt, dis_opt], [gen_sched, dis_sched] # example with optimizer frequencies # see training procedure in `Improved Training of Wasserstein GANs`, Algorithm 1 # https://arxiv.org/abs/1704.00028 def configure_optimizers(self): gen_opt = Adam(self.model_gen.parameters(), lr=0.01) dis_opt = Adam(self.model_disc.parameters(), lr=0.02) n_critic = 5 return ( {'optimizer': dis_opt, 'frequency': n_critic}, {'optimizer': gen_opt, 'frequency': 1} ) Note: Some things to know: - Lightning calls ``.backward()`` and ``.step()`` on each optimizer and learning rate scheduler as needed. - If you use 16-bit precision (``precision=16``), Lightning will automatically handle the optimizers for you. - If you use multiple optimizers, :meth:`training_step` will have an additional ``optimizer_idx`` parameter. - If you use LBFGS Lightning handles the closure function automatically for you. - If you use multiple optimizers, gradients will be calculated only for the parameters of current optimizer at each training step. - If you need to control how often those optimizers step or override the default ``.step()`` schedule, override the :meth:`optimizer_step` hook. - If you only want to call a learning rate scheduler every ``x`` step or epoch, or want to monitor a custom metric, you can specify these in a lr_dict: .. code-block:: python { 'scheduler': lr_scheduler, 'interval': 'step', # or 'epoch' 'monitor': 'val_f1', 'frequency': x, } Source code in thunder/quartznet/module.py def configure_optimizers ( self ): return torch . optim . Adam ( filter ( lambda p : p . requires_grad , self . parameters ()), lr = self . hparams . learning_rate , )","title":"configure_optimizers()"},{"location":"api/Quartznet/module/#thunder.quartznet.module.QuartznetModule.forward","text":"Same as :meth: torch.nn.Module.forward() , however in Lightning you want this to define the operations you want to use for prediction (i.e.: on a server or as a feature extractor). Normally you'd call self() from your :meth: training_step method. This makes it easy to write a complex system for training with the outputs you'd want in a prediction setting. You may also find the :func: ~pytorch_lightning.core.decorators.auto_move_data decorator useful when using the module outside Lightning in a production setting. Parameters: Name Type Description Default *args Whatever you decide to pass into the forward method. required **kwargs Keyword arguments are also possible. required Returns: Type Description Tensor Predicted output Examples:: # example if we were using this model as a feature extractor def forward(self, x): feature_maps = self.convnet(x) return feature_maps def training_step(self, batch, batch_idx): x, y = batch feature_maps = self(x) logits = self.classifier(feature_maps) # ... return loss # splitting it this way allows model to be used a feature extractor model = MyModelAbove() inputs = server.get_request() results = model(inputs) server.write_results(results) # ------------- # This is in stark contrast to torch.nn.Module where normally you would have this: def forward(self, batch): x, y = batch feature_maps = self.convnet(x) logits = self.classifier(feature_maps) return logits Source code in thunder/quartznet/module.py def forward ( self , x : torch . Tensor ) -> torch . Tensor : features = self . features ( x ) encoded = self . encoder ( features ) return self . decoder ( encoded )","title":"forward()"},{"location":"api/Quartznet/module/#thunder.quartznet.module.QuartznetModule.predict","text":"Use this function with trainer.predict(...). Override if you need to add any processing logic. Source code in thunder/quartznet/module.py @torch . jit . export def predict ( self , x : torch . Tensor ) -> List [ str ]: pred = self ( x ) return self . text_pipeline . decode_prediction ( pred . argmax ( 1 ))","title":"predict()"},{"location":"api/Quartznet/module/#thunder.quartznet.module.QuartznetModule.training_step","text":"Here you compute and return the training loss and some additional metrics for e.g. the progress bar or logger. Parameters: Name Type Description Default batch class: ~torch.Tensor | (:class: ~torch.Tensor , ...) | [:class: ~torch.Tensor , ...]): The output of your :class: ~torch.utils.data.DataLoader . A tensor, tuple or list. required batch_idx int Integer displaying index of this batch required optimizer_idx int When using multiple optimizers, this argument will also be present. required hiddens( class: ~torch.Tensor ): Passed in if :paramref: ~pytorch_lightning.trainer.trainer.Trainer.truncated_bptt_steps > 0. required Returns: Type Description Any of. - class: ~torch.Tensor - The loss tensor - dict - A dictionary. Can include any keys, but must include the key 'loss' - None - Training will skip to the next batch Note Returning None is currently not supported for multi-GPU or TPU, or with 16-bit precision enabled. In this step you'd normally do the forward pass and calculate the loss for a batch. You can also do fancier things like multiple forward passes or something model specific. Example:: def training_step(self, batch, batch_idx): x, y, z = batch out = self.encoder(x) loss = self.loss(out, x) return loss If you define multiple optimizers, this step will be called with an additional optimizer_idx parameter. .. code-block:: python # Multiple optimizers (e.g.: GANs) def training_step(self, batch, batch_idx, optimizer_idx): if optimizer_idx == 0: # do training_step with encoder if optimizer_idx == 1: # do training_step with decoder If you add truncated back propagation through time you will also get an additional argument with the hidden states of the previous step. .. code-block:: python # Truncated back-propagation through time def training_step(self, batch, batch_idx, hiddens): # hiddens are the hidden states from the previous truncated backprop step ... out, hiddens = self.lstm(data, hiddens) ... return {'loss': loss, 'hiddens': hiddens} Note The loss value shown in the progress bar is smoothed (averaged) over the last values, so it differs from the actual loss returned in train/validation step. Source code in thunder/quartznet/module.py def training_step ( self , batch , batch_idx ): audio , audio_lens , texts = batch y , y_lens = self . text_pipeline . encode ( texts , device = self . device ) probabilities = self ( audio ) loss = self . calculate_loss ( probabilities , y , audio_lens , y_lens ) self . log ( \"loss/train_loss\" , loss ) return loss","title":"training_step()"},{"location":"api/Quartznet/module/#thunder.quartznet.module.QuartznetModule.validation_step","text":"Operates on a single batch of data from the validation set. In this step you'd might generate examples or calculate anything of interest like accuracy. .. code-block:: python # the pseudocode for these calls val_outs = [] for val_batch in val_data: out = validation_step(val_batch) val_outs.append(out) validation_epoch_end(val_outs) Parameters: Name Type Description Default batch class: ~torch.Tensor | (:class: ~torch.Tensor , ...) | [:class: ~torch.Tensor , ...]): The output of your :class: ~torch.utils.data.DataLoader . A tensor, tuple or list. required batch_idx int The index of this batch required dataloader_idx int The index of the dataloader that produced this batch (only if multiple val dataloaders used) required Returns: Type Description Any of. Any object or value None - Validation will skip to the next batch .. code-block:: python # pseudocode of order val_outs = [] for val_batch in val_data: out = validation_step(val_batch) if defined('validation_step_end'): out = validation_step_end(out) val_outs.append(out) val_outs = validation_epoch_end(val_outs) .. code-block:: python # if you have one val dataloader: def validation_step(self, batch, batch_idx) # if you have multiple val dataloaders: def validation_step(self, batch, batch_idx, dataloader_idx) Examples:: # CASE 1: A single validation dataset def validation_step(self, batch, batch_idx): x, y = batch # implement your own out = self(x) loss = self.loss(out, y) # log 6 example images # or generated text... or whatever sample_imgs = x[:6] grid = torchvision.utils.make_grid(sample_imgs) self.logger.experiment.add_image('example_images', grid, 0) # calculate acc labels_hat = torch.argmax(out, dim=1) val_acc = torch.sum(y == labels_hat).item() / (len(y) * 1.0) # log the outputs! self.log_dict({'val_loss': loss, 'val_acc': val_acc}) If you pass in multiple val dataloaders, :meth: validation_step will have an additional argument. .. code-block:: python # CASE 2: multiple validation dataloaders def validation_step(self, batch, batch_idx, dataloader_idx): # dataloader_idx tells you which dataset this is. Note If you don't need to validate you don't need to implement this method. Note When the :meth: validation_step is called, the model has been put in eval mode and PyTorch gradients have been disabled. At the end of validation, the model goes back to training mode and gradients are enabled. Source code in thunder/quartznet/module.py def validation_step ( self , batch , batch_idx ): audio , audio_lens , texts = batch y , y_lens = self . text_pipeline . encode ( texts , device = self . device ) probabilities = self ( audio ) loss = self . calculate_loss ( probabilities , y , audio_lens , y_lens ) decoded_preds = self . text_pipeline . decode_prediction ( probabilities . argmax ( 1 )) decoded_targets = self . text_pipeline . decode_prediction ( y ) self . val_cer ( decoded_preds , decoded_targets ) self . val_wer ( decoded_preds , decoded_targets ) self . log ( \"loss/val_loss\" , loss ) self . log ( \"metrics/cer\" , self . val_cer , on_epoch = True ) self . log ( \"metrics/wer\" , self . val_wer , on_epoch = True ) return loss","title":"validation_step()"},{"location":"api/Quartznet/preprocess/","text":"Functionality to preprocess the audio input in the same way that the Quartznet model expects it. DitherAudio __init__ ( self , dither = 1e-05 ) special Add some dithering to the audio tensor. Note From wikipedia: Dither is an intentionally applied form of noise used to randomize quantization error. Parameters: Name Type Description Default dither float Amount of dither to add. 1e-05 Source code in thunder/quartznet/preprocess.py def __init__ ( self , dither : float = 1e-5 ): \"\"\"Add some dithering to the audio tensor. Note: From wikipedia: Dither is an intentionally applied form of noise used to randomize quantization error. Args: dither : Amount of dither to add. \"\"\" super () . __init__ () self . dither = dither forward ( self , x ) Parameters: Name Type Description Default x Tensor Tensor of shape (batch, time) required Source code in thunder/quartznet/preprocess.py @torch . no_grad () def forward ( self , x : torch . Tensor ) -> torch . Tensor : \"\"\" Args: x : Tensor of shape (batch, time) \"\"\" if self . training : return x + self . dither * torch . randn_like ( x ) else : return x FeatureBatchNormalizer __init__ ( self ) special Normalize batch at the feature dimension. Source code in thunder/quartznet/preprocess.py def __init__ ( self ): \"\"\"Normalize batch at the feature dimension.\"\"\" super () . __init__ () self . div_guard = 1e-5 forward ( self , x ) Parameters: Name Type Description Default x Tensor Tensor of shape (batch, features, time) required Source code in thunder/quartznet/preprocess.py def forward ( self , x : torch . Tensor ) -> torch . Tensor : \"\"\" Args: x : Tensor of shape (batch, features, time) \"\"\" x_mean = x . mean ( dim = 2 , keepdim = True ) . detach () x_std = x . std ( dim = 2 , keepdim = True ) . detach () # make sure x_std is not zero x_std += self . div_guard return ( x - x_mean ) / x_std MelScale __init__ ( self , sample_rate , n_fft , nfilt , log_scale = True ) special Convert a spectrogram to Mel scale, following the default formula of librosa instead of the one used by torchaudio. Also converts to log scale. Parameters: Name Type Description Default sample_rate int Sampling rate of the signal required n_fft int Number of fourier features required nfilt int Number of output mel filters to use required log_scale bool Controls if the output should also be applied a log scale. True Source code in thunder/quartznet/preprocess.py def __init__ ( self , sample_rate : int , n_fft : int , nfilt : int , log_scale : bool = True ): \"\"\"Convert a spectrogram to Mel scale, following the default formula of librosa instead of the one used by torchaudio. Also converts to log scale. Args: sample_rate : Sampling rate of the signal n_fft : Number of fourier features nfilt : Number of output mel filters to use log_scale : Controls if the output should also be applied a log scale. \"\"\" super () . __init__ () filterbanks = ( create_fb_matrix ( int ( 1 + n_fft // 2 ), n_mels = nfilt , sample_rate = sample_rate , f_min = 0 , f_max = sample_rate / 2 , norm = \"slaney\" , htk = True , ) . transpose ( 0 , 1 ) . unsqueeze ( 0 ) ) self . register_buffer ( \"fb\" , filterbanks ) self . log_scale = log_scale forward ( self , x ) Parameters: Name Type Description Default x Tensor Tensor of shape (batch, features, time) required Source code in thunder/quartznet/preprocess.py @torch . no_grad () def forward ( self , x : torch . Tensor ) -> torch . Tensor : \"\"\" Args: x : Tensor of shape (batch, features, time) \"\"\" # dot with filterbank energies x = torch . matmul ( self . fb . to ( x . dtype ), x ) # log features # We want to avoid taking the log of zero if self . log_scale : x = torch . log ( x + 2 ** - 24 ) return x PowerSpectrum __init__ ( self , n_window_size = 320 , n_window_stride = 160 , n_fft = None ) special Calculates the power spectrum of the audio signal, following the same method as used in NEMO. Parameters: Name Type Description Default n_window_size int Number of elements in the window size. 320 n_window_stride int Number of elements in the window stride. 160 n_fft Optional[int] Number of fourier features. None Exceptions: Type Description ValueError Raised when incompatible parameters are passed. Source code in thunder/quartznet/preprocess.py def __init__ ( self , n_window_size : int = 320 , n_window_stride : int = 160 , n_fft : Optional [ int ] = None , ): \"\"\"Calculates the power spectrum of the audio signal, following the same method as used in NEMO. Args: n_window_size : Number of elements in the window size. n_window_stride : Number of elements in the window stride. n_fft : Number of fourier features. Raises: ValueError: Raised when incompatible parameters are passed. \"\"\" super () . __init__ () if n_window_size <= 0 or n_window_stride <= 0 : raise ValueError ( f \" { self } got an invalid value for either n_window_size or \" f \"n_window_stride. Both must be positive ints.\" ) self . win_length = n_window_size self . hop_length = n_window_stride self . n_fft = n_fft or 2 ** math . ceil ( math . log2 ( self . win_length )) window_tensor = torch . hann_window ( self . win_length , periodic = False ) self . register_buffer ( \"window\" , window_tensor ) forward ( self , x ) Parameters: Name Type Description Default x Tensor Tensor of shape (batch, time) required Source code in thunder/quartznet/preprocess.py @torch . no_grad () def forward ( self , x : torch . Tensor ) -> torch . Tensor : \"\"\" Args: x : Tensor of shape (batch, time) \"\"\" x = torch . stft ( x , n_fft = self . n_fft , hop_length = self . hop_length , win_length = self . win_length , center = True , window = self . window . to ( dtype = torch . float ), return_complex = False , ) # torch returns real, imag; so convert to magnitude x = torch . sqrt ( x . pow ( 2 ) . sum ( - 1 )) # get power spectrum x = x . pow ( 2.0 ) return x PreEmphasisFilter __init__ ( self , preemph = 0.97 ) special Applies preemphasis filtering to the audio signal. This is a classic signal processing function to emphasise the high frequency portion of the content compared to the low frequency. It applies a FIR filter of the form: y[n] = y[n] - preemph * y[n-1] Parameters: Name Type Description Default preemph float Filter control factor. 0.97 Source code in thunder/quartznet/preprocess.py def __init__ ( self , preemph : float = 0.97 ): \"\"\"Applies preemphasis filtering to the audio signal. This is a classic signal processing function to emphasise the high frequency portion of the content compared to the low frequency. It applies a FIR filter of the form: `y[n] = y[n] - preemph * y[n-1]` Args: preemph : Filter control factor. \"\"\" super () . __init__ () self . preemph = preemph forward ( self , x ) Parameters: Name Type Description Default x Tensor Tensor of shape (batch, time) required Source code in thunder/quartznet/preprocess.py @torch . no_grad () def forward ( self , x : torch . Tensor ) -> torch . Tensor : \"\"\" Args: x : Tensor of shape (batch, time) \"\"\" return torch . cat ( ( x [:, 0 ] . unsqueeze ( 1 ), x [:, 1 :] - self . preemph * x [:, : - 1 ]), dim = 1 ) FilterbankFeatures ( sample_rate = 16000 , n_window_size = 320 , n_window_stride = 160 , n_fft = 512 , preemph = 0.97 , nfilt = 64 , dither = 1e-05 , ** kwargs ) Creates the Filterbank features used in the Quartznet model. Parameters: Name Type Description Default sample_rate int Sampling rate of the signal 16000 n_window_size int Number of elements in the window size. 320 n_window_stride int Number of elements in the window stride. 160 n_fft int Number of fourier features. 512 preemph float Preemphasis filtering control factor. 0.97 nfilt int Number of output mel filters to use 64 dither float Amount of dither to add. 1e-05 Returns: Type Description Module Module that computes the features based on raw audio tensor. Source code in thunder/quartznet/preprocess.py def FilterbankFeatures ( sample_rate : int = 16000 , n_window_size : int = 320 , n_window_stride : int = 160 , n_fft : int = 512 , preemph : float = 0.97 , nfilt : int = 64 , dither : float = 1e-5 , ** kwargs , ) -> nn . Module : \"\"\"Creates the Filterbank features used in the Quartznet model. Args: sample_rate : Sampling rate of the signal n_window_size : Number of elements in the window size. n_window_stride : Number of elements in the window stride. n_fft : Number of fourier features. preemph : Preemphasis filtering control factor. nfilt : Number of output mel filters to use dither : Amount of dither to add. Returns: Module that computes the features based on raw audio tensor. \"\"\" return nn . Sequential ( DitherAudio ( dither = dither ), PreEmphasisFilter ( preemph = preemph ), PowerSpectrum ( n_window_size = n_window_size , n_window_stride = n_window_stride , n_fft = n_fft , ), MelScale ( sample_rate = sample_rate , n_fft = n_fft , nfilt = nfilt ), FeatureBatchNormalizer (), )","title":"Preprocess"},{"location":"api/Quartznet/preprocess/#thunder.quartznet.preprocess.DitherAudio","text":"","title":"DitherAudio"},{"location":"api/Quartznet/preprocess/#thunder.quartznet.preprocess.DitherAudio.__init__","text":"Add some dithering to the audio tensor. Note From wikipedia: Dither is an intentionally applied form of noise used to randomize quantization error. Parameters: Name Type Description Default dither float Amount of dither to add. 1e-05 Source code in thunder/quartznet/preprocess.py def __init__ ( self , dither : float = 1e-5 ): \"\"\"Add some dithering to the audio tensor. Note: From wikipedia: Dither is an intentionally applied form of noise used to randomize quantization error. Args: dither : Amount of dither to add. \"\"\" super () . __init__ () self . dither = dither","title":"__init__()"},{"location":"api/Quartznet/preprocess/#thunder.quartznet.preprocess.DitherAudio.forward","text":"Parameters: Name Type Description Default x Tensor Tensor of shape (batch, time) required Source code in thunder/quartznet/preprocess.py @torch . no_grad () def forward ( self , x : torch . Tensor ) -> torch . Tensor : \"\"\" Args: x : Tensor of shape (batch, time) \"\"\" if self . training : return x + self . dither * torch . randn_like ( x ) else : return x","title":"forward()"},{"location":"api/Quartznet/preprocess/#thunder.quartznet.preprocess.FeatureBatchNormalizer","text":"","title":"FeatureBatchNormalizer"},{"location":"api/Quartznet/preprocess/#thunder.quartznet.preprocess.FeatureBatchNormalizer.__init__","text":"Normalize batch at the feature dimension. Source code in thunder/quartznet/preprocess.py def __init__ ( self ): \"\"\"Normalize batch at the feature dimension.\"\"\" super () . __init__ () self . div_guard = 1e-5","title":"__init__()"},{"location":"api/Quartznet/preprocess/#thunder.quartznet.preprocess.FeatureBatchNormalizer.forward","text":"Parameters: Name Type Description Default x Tensor Tensor of shape (batch, features, time) required Source code in thunder/quartznet/preprocess.py def forward ( self , x : torch . Tensor ) -> torch . Tensor : \"\"\" Args: x : Tensor of shape (batch, features, time) \"\"\" x_mean = x . mean ( dim = 2 , keepdim = True ) . detach () x_std = x . std ( dim = 2 , keepdim = True ) . detach () # make sure x_std is not zero x_std += self . div_guard return ( x - x_mean ) / x_std","title":"forward()"},{"location":"api/Quartznet/preprocess/#thunder.quartznet.preprocess.MelScale","text":"","title":"MelScale"},{"location":"api/Quartznet/preprocess/#thunder.quartznet.preprocess.MelScale.__init__","text":"Convert a spectrogram to Mel scale, following the default formula of librosa instead of the one used by torchaudio. Also converts to log scale. Parameters: Name Type Description Default sample_rate int Sampling rate of the signal required n_fft int Number of fourier features required nfilt int Number of output mel filters to use required log_scale bool Controls if the output should also be applied a log scale. True Source code in thunder/quartznet/preprocess.py def __init__ ( self , sample_rate : int , n_fft : int , nfilt : int , log_scale : bool = True ): \"\"\"Convert a spectrogram to Mel scale, following the default formula of librosa instead of the one used by torchaudio. Also converts to log scale. Args: sample_rate : Sampling rate of the signal n_fft : Number of fourier features nfilt : Number of output mel filters to use log_scale : Controls if the output should also be applied a log scale. \"\"\" super () . __init__ () filterbanks = ( create_fb_matrix ( int ( 1 + n_fft // 2 ), n_mels = nfilt , sample_rate = sample_rate , f_min = 0 , f_max = sample_rate / 2 , norm = \"slaney\" , htk = True , ) . transpose ( 0 , 1 ) . unsqueeze ( 0 ) ) self . register_buffer ( \"fb\" , filterbanks ) self . log_scale = log_scale","title":"__init__()"},{"location":"api/Quartznet/preprocess/#thunder.quartznet.preprocess.MelScale.forward","text":"Parameters: Name Type Description Default x Tensor Tensor of shape (batch, features, time) required Source code in thunder/quartznet/preprocess.py @torch . no_grad () def forward ( self , x : torch . Tensor ) -> torch . Tensor : \"\"\" Args: x : Tensor of shape (batch, features, time) \"\"\" # dot with filterbank energies x = torch . matmul ( self . fb . to ( x . dtype ), x ) # log features # We want to avoid taking the log of zero if self . log_scale : x = torch . log ( x + 2 ** - 24 ) return x","title":"forward()"},{"location":"api/Quartznet/preprocess/#thunder.quartznet.preprocess.PowerSpectrum","text":"","title":"PowerSpectrum"},{"location":"api/Quartznet/preprocess/#thunder.quartznet.preprocess.PowerSpectrum.__init__","text":"Calculates the power spectrum of the audio signal, following the same method as used in NEMO. Parameters: Name Type Description Default n_window_size int Number of elements in the window size. 320 n_window_stride int Number of elements in the window stride. 160 n_fft Optional[int] Number of fourier features. None Exceptions: Type Description ValueError Raised when incompatible parameters are passed. Source code in thunder/quartznet/preprocess.py def __init__ ( self , n_window_size : int = 320 , n_window_stride : int = 160 , n_fft : Optional [ int ] = None , ): \"\"\"Calculates the power spectrum of the audio signal, following the same method as used in NEMO. Args: n_window_size : Number of elements in the window size. n_window_stride : Number of elements in the window stride. n_fft : Number of fourier features. Raises: ValueError: Raised when incompatible parameters are passed. \"\"\" super () . __init__ () if n_window_size <= 0 or n_window_stride <= 0 : raise ValueError ( f \" { self } got an invalid value for either n_window_size or \" f \"n_window_stride. Both must be positive ints.\" ) self . win_length = n_window_size self . hop_length = n_window_stride self . n_fft = n_fft or 2 ** math . ceil ( math . log2 ( self . win_length )) window_tensor = torch . hann_window ( self . win_length , periodic = False ) self . register_buffer ( \"window\" , window_tensor )","title":"__init__()"},{"location":"api/Quartznet/preprocess/#thunder.quartznet.preprocess.PowerSpectrum.forward","text":"Parameters: Name Type Description Default x Tensor Tensor of shape (batch, time) required Source code in thunder/quartznet/preprocess.py @torch . no_grad () def forward ( self , x : torch . Tensor ) -> torch . Tensor : \"\"\" Args: x : Tensor of shape (batch, time) \"\"\" x = torch . stft ( x , n_fft = self . n_fft , hop_length = self . hop_length , win_length = self . win_length , center = True , window = self . window . to ( dtype = torch . float ), return_complex = False , ) # torch returns real, imag; so convert to magnitude x = torch . sqrt ( x . pow ( 2 ) . sum ( - 1 )) # get power spectrum x = x . pow ( 2.0 ) return x","title":"forward()"},{"location":"api/Quartznet/preprocess/#thunder.quartznet.preprocess.PreEmphasisFilter","text":"","title":"PreEmphasisFilter"},{"location":"api/Quartznet/preprocess/#thunder.quartznet.preprocess.PreEmphasisFilter.__init__","text":"Applies preemphasis filtering to the audio signal. This is a classic signal processing function to emphasise the high frequency portion of the content compared to the low frequency. It applies a FIR filter of the form: y[n] = y[n] - preemph * y[n-1] Parameters: Name Type Description Default preemph float Filter control factor. 0.97 Source code in thunder/quartznet/preprocess.py def __init__ ( self , preemph : float = 0.97 ): \"\"\"Applies preemphasis filtering to the audio signal. This is a classic signal processing function to emphasise the high frequency portion of the content compared to the low frequency. It applies a FIR filter of the form: `y[n] = y[n] - preemph * y[n-1]` Args: preemph : Filter control factor. \"\"\" super () . __init__ () self . preemph = preemph","title":"__init__()"},{"location":"api/Quartznet/preprocess/#thunder.quartznet.preprocess.PreEmphasisFilter.forward","text":"Parameters: Name Type Description Default x Tensor Tensor of shape (batch, time) required Source code in thunder/quartznet/preprocess.py @torch . no_grad () def forward ( self , x : torch . Tensor ) -> torch . Tensor : \"\"\" Args: x : Tensor of shape (batch, time) \"\"\" return torch . cat ( ( x [:, 0 ] . unsqueeze ( 1 ), x [:, 1 :] - self . preemph * x [:, : - 1 ]), dim = 1 )","title":"forward()"},{"location":"api/Quartznet/preprocess/#thunder.quartznet.preprocess.FilterbankFeatures","text":"Creates the Filterbank features used in the Quartznet model. Parameters: Name Type Description Default sample_rate int Sampling rate of the signal 16000 n_window_size int Number of elements in the window size. 320 n_window_stride int Number of elements in the window stride. 160 n_fft int Number of fourier features. 512 preemph float Preemphasis filtering control factor. 0.97 nfilt int Number of output mel filters to use 64 dither float Amount of dither to add. 1e-05 Returns: Type Description Module Module that computes the features based on raw audio tensor. Source code in thunder/quartznet/preprocess.py def FilterbankFeatures ( sample_rate : int = 16000 , n_window_size : int = 320 , n_window_stride : int = 160 , n_fft : int = 512 , preemph : float = 0.97 , nfilt : int = 64 , dither : float = 1e-5 , ** kwargs , ) -> nn . Module : \"\"\"Creates the Filterbank features used in the Quartznet model. Args: sample_rate : Sampling rate of the signal n_window_size : Number of elements in the window size. n_window_stride : Number of elements in the window stride. n_fft : Number of fourier features. preemph : Preemphasis filtering control factor. nfilt : Number of output mel filters to use dither : Amount of dither to add. Returns: Module that computes the features based on raw audio tensor. \"\"\" return nn . Sequential ( DitherAudio ( dither = dither ), PreEmphasisFilter ( preemph = preemph ), PowerSpectrum ( n_window_size = n_window_size , n_window_stride = n_window_stride , n_fft = n_fft , ), MelScale ( sample_rate = sample_rate , n_fft = n_fft , nfilt = nfilt ), FeatureBatchNormalizer (), )","title":"FilterbankFeatures()"},{"location":"api/Text%20Processing/preprocess/","text":"expand_numbers ( text , language = 'en' ) Expand the numbers present inside the text. That means converting \"42\" into \"forty two\". It also detects if the number is ordinal automatically. Parameters: Name Type Description Default text str Input text required language str Language used to expand the numbers. Defaults to \"en\". 'en' Returns: Type Description str Output text Source code in thunder/text_processing/preprocess.py def expand_numbers ( text : str , language : str = \"en\" ) -> str : \"\"\"Expand the numbers present inside the text. That means converting \"42\" into \"forty two\". It also detects if the number is ordinal automatically. Args: text : Input text language : Language used to expand the numbers. Defaults to \"en\". Returns: Output text \"\"\" number_regex = re . compile ( r \"\\d+\u00ba*\" ) all_numbers = number_regex . findall ( text ) for num in all_numbers : if \"\u00ba\" in num : pure_number = num . replace ( \"\u00ba\" , \"\" ) . strip () expanded = num2words ( int ( pure_number ), lang = language , to = \"ordinal\" ) else : expanded = num2words ( int ( num ), lang = language ) text = text . replace ( num , expanded ) return text lower_text ( text ) Transform all the text to lowercase. Parameters: Name Type Description Default text str Input text required Returns: Type Description str Output text Source code in thunder/text_processing/preprocess.py def lower_text ( text : str ) -> str : \"\"\"Transform all the text to lowercase. Args: text : Input text Returns: Output text \"\"\" return text . lower () normalize_text ( text ) Normalize the text to remove accents and ensure all the characters are valid ascii symbols. Parameters: Name Type Description Default text str Input text required Returns: Type Description str Output text Source code in thunder/text_processing/preprocess.py def normalize_text ( text : str ) -> str : \"\"\"Normalize the text to remove accents and ensure all the characters are valid ascii symbols. Args: text : Input text Returns: Output text \"\"\" nfkd_form = unicodedata . normalize ( \"NFKD\" , text ) only_ascii = nfkd_form . encode ( \"ASCII\" , \"ignore\" ) return only_ascii . decode ()","title":"Preprocess"},{"location":"api/Text%20Processing/preprocess/#thunder.text_processing.preprocess.expand_numbers","text":"Expand the numbers present inside the text. That means converting \"42\" into \"forty two\". It also detects if the number is ordinal automatically. Parameters: Name Type Description Default text str Input text required language str Language used to expand the numbers. Defaults to \"en\". 'en' Returns: Type Description str Output text Source code in thunder/text_processing/preprocess.py def expand_numbers ( text : str , language : str = \"en\" ) -> str : \"\"\"Expand the numbers present inside the text. That means converting \"42\" into \"forty two\". It also detects if the number is ordinal automatically. Args: text : Input text language : Language used to expand the numbers. Defaults to \"en\". Returns: Output text \"\"\" number_regex = re . compile ( r \"\\d+\u00ba*\" ) all_numbers = number_regex . findall ( text ) for num in all_numbers : if \"\u00ba\" in num : pure_number = num . replace ( \"\u00ba\" , \"\" ) . strip () expanded = num2words ( int ( pure_number ), lang = language , to = \"ordinal\" ) else : expanded = num2words ( int ( num ), lang = language ) text = text . replace ( num , expanded ) return text","title":"expand_numbers()"},{"location":"api/Text%20Processing/preprocess/#thunder.text_processing.preprocess.lower_text","text":"Transform all the text to lowercase. Parameters: Name Type Description Default text str Input text required Returns: Type Description str Output text Source code in thunder/text_processing/preprocess.py def lower_text ( text : str ) -> str : \"\"\"Transform all the text to lowercase. Args: text : Input text Returns: Output text \"\"\" return text . lower ()","title":"lower_text()"},{"location":"api/Text%20Processing/preprocess/#thunder.text_processing.preprocess.normalize_text","text":"Normalize the text to remove accents and ensure all the characters are valid ascii symbols. Parameters: Name Type Description Default text str Input text required Returns: Type Description str Output text Source code in thunder/text_processing/preprocess.py def normalize_text ( text : str ) -> str : \"\"\"Normalize the text to remove accents and ensure all the characters are valid ascii symbols. Args: text : Input text Returns: Output text \"\"\" nfkd_form = unicodedata . normalize ( \"NFKD\" , text ) only_ascii = nfkd_form . encode ( \"ASCII\" , \"ignore\" ) return only_ascii . decode ()","title":"normalize_text()"},{"location":"api/Text%20Processing/tokenize/","text":"char_tokenizer ( text ) Tokenize input text splitting into characters Parameters: Name Type Description Default text str Input text required Returns: Type Description List[str] Tokenized text Source code in thunder/text_processing/tokenizer.py def char_tokenizer ( text : str ) -> List [ str ]: \"\"\"Tokenize input text splitting into characters Args: text : Input text Returns: Tokenized text \"\"\" return list ( text ) word_tokenizer ( text ) Tokenize input text splitting into words Parameters: Name Type Description Default text str Input text required Returns: Type Description List[str] Tokenized text Source code in thunder/text_processing/tokenizer.py def word_tokenizer ( text : str ) -> List [ str ]: \"\"\"Tokenize input text splitting into words Args: text : Input text Returns: Tokenized text \"\"\" return text . split ()","title":"Tokenize"},{"location":"api/Text%20Processing/tokenize/#thunder.text_processing.tokenizer.char_tokenizer","text":"Tokenize input text splitting into characters Parameters: Name Type Description Default text str Input text required Returns: Type Description List[str] Tokenized text Source code in thunder/text_processing/tokenizer.py def char_tokenizer ( text : str ) -> List [ str ]: \"\"\"Tokenize input text splitting into characters Args: text : Input text Returns: Tokenized text \"\"\" return list ( text )","title":"char_tokenizer()"},{"location":"api/Text%20Processing/tokenize/#thunder.text_processing.tokenizer.word_tokenizer","text":"Tokenize input text splitting into words Parameters: Name Type Description Default text str Input text required Returns: Type Description List[str] Tokenized text Source code in thunder/text_processing/tokenizer.py def word_tokenizer ( text : str ) -> List [ str ]: \"\"\"Tokenize input text splitting into words Args: text : Input text Returns: Tokenized text \"\"\" return text . split ()","title":"word_tokenizer()"},{"location":"api/Text%20Processing/transform/","text":"BatchTextTransformer __init__ ( self , vocab , tokenize_func =< function char_tokenizer at 0x7f94c48279d0 > , preprocessing_transforms =< function chain_calls .< locals >. _inner at 0x7f94c482c1f0 > , after_tokenize = None , after_numericalize = None ) special That class is the glue code that uses all of the text processing stuff to encode an entire batch of text at once. Parameters: Name Type Description Default vocab Vocab Vocabulary to be used required tokenize_func Function that will perform the tokenization of each individual text sample. Defaults to char_tokenizer. <function char_tokenizer at 0x7f94c48279d0> preprocessing_transforms Functions that will be applied before tokenization, as the first step. Defaults to chain_calls(lower_text, normalize_text). <function chain_calls.<locals>._inner at 0x7f94c482c1f0> after_tokenize Functions to be applied after the tokenization but before numericalization. Defaults to None. None after_numericalize Functions to be applied at the end of the pipeline. Defaults to torch.LongTensor. None Source code in thunder/text_processing/transform.py def __init__ ( self , vocab : Vocab , tokenize_func = char_tokenizer , preprocessing_transforms = chain_calls ( lower_text , normalize_text ), after_tokenize = None , after_numericalize = None , ): \"\"\"That class is the glue code that uses all of the text processing stuff to encode an entire batch of text at once. Args: vocab : Vocabulary to be used tokenize_func : Function that will perform the tokenization of each individual text sample. Defaults to char_tokenizer. preprocessing_transforms : Functions that will be applied before tokenization, as the first step. Defaults to chain_calls(lower_text, normalize_text). after_tokenize : Functions to be applied after the tokenization but before numericalization. Defaults to None. after_numericalize : Functions to be applied at the end of the pipeline. Defaults to torch.LongTensor. \"\"\" super () . __init__ () self . vocab = vocab self . tokenize_func = tokenize_func self . preprocessing_transforms = preprocessing_transforms self . after_tokenize = after_tokenize self . after_numericalize = after_numericalize decode_prediction ( self , predictions ) Parameters: Name Type Description Default predictions Tensor Tensor of shape (batch, time) required Returns: Type Description List[str] A list of decoded strings, one for each element in the batch. Source code in thunder/text_processing/transform.py @torch . jit . export def decode_prediction ( self , predictions : torch . Tensor ) -> List [ str ]: \"\"\" Args: predictions : Tensor of shape (batch, time) Returns: A list of decoded strings, one for each element in the batch. \"\"\" out_list : List [ str ] = [] for element in predictions : # Remove consecutive repeated elements element = torch . unique_consecutive ( element ) # Map back to string out = self . vocab . decode_into_text ( element ) # Join prediction into one string out = \"\" . join ( out ) # Remove the blank and pad token from output out = out . replace ( self . vocab . blank_token , \"\" ) out = out . replace ( self . vocab . pad_token , \"\" ) out_list . append ( out ) return out_list","title":"Transform"},{"location":"api/Text%20Processing/transform/#thunder.text_processing.transform.BatchTextTransformer","text":"","title":"BatchTextTransformer"},{"location":"api/Text%20Processing/transform/#thunder.text_processing.transform.BatchTextTransformer.__init__","text":"That class is the glue code that uses all of the text processing stuff to encode an entire batch of text at once. Parameters: Name Type Description Default vocab Vocab Vocabulary to be used required tokenize_func Function that will perform the tokenization of each individual text sample. Defaults to char_tokenizer. <function char_tokenizer at 0x7f94c48279d0> preprocessing_transforms Functions that will be applied before tokenization, as the first step. Defaults to chain_calls(lower_text, normalize_text). <function chain_calls.<locals>._inner at 0x7f94c482c1f0> after_tokenize Functions to be applied after the tokenization but before numericalization. Defaults to None. None after_numericalize Functions to be applied at the end of the pipeline. Defaults to torch.LongTensor. None Source code in thunder/text_processing/transform.py def __init__ ( self , vocab : Vocab , tokenize_func = char_tokenizer , preprocessing_transforms = chain_calls ( lower_text , normalize_text ), after_tokenize = None , after_numericalize = None , ): \"\"\"That class is the glue code that uses all of the text processing stuff to encode an entire batch of text at once. Args: vocab : Vocabulary to be used tokenize_func : Function that will perform the tokenization of each individual text sample. Defaults to char_tokenizer. preprocessing_transforms : Functions that will be applied before tokenization, as the first step. Defaults to chain_calls(lower_text, normalize_text). after_tokenize : Functions to be applied after the tokenization but before numericalization. Defaults to None. after_numericalize : Functions to be applied at the end of the pipeline. Defaults to torch.LongTensor. \"\"\" super () . __init__ () self . vocab = vocab self . tokenize_func = tokenize_func self . preprocessing_transforms = preprocessing_transforms self . after_tokenize = after_tokenize self . after_numericalize = after_numericalize","title":"__init__()"},{"location":"api/Text%20Processing/transform/#thunder.text_processing.transform.BatchTextTransformer.decode_prediction","text":"Parameters: Name Type Description Default predictions Tensor Tensor of shape (batch, time) required Returns: Type Description List[str] A list of decoded strings, one for each element in the batch. Source code in thunder/text_processing/transform.py @torch . jit . export def decode_prediction ( self , predictions : torch . Tensor ) -> List [ str ]: \"\"\" Args: predictions : Tensor of shape (batch, time) Returns: A list of decoded strings, one for each element in the batch. \"\"\" out_list : List [ str ] = [] for element in predictions : # Remove consecutive repeated elements element = torch . unique_consecutive ( element ) # Map back to string out = self . vocab . decode_into_text ( element ) # Join prediction into one string out = \"\" . join ( out ) # Remove the blank and pad token from output out = out . replace ( self . vocab . blank_token , \"\" ) out = out . replace ( self . vocab . pad_token , \"\" ) out_list . append ( out ) return out_list","title":"decode_prediction()"},{"location":"api/Text%20Processing/vocab/","text":"Vocab __init__ ( self , initial_vocab_tokens , pad_token = '<pad>' , unknown_token = '<unk>' , start_token = '<bos>' , end_token = '<eos>' , nemo_compat = False ) special Class that represents a vocabulary, with the related methods to numericalize a sequence of tokens into numbers, and do the reverse mapping of numbers back to tokens. Parameters: Name Type Description Default initial_vocab_tokens List[str] Basic list of tokens that will be part of the vocabulary. DO NOT INCLUDE SPECIAL TOKENS THERE. Even the blank is automatically added by the class. required pad_token str Token that will represent padding. '<pad>' unknown_token str Token that will represent unknown elements. Notice that this is different than the blank used by ctc. '<unk>' start_token str Token that will represent the beginning of the sequence. '<bos>' end_token str Token that will represent the end of the sequence. '<eos>' nemo_compat bool Compatibility mode to work with original Nemo models. False Source code in thunder/text_processing/vocab.py def __init__ ( self , initial_vocab_tokens : List [ str ], pad_token : str = \"<pad>\" , unknown_token : str = \"<unk>\" , start_token : str = \"<bos>\" , end_token : str = \"<eos>\" , nemo_compat : bool = False , ): \"\"\"Class that represents a vocabulary, with the related methods to numericalize a sequence of tokens into numbers, and do the reverse mapping of numbers back to tokens. Args: initial_vocab_tokens : Basic list of tokens that will be part of the vocabulary. DO NOT INCLUDE SPECIAL TOKENS THERE. Even the blank is automatically added by the class. pad_token : Token that will represent padding. unknown_token : Token that will represent unknown elements. Notice that this is different than the blank used by ctc. start_token : Token that will represent the beginning of the sequence. end_token : Token that will represent the end of the sequence. nemo_compat: Compatibility mode to work with original Nemo models. \"\"\" super () . __init__ () self . pad_token = pad_token self . unknown_token = unknown_token self . start_token = start_token self . end_token = end_token # There's no problem if the blank_idx == pad_idx self . blank_token = self . pad_token self . nemo_compat = nemo_compat self . itos = initial_vocab_tokens + [ pad_token , unknown_token , start_token , end_token , ] self . stoi = { token : i for i , token in enumerate ( self . itos )} self . update_special_idx () add_special_tokens ( self , tokens ) Function to add the special start and end tokens to some tokenized text. Parameters: Name Type Description Default tokens List[str] Tokenized text required Returns: Type Description List[str] Text with the special tokens added. Source code in thunder/text_processing/vocab.py @torch . jit . export def add_special_tokens ( self , tokens : List [ str ]) -> List [ str ]: \"\"\"Function to add the special start and end tokens to some tokenized text. Args: tokens : Tokenized text Returns: Text with the special tokens added. \"\"\" if self . nemo_compat : return tokens return [ self . start_token ] + tokens + [ self . end_token ] decode_into_text ( self , indices ) Function to transform back a list of numbers into the corresponding tokens. Parameters: Name Type Description Default indices Tensor Numeric representation. Usually is the result of the model, after a greedy decoding required Returns: Type Description List[str] Corresponding tokens Source code in thunder/text_processing/vocab.py @torch . jit . export def decode_into_text ( self , indices : torch . Tensor ) -> List [ str ]: \"\"\"Function to transform back a list of numbers into the corresponding tokens. Args: indices : Numeric representation. Usually is the result of the model, after a greedy decoding Returns: Corresponding tokens \"\"\" return [ self . itos [ it ] for it in indices ] numericalize ( self , tokens ) Function to transform a list of tokens into the corresponding numeric representation. Parameters: Name Type Description Default tokens List[str] A single list of tokens to be transformed required Returns: Type Description Tensor The corresponding numeric representation Source code in thunder/text_processing/vocab.py def numericalize ( self , tokens : List [ str ]) -> torch . Tensor : \"\"\"Function to transform a list of tokens into the corresponding numeric representation. Args: tokens : A single list of tokens to be transformed Returns: The corresponding numeric representation \"\"\" if self . nemo_compat : # When in nemo_compat mode, there's no unknown token # So we filter out all of the tokens not in the vocab tokens = filter ( lambda x : x in self . itos , tokens ) return torch . tensor ( [ self . stoi . get ( it , self . unknown_idx ) for it in tokens ], dtype = torch . long )","title":"Vocab"},{"location":"api/Text%20Processing/vocab/#thunder.text_processing.vocab.Vocab","text":"","title":"Vocab"},{"location":"api/Text%20Processing/vocab/#thunder.text_processing.vocab.Vocab.__init__","text":"Class that represents a vocabulary, with the related methods to numericalize a sequence of tokens into numbers, and do the reverse mapping of numbers back to tokens. Parameters: Name Type Description Default initial_vocab_tokens List[str] Basic list of tokens that will be part of the vocabulary. DO NOT INCLUDE SPECIAL TOKENS THERE. Even the blank is automatically added by the class. required pad_token str Token that will represent padding. '<pad>' unknown_token str Token that will represent unknown elements. Notice that this is different than the blank used by ctc. '<unk>' start_token str Token that will represent the beginning of the sequence. '<bos>' end_token str Token that will represent the end of the sequence. '<eos>' nemo_compat bool Compatibility mode to work with original Nemo models. False Source code in thunder/text_processing/vocab.py def __init__ ( self , initial_vocab_tokens : List [ str ], pad_token : str = \"<pad>\" , unknown_token : str = \"<unk>\" , start_token : str = \"<bos>\" , end_token : str = \"<eos>\" , nemo_compat : bool = False , ): \"\"\"Class that represents a vocabulary, with the related methods to numericalize a sequence of tokens into numbers, and do the reverse mapping of numbers back to tokens. Args: initial_vocab_tokens : Basic list of tokens that will be part of the vocabulary. DO NOT INCLUDE SPECIAL TOKENS THERE. Even the blank is automatically added by the class. pad_token : Token that will represent padding. unknown_token : Token that will represent unknown elements. Notice that this is different than the blank used by ctc. start_token : Token that will represent the beginning of the sequence. end_token : Token that will represent the end of the sequence. nemo_compat: Compatibility mode to work with original Nemo models. \"\"\" super () . __init__ () self . pad_token = pad_token self . unknown_token = unknown_token self . start_token = start_token self . end_token = end_token # There's no problem if the blank_idx == pad_idx self . blank_token = self . pad_token self . nemo_compat = nemo_compat self . itos = initial_vocab_tokens + [ pad_token , unknown_token , start_token , end_token , ] self . stoi = { token : i for i , token in enumerate ( self . itos )} self . update_special_idx ()","title":"__init__()"},{"location":"api/Text%20Processing/vocab/#thunder.text_processing.vocab.Vocab.add_special_tokens","text":"Function to add the special start and end tokens to some tokenized text. Parameters: Name Type Description Default tokens List[str] Tokenized text required Returns: Type Description List[str] Text with the special tokens added. Source code in thunder/text_processing/vocab.py @torch . jit . export def add_special_tokens ( self , tokens : List [ str ]) -> List [ str ]: \"\"\"Function to add the special start and end tokens to some tokenized text. Args: tokens : Tokenized text Returns: Text with the special tokens added. \"\"\" if self . nemo_compat : return tokens return [ self . start_token ] + tokens + [ self . end_token ]","title":"add_special_tokens()"},{"location":"api/Text%20Processing/vocab/#thunder.text_processing.vocab.Vocab.decode_into_text","text":"Function to transform back a list of numbers into the corresponding tokens. Parameters: Name Type Description Default indices Tensor Numeric representation. Usually is the result of the model, after a greedy decoding required Returns: Type Description List[str] Corresponding tokens Source code in thunder/text_processing/vocab.py @torch . jit . export def decode_into_text ( self , indices : torch . Tensor ) -> List [ str ]: \"\"\"Function to transform back a list of numbers into the corresponding tokens. Args: indices : Numeric representation. Usually is the result of the model, after a greedy decoding Returns: Corresponding tokens \"\"\" return [ self . itos [ it ] for it in indices ]","title":"decode_into_text()"},{"location":"api/Text%20Processing/vocab/#thunder.text_processing.vocab.Vocab.numericalize","text":"Function to transform a list of tokens into the corresponding numeric representation. Parameters: Name Type Description Default tokens List[str] A single list of tokens to be transformed required Returns: Type Description Tensor The corresponding numeric representation Source code in thunder/text_processing/vocab.py def numericalize ( self , tokens : List [ str ]) -> torch . Tensor : \"\"\"Function to transform a list of tokens into the corresponding numeric representation. Args: tokens : A single list of tokens to be transformed Returns: The corresponding numeric representation \"\"\" if self . nemo_compat : # When in nemo_compat mode, there's no unknown token # So we filter out all of the tokens not in the vocab tokens = filter ( lambda x : x in self . itos , tokens ) return torch . tensor ( [ self . stoi . get ( it , self . unknown_idx ) for it in tokens ], dtype = torch . long )","title":"numericalize()"}]}