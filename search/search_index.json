{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Thunder speech A Hackable speech recognition library. Note This project has been set up using PyScaffold 3.3. For details and usage information on PyScaffold see https://pyscaffold.org/.","title":"Home"},{"location":"#thunder-speech","text":"A Hackable speech recognition library.","title":"Thunder speech"},{"location":"#note","text":"This project has been set up using PyScaffold 3.3. For details and usage information on PyScaffold see https://pyscaffold.org/.","title":"Note"},{"location":"api/jasper/blocks/","text":"GroupShuffle forward ( self , x ) Defines the computation performed at every call. Should be overridden by all subclasses. .. note:: Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them. Source code in thunder/jasper/blocks.py def forward ( self , x ): sh = x . shape x = x . view ( - 1 , self . groups , self . channels_per_group , sh [ - 1 ]) x = torch . transpose ( x , 1 , 2 ) . contiguous () x = x . view ( - 1 , self . groups * self . channels_per_group , sh [ - 1 ]) return x JasperBlock forward ( self , input_ ) Defines the computation performed at every call. Should be overridden by all subclasses. .. note:: Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them. Source code in thunder/jasper/blocks.py def forward ( self , input_ : Tuple [ List [ Tensor ], Optional [ Tensor ]]): # type: (Tuple[List[Tensor], Optional[Tensor]]) -> Tuple[List[Tensor], Optional[Tensor]] # nopep8 lens_orig = None xs = input_ [ 0 ] if len ( input_ ) == 2 : xs , lens_orig = input_ # compute forward convolutions out = xs [ - 1 ] lens = lens_orig for i , l in enumerate ( self . mconv ): # if we're doing masked convolutions, we need to pass in and # possibly update the sequence lengths # if (i % 4) == 0 and self.conv_mask: if isinstance ( l , MaskedConv1d ): out , lens = l ( out , lens ) else : out = l ( out ) # compute the residuals if self . res is not None : for i , layer in enumerate ( self . res ): res_out = xs [ i ] for j , res_layer in enumerate ( layer ): if isinstance ( res_layer , MaskedConv1d ): res_out , _ = res_layer ( res_out , lens_orig ) else : res_out = res_layer ( res_out ) if self . residual_mode == \"add\" or self . residual_mode == \"stride_add\" : out = out + res_out else : out = torch . max ( out , res_out ) # compute the output out = self . mout ( out ) if self . res is not None and self . dense_residual : return xs + [ out ], lens return [ out ], lens MaskedConv1d forward ( self , x , lens ) Defines the computation performed at every call. Should be overridden by all subclasses. .. note:: Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them. Source code in thunder/jasper/blocks.py def forward ( self , x , lens ): if self . use_mask : lens = lens . to ( dtype = torch . long ) max_len = x . size ( 2 ) mask = torch . arange ( max_len ) . to ( lens . device ) . expand ( len ( lens ), max_len ) >= lens . unsqueeze ( 1 ) x = x . masked_fill ( mask . unsqueeze ( 1 ) . to ( device = x . device ), 0 ) # del mask lens = self . get_seq_len ( lens ) sh = x . shape if self . heads != - 1 : x = x . view ( - 1 , self . heads , sh [ - 1 ]) out = self . conv ( x ) if self . heads != - 1 : out = out . view ( sh [ 0 ], self . real_out_channels , - 1 ) return out , lens SqueezeExcite __init__ ( self , channels , reduction_ratio , context_window =- 1 , interpolation_mode = 'nearest' , activation = None ) special Squeeze-and-Excitation sub-module. Parameters: Name Type Description Default channels int Input number of channels. required reduction_ratio int Reduction ratio for \"squeeze\" layer. required context_window int Integer number of timesteps that the context should be computed over, using stride 1 average pooling. If value < 1, then global context is computed. -1 interpolation_mode str Interpolation mode of timestep dimension. Used only if context window is > 1. The modes available for resizing are: nearest , linear (3D-only), bilinear , area 'nearest' activation Optional[Callable] Intermediate activation function used. Must be a callable activation function. None Source code in thunder/jasper/blocks.py def __init__ ( self , channels : int , reduction_ratio : int , context_window : int = - 1 , interpolation_mode : str = \"nearest\" , activation : Optional [ Callable ] = None , ): \"\"\" Squeeze-and-Excitation sub-module. Args: channels: Input number of channels. reduction_ratio: Reduction ratio for \"squeeze\" layer. context_window: Integer number of timesteps that the context should be computed over, using stride 1 average pooling. If value < 1, then global context is computed. interpolation_mode: Interpolation mode of timestep dimension. Used only if context window is > 1. The modes available for resizing are: `nearest`, `linear` (3D-only), `bilinear`, `area` activation: Intermediate activation function used. Must be a callable activation function. \"\"\" super ( SqueezeExcite , self ) . __init__ () self . context_window = int ( context_window ) self . interpolation_mode = interpolation_mode if self . context_window <= 0 : self . pool = nn . AdaptiveAvgPool1d ( 1 ) # context window = T else : self . pool = nn . AvgPool1d ( self . context_window , stride = 1 ) if activation is None : activation = nn . ReLU ( inplace = True ) self . fc = nn . Sequential ( nn . Linear ( channels , channels // reduction_ratio , bias = False ), activation , nn . Linear ( channels // reduction_ratio , channels , bias = False ), ) forward ( self , x ) Defines the computation performed at every call. Should be overridden by all subclasses. .. note:: Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them. Source code in thunder/jasper/blocks.py def forward ( self , x ): # The use of negative indices on the transpose allow for expanded SqueezeExcite batch , channels , timesteps = x . size ()[: 3 ] y = self . pool ( x ) # [B, C, T - context_window + 1] y = y . transpose ( 1 , - 1 ) # [B, T - context_window + 1, C] y = self . fc ( y ) # [B, T - context_window + 1, C] y = y . transpose ( 1 , - 1 ) # [B, C, T - context_window + 1] if self . context_window > 0 : y = torch . nn . functional . interpolate ( y , size = timesteps , mode = self . interpolation_mode ) y = torch . sigmoid ( y ) return x * y StatsPoolLayer forward ( self , encoder_output ) Defines the computation performed at every call. Should be overridden by all subclasses. .. note:: Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them. Source code in thunder/jasper/blocks.py def forward ( self , encoder_output ): mean = encoder_output . mean ( dim =- 1 ) # Time Axis std = encoder_output . std ( dim =- 1 ) pooled = torch . cat ([ mean , std ], dim =- 1 ) if self . gram : time_len = encoder_output . shape [ - 1 ] # encoder_output = encoder_output cov = encoder_output . bmm ( encoder_output . transpose ( 2 , 1 )) # cov matrix cov = cov . view ( cov . shape [ 0 ], - 1 ) / time_len if self . gram and not self . super : return cov if self . super and self . gram : pooled = torch . cat ([ pooled , cov ], dim =- 1 ) return pooled","title":"Blocks"},{"location":"api/jasper/blocks/#thunder.jasper.blocks","text":"","title":"thunder.jasper.blocks"},{"location":"api/jasper/blocks/#thunder.jasper.blocks.GroupShuffle","text":"","title":"GroupShuffle"},{"location":"api/jasper/blocks/#thunder.jasper.blocks.GroupShuffle.forward","text":"Defines the computation performed at every call. Should be overridden by all subclasses. .. note:: Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them. Source code in thunder/jasper/blocks.py def forward ( self , x ): sh = x . shape x = x . view ( - 1 , self . groups , self . channels_per_group , sh [ - 1 ]) x = torch . transpose ( x , 1 , 2 ) . contiguous () x = x . view ( - 1 , self . groups * self . channels_per_group , sh [ - 1 ]) return x","title":"forward()"},{"location":"api/jasper/blocks/#thunder.jasper.blocks.JasperBlock","text":"","title":"JasperBlock"},{"location":"api/jasper/blocks/#thunder.jasper.blocks.JasperBlock.forward","text":"Defines the computation performed at every call. Should be overridden by all subclasses. .. note:: Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them. Source code in thunder/jasper/blocks.py def forward ( self , input_ : Tuple [ List [ Tensor ], Optional [ Tensor ]]): # type: (Tuple[List[Tensor], Optional[Tensor]]) -> Tuple[List[Tensor], Optional[Tensor]] # nopep8 lens_orig = None xs = input_ [ 0 ] if len ( input_ ) == 2 : xs , lens_orig = input_ # compute forward convolutions out = xs [ - 1 ] lens = lens_orig for i , l in enumerate ( self . mconv ): # if we're doing masked convolutions, we need to pass in and # possibly update the sequence lengths # if (i % 4) == 0 and self.conv_mask: if isinstance ( l , MaskedConv1d ): out , lens = l ( out , lens ) else : out = l ( out ) # compute the residuals if self . res is not None : for i , layer in enumerate ( self . res ): res_out = xs [ i ] for j , res_layer in enumerate ( layer ): if isinstance ( res_layer , MaskedConv1d ): res_out , _ = res_layer ( res_out , lens_orig ) else : res_out = res_layer ( res_out ) if self . residual_mode == \"add\" or self . residual_mode == \"stride_add\" : out = out + res_out else : out = torch . max ( out , res_out ) # compute the output out = self . mout ( out ) if self . res is not None and self . dense_residual : return xs + [ out ], lens return [ out ], lens","title":"forward()"},{"location":"api/jasper/blocks/#thunder.jasper.blocks.MaskedConv1d","text":"","title":"MaskedConv1d"},{"location":"api/jasper/blocks/#thunder.jasper.blocks.MaskedConv1d.forward","text":"Defines the computation performed at every call. Should be overridden by all subclasses. .. note:: Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them. Source code in thunder/jasper/blocks.py def forward ( self , x , lens ): if self . use_mask : lens = lens . to ( dtype = torch . long ) max_len = x . size ( 2 ) mask = torch . arange ( max_len ) . to ( lens . device ) . expand ( len ( lens ), max_len ) >= lens . unsqueeze ( 1 ) x = x . masked_fill ( mask . unsqueeze ( 1 ) . to ( device = x . device ), 0 ) # del mask lens = self . get_seq_len ( lens ) sh = x . shape if self . heads != - 1 : x = x . view ( - 1 , self . heads , sh [ - 1 ]) out = self . conv ( x ) if self . heads != - 1 : out = out . view ( sh [ 0 ], self . real_out_channels , - 1 ) return out , lens","title":"forward()"},{"location":"api/jasper/blocks/#thunder.jasper.blocks.SqueezeExcite","text":"","title":"SqueezeExcite"},{"location":"api/jasper/blocks/#thunder.jasper.blocks.SqueezeExcite.__init__","text":"Squeeze-and-Excitation sub-module. Parameters: Name Type Description Default channels int Input number of channels. required reduction_ratio int Reduction ratio for \"squeeze\" layer. required context_window int Integer number of timesteps that the context should be computed over, using stride 1 average pooling. If value < 1, then global context is computed. -1 interpolation_mode str Interpolation mode of timestep dimension. Used only if context window is > 1. The modes available for resizing are: nearest , linear (3D-only), bilinear , area 'nearest' activation Optional[Callable] Intermediate activation function used. Must be a callable activation function. None Source code in thunder/jasper/blocks.py def __init__ ( self , channels : int , reduction_ratio : int , context_window : int = - 1 , interpolation_mode : str = \"nearest\" , activation : Optional [ Callable ] = None , ): \"\"\" Squeeze-and-Excitation sub-module. Args: channels: Input number of channels. reduction_ratio: Reduction ratio for \"squeeze\" layer. context_window: Integer number of timesteps that the context should be computed over, using stride 1 average pooling. If value < 1, then global context is computed. interpolation_mode: Interpolation mode of timestep dimension. Used only if context window is > 1. The modes available for resizing are: `nearest`, `linear` (3D-only), `bilinear`, `area` activation: Intermediate activation function used. Must be a callable activation function. \"\"\" super ( SqueezeExcite , self ) . __init__ () self . context_window = int ( context_window ) self . interpolation_mode = interpolation_mode if self . context_window <= 0 : self . pool = nn . AdaptiveAvgPool1d ( 1 ) # context window = T else : self . pool = nn . AvgPool1d ( self . context_window , stride = 1 ) if activation is None : activation = nn . ReLU ( inplace = True ) self . fc = nn . Sequential ( nn . Linear ( channels , channels // reduction_ratio , bias = False ), activation , nn . Linear ( channels // reduction_ratio , channels , bias = False ), )","title":"__init__()"},{"location":"api/jasper/blocks/#thunder.jasper.blocks.SqueezeExcite.forward","text":"Defines the computation performed at every call. Should be overridden by all subclasses. .. note:: Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them. Source code in thunder/jasper/blocks.py def forward ( self , x ): # The use of negative indices on the transpose allow for expanded SqueezeExcite batch , channels , timesteps = x . size ()[: 3 ] y = self . pool ( x ) # [B, C, T - context_window + 1] y = y . transpose ( 1 , - 1 ) # [B, T - context_window + 1, C] y = self . fc ( y ) # [B, T - context_window + 1, C] y = y . transpose ( 1 , - 1 ) # [B, C, T - context_window + 1] if self . context_window > 0 : y = torch . nn . functional . interpolate ( y , size = timesteps , mode = self . interpolation_mode ) y = torch . sigmoid ( y ) return x * y","title":"forward()"},{"location":"api/jasper/blocks/#thunder.jasper.blocks.StatsPoolLayer","text":"","title":"StatsPoolLayer"},{"location":"api/jasper/blocks/#thunder.jasper.blocks.StatsPoolLayer.forward","text":"Defines the computation performed at every call. Should be overridden by all subclasses. .. note:: Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them. Source code in thunder/jasper/blocks.py def forward ( self , encoder_output ): mean = encoder_output . mean ( dim =- 1 ) # Time Axis std = encoder_output . std ( dim =- 1 ) pooled = torch . cat ([ mean , std ], dim =- 1 ) if self . gram : time_len = encoder_output . shape [ - 1 ] # encoder_output = encoder_output cov = encoder_output . bmm ( encoder_output . transpose ( 2 , 1 )) # cov matrix cov = cov . view ( cov . shape [ 0 ], - 1 ) / time_len if self . gram and not self . super : return cov if self . super and self . gram : pooled = torch . cat ([ pooled , cov ], dim =- 1 ) return pooled","title":"forward()"},{"location":"api/jasper/model/","text":"All of the stuff to load the Jasper checkpoint get_jasper ( name , checkpoint_folder ) Get Jasper model by idenfitier. This method downloads the checkpoint, creates the corresponding model and load the weights. Parameters: Name Type Description Default name str Model idenfitier. Check checkpoint_archives.keys() required checkpoint_folder str Folder where the checkpoint will be saved to. required Returns: Type Description Tuple[torch.nn.modules.module.Module, torch.nn.modules.module.Module] Encoder and decoder Modules with the checkpoint weights loaded Source code in thunder/jasper/model.py def get_jasper ( name : str , checkpoint_folder : str ) -> Tuple [ nn . Module , nn . Module ]: \"\"\"Get Jasper model by idenfitier. This method downloads the checkpoint, creates the corresponding model and load the weights. Args: name: Model idenfitier. Check checkpoint_archives.keys() checkpoint_folder: Folder where the checkpoint will be saved to. Returns: Encoder and decoder Modules with the checkpoint weights loaded \"\"\" url = checkpoint_archives [ name ] download_url ( url , download_folder = checkpoint_folder , resume = True , ) filename = url . split ( \"/\" )[ - 1 ] checkpoint_path = Path ( checkpoint_folder ) / filename with TemporaryDirectory () as extract_path : extract_path = Path ( extract_path ) extract_archive ( str ( checkpoint_path ), extract_path ) encoder , decoder = load_jasper_weights ( extract_path / \"model_config.yaml\" , extract_path / \"model_weights.ckpt\" ) return encoder , decoder load_jasper_weights ( config_path , weights_path ) Load Jasper/Quartznet model from data present inside .nemo file Returns: Type Description Tuple[torch.nn.modules.module.Module, torch.nn.modules.module.Module] Encoder and decoder Modules with the checkpoint weights loaded Source code in thunder/jasper/model.py def load_jasper_weights ( config_path : str , weights_path : str ) -> Tuple [ nn . Module , nn . Module ]: \"\"\"Load Jasper/Quartznet model from data present inside .nemo file Returns: Encoder and decoder Modules with the checkpoint weights loaded \"\"\" encoder , decoder = read_config ( config_path ) weights = torch . load ( weights_path ) # We remove the 'encoder.' and 'decoder.' prefix from the weights to enable # compatibility to load with plain nn.Modules created by reading the config encoder_weights = { k . replace ( \"encoder.\" , \"\" ): v for k , v in weights . items () if \"encoder\" in k } encoder . load_state_dict ( encoder_weights , strict = True ) decoder_weights = { k . replace ( \"decoder.decoder_layers.\" , \"\" ): v for k , v in weights . items () if \"decoder\" in k } decoder . load_state_dict ( decoder_weights , strict = True ) return encoder , decoder read_config ( config_path ) Read .yaml config and creates the encoder and decoder modules Parameters: Name Type Description Default config_path str Hydra config describing the Jasper/Quartznet model required Returns: Type Description Tuple[torch.nn.modules.module.Module, torch.nn.modules.module.Module] Encoder and decoder Modules randomly initializated Source code in thunder/jasper/model.py def read_config ( config_path : str ) -> Tuple [ nn . Module , nn . Module ]: \"\"\"Read .yaml config and creates the encoder and decoder modules Args: config_path: Hydra config describing the Jasper/Quartznet model Returns: Encoder and decoder Modules randomly initializated \"\"\" conf = OmegaConf . load ( config_path ) encoder_params = conf [ \"encoder\" ][ \"params\" ] inplanes = encoder_params [ \"feat_in\" ] * encoder_params . get ( \"frame_splicing\" , 1 ) jasper_conf = OmegaConf . to_container ( encoder_params [ \"jasper\" ]) activation = jasper_activations [ encoder_params [ \"activation\" ]]() residual_panes = [] layers = [] for cfg in jasper_conf : if cfg . pop ( \"residual_dense\" , False ): residual_panes . append ( inplanes ) cfg [ \"conv_mask\" ] = encoder_params [ \"conv_mask\" ] cfg [ \"planes\" ] = cfg . pop ( \"filters\" ) cfg [ \"kernel_size\" ] = cfg . pop ( \"kernel\" ) layers . append ( JasperBlock ( inplanes = inplanes , activation = activation , residual_panes = residual_panes , ** cfg ) ) inplanes = cfg [ \"planes\" ] encoder = nn . Sequential ( * layers ) encoder . apply ( init_weights ) decoder_params = conf [ \"decoder\" ][ \"params\" ] decoder = torch . nn . Sequential ( torch . nn . Conv1d ( decoder_params [ \"feat_in\" ], decoder_params [ \"num_classes\" ] + 1 , kernel_size = 1 , bias = True , ) ) decoder . apply ( init_weights ) return encoder , decoder","title":"Model"},{"location":"api/jasper/model/#thunder.jasper.model","text":"All of the stuff to load the Jasper checkpoint","title":"thunder.jasper.model"},{"location":"api/jasper/model/#thunder.jasper.model.get_jasper","text":"Get Jasper model by idenfitier. This method downloads the checkpoint, creates the corresponding model and load the weights. Parameters: Name Type Description Default name str Model idenfitier. Check checkpoint_archives.keys() required checkpoint_folder str Folder where the checkpoint will be saved to. required Returns: Type Description Tuple[torch.nn.modules.module.Module, torch.nn.modules.module.Module] Encoder and decoder Modules with the checkpoint weights loaded Source code in thunder/jasper/model.py def get_jasper ( name : str , checkpoint_folder : str ) -> Tuple [ nn . Module , nn . Module ]: \"\"\"Get Jasper model by idenfitier. This method downloads the checkpoint, creates the corresponding model and load the weights. Args: name: Model idenfitier. Check checkpoint_archives.keys() checkpoint_folder: Folder where the checkpoint will be saved to. Returns: Encoder and decoder Modules with the checkpoint weights loaded \"\"\" url = checkpoint_archives [ name ] download_url ( url , download_folder = checkpoint_folder , resume = True , ) filename = url . split ( \"/\" )[ - 1 ] checkpoint_path = Path ( checkpoint_folder ) / filename with TemporaryDirectory () as extract_path : extract_path = Path ( extract_path ) extract_archive ( str ( checkpoint_path ), extract_path ) encoder , decoder = load_jasper_weights ( extract_path / \"model_config.yaml\" , extract_path / \"model_weights.ckpt\" ) return encoder , decoder","title":"get_jasper()"},{"location":"api/jasper/model/#thunder.jasper.model.load_jasper_weights","text":"Load Jasper/Quartznet model from data present inside .nemo file Returns: Type Description Tuple[torch.nn.modules.module.Module, torch.nn.modules.module.Module] Encoder and decoder Modules with the checkpoint weights loaded Source code in thunder/jasper/model.py def load_jasper_weights ( config_path : str , weights_path : str ) -> Tuple [ nn . Module , nn . Module ]: \"\"\"Load Jasper/Quartznet model from data present inside .nemo file Returns: Encoder and decoder Modules with the checkpoint weights loaded \"\"\" encoder , decoder = read_config ( config_path ) weights = torch . load ( weights_path ) # We remove the 'encoder.' and 'decoder.' prefix from the weights to enable # compatibility to load with plain nn.Modules created by reading the config encoder_weights = { k . replace ( \"encoder.\" , \"\" ): v for k , v in weights . items () if \"encoder\" in k } encoder . load_state_dict ( encoder_weights , strict = True ) decoder_weights = { k . replace ( \"decoder.decoder_layers.\" , \"\" ): v for k , v in weights . items () if \"decoder\" in k } decoder . load_state_dict ( decoder_weights , strict = True ) return encoder , decoder","title":"load_jasper_weights()"},{"location":"api/jasper/model/#thunder.jasper.model.read_config","text":"Read .yaml config and creates the encoder and decoder modules Parameters: Name Type Description Default config_path str Hydra config describing the Jasper/Quartznet model required Returns: Type Description Tuple[torch.nn.modules.module.Module, torch.nn.modules.module.Module] Encoder and decoder Modules randomly initializated Source code in thunder/jasper/model.py def read_config ( config_path : str ) -> Tuple [ nn . Module , nn . Module ]: \"\"\"Read .yaml config and creates the encoder and decoder modules Args: config_path: Hydra config describing the Jasper/Quartznet model Returns: Encoder and decoder Modules randomly initializated \"\"\" conf = OmegaConf . load ( config_path ) encoder_params = conf [ \"encoder\" ][ \"params\" ] inplanes = encoder_params [ \"feat_in\" ] * encoder_params . get ( \"frame_splicing\" , 1 ) jasper_conf = OmegaConf . to_container ( encoder_params [ \"jasper\" ]) activation = jasper_activations [ encoder_params [ \"activation\" ]]() residual_panes = [] layers = [] for cfg in jasper_conf : if cfg . pop ( \"residual_dense\" , False ): residual_panes . append ( inplanes ) cfg [ \"conv_mask\" ] = encoder_params [ \"conv_mask\" ] cfg [ \"planes\" ] = cfg . pop ( \"filters\" ) cfg [ \"kernel_size\" ] = cfg . pop ( \"kernel\" ) layers . append ( JasperBlock ( inplanes = inplanes , activation = activation , residual_panes = residual_panes , ** cfg ) ) inplanes = cfg [ \"planes\" ] encoder = nn . Sequential ( * layers ) encoder . apply ( init_weights ) decoder_params = conf [ \"decoder\" ][ \"params\" ] decoder = torch . nn . Sequential ( torch . nn . Conv1d ( decoder_params [ \"feat_in\" ], decoder_params [ \"num_classes\" ] + 1 , kernel_size = 1 , bias = True , ) ) decoder . apply ( init_weights ) return encoder , decoder","title":"read_config()"}]}