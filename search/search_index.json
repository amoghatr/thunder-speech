{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Thunder speech A Hackable speech recognition library. What to expect from this project: End-to-end speech recognition models Simple fine tuning to new languages Inference support as a first-class feature Developer oriented api What it's not: A general-purpose speech toolkit A collection of complex systems that require thousands of gpu-hours and expert knowledge, only focusing on the state-of-the-art results Quick usage guide Install Install the library from PyPI: pip install thunder-speech Import desired models from thunder.quartznet.module import QuartznetModule , NemoCheckpoint # Tab completion works to discover other Nemocheckpoint.* model = QuartznetModule . load_from_nemo ( checkpoint_name = NemoCheckpoint . QuartzNet5x5LS_En ) Load audio and predict import torchaudio audio , sr = torchaudio . load ( \"my_sample_file.wav\" ) transcriptions = model . predict ( audio ) # transcriptions is a list of strings with the captions. More quick tips If you want to know how to export the models using torchscript, access the raw probabilities and decode manually or fine-tune the models you can access the documentation here . Contributing The first step to contribute is to do a editable install of the library: git clone https://github.com/scart97/thunder-speech.git cd thunder-speech pip install -e .[dev,testing] pre-commit install Then, make sure that everything is working. You can run the test suit, that is based on pytest: RUN_SLOW=1 pytest Here the RUN_SLOW flag is used to run all of the tests, including the ones that might download checkpoints or do small training runs and are marked as slow. If you don't have a CUDA capable gpu, some of the tests will be unconditionally skipped. Influences This library has heavy influence of the best practices in the pytorch ecosystem. The original model code, including checkpoints, is based on the NeMo ASR toolkit. From there also came the inspiration for the fine-tuning and prediction api's. The data loading and processing is loosely based on my experience using fast.ai. It tries to decouple transforms that happen at the item level from the ones that are efficiently implemented for the whole batch at the GPU. Also the idea that default parameters should be great. The overall organization of code and decoupling follows the pytorch-lightning ideals, with self contained modules that try to reduce the boilerplate necessary. Finally the transformers library inspired the simple model implementations, with a clear separation in folders containing the specific code that you need to understand each architecture and preprocessing. Also their strong test suit. Note This project has been set up using PyScaffold 3.3. For details and usage information on PyScaffold see https://pyscaffold.org/.","title":"Home"},{"location":"#thunder-speech","text":"A Hackable speech recognition library. What to expect from this project: End-to-end speech recognition models Simple fine tuning to new languages Inference support as a first-class feature Developer oriented api What it's not: A general-purpose speech toolkit A collection of complex systems that require thousands of gpu-hours and expert knowledge, only focusing on the state-of-the-art results","title":"Thunder speech"},{"location":"#quick-usage-guide","text":"","title":"Quick usage guide"},{"location":"#install","text":"Install the library from PyPI: pip install thunder-speech","title":"Install"},{"location":"#import-desired-models","text":"from thunder.quartznet.module import QuartznetModule , NemoCheckpoint # Tab completion works to discover other Nemocheckpoint.* model = QuartznetModule . load_from_nemo ( checkpoint_name = NemoCheckpoint . QuartzNet5x5LS_En )","title":"Import desired models"},{"location":"#load-audio-and-predict","text":"import torchaudio audio , sr = torchaudio . load ( \"my_sample_file.wav\" ) transcriptions = model . predict ( audio ) # transcriptions is a list of strings with the captions.","title":"Load audio and predict"},{"location":"#more-quick-tips","text":"If you want to know how to export the models using torchscript, access the raw probabilities and decode manually or fine-tune the models you can access the documentation here .","title":"More quick tips"},{"location":"#contributing","text":"The first step to contribute is to do a editable install of the library: git clone https://github.com/scart97/thunder-speech.git cd thunder-speech pip install -e .[dev,testing] pre-commit install Then, make sure that everything is working. You can run the test suit, that is based on pytest: RUN_SLOW=1 pytest Here the RUN_SLOW flag is used to run all of the tests, including the ones that might download checkpoints or do small training runs and are marked as slow. If you don't have a CUDA capable gpu, some of the tests will be unconditionally skipped.","title":"Contributing"},{"location":"#influences","text":"This library has heavy influence of the best practices in the pytorch ecosystem. The original model code, including checkpoints, is based on the NeMo ASR toolkit. From there also came the inspiration for the fine-tuning and prediction api's. The data loading and processing is loosely based on my experience using fast.ai. It tries to decouple transforms that happen at the item level from the ones that are efficiently implemented for the whole batch at the GPU. Also the idea that default parameters should be great. The overall organization of code and decoupling follows the pytorch-lightning ideals, with self contained modules that try to reduce the boilerplate necessary. Finally the transformers library inspired the simple model implementations, with a clear separation in folders containing the specific code that you need to understand each architecture and preprocessing. Also their strong test suit.","title":"Influences"},{"location":"#note","text":"This project has been set up using PyScaffold 3.3. For details and usage information on PyScaffold see https://pyscaffold.org/.","title":"Note"},{"location":"Ultimate%20guide/","text":"The ultimate guide to speech recognition (WIP) This guide has the purpose to give you all the steps necessary to achieve a decent (but not necessarily state-of-the-art) speech recognition system in a new language. Gathering the data Speech recognition systems are really sensitive to the quality of data used to train them. Also, they usually require from hundreads to thousands of hours depending on the quality expected. Some good sources for data are Mozilla commonvoice , the OpenSLR project or Tatoeba . After you download some initial data, there's a number of data quality problems that are expected and need to be fixed if you want to increase the performance of the trained models. First of all, list all of the audio files by increasing size and check if there's any corrupted file (usually they're very small). Remove them from the training data. Then install sox , that's the best tool to inspect and convert audio files. It should come with a basic tool to inspect any file in the terminal, called soxi . As an example: $ soxi example_file.wav Input File : 'example_file.wav' Channels : 1 Sample Rate : 16000 Precision : 16-bit Duration : 00:00:04.27 = 94053 samples ~ 319.908 CDDA sectors File Size : 188k Bit Rate : 353k Sample Encoding: 16-bit Signed Integer PCM That's the usual format of files used in speech recognition research. Wav files, encoded with a 16-bit PCM codec and a sample rate of 16 kHz. The file format and codec can vary and will only affect the quality of the audio, but the sample rate is the essential one. Trained models only work with a specific sample rate, and any file with a different one must be resampled either at the file level or directly after loading with torchaudio. Sox has more capabilities than just listing audio metadata. It can read almost any file format and convert to others. If you have an mp3 file at 44.1 kHz, and want to convert into the usual wav format above, you can use: sox input_file.mp3 -r 16000 -c 1 -b 16 output_file.wav The flags used represent: -r 16000 : 16 kHz sample rate -c 1 : convert to mono (1 channel) -b 16 : convert to PCM 16-bit output_file.wav : Sox understand that the output will be wav just by the file extension Ideally all of the training and inference audio files should have the same characteristics, so it's a good idea to transform them into a common format before training. As the wav format does not have any compression, the resulting data will demand a huge HDD space. If that's a problem, you can instead convert the files to mp3, that way you lose a small percentage of the performance but can achieve up to 10x smaller dataset sizes. Now take a look at the labels. We are searching for a number of different problems here: Strange symbols: can easily find if you list all unique characters in the dataset Text in another language: remove these files Additional info that should not be there, like speaker identification as part of the transcription (common in subtitles) Regional/temporal differences that can cause the same words to have multiple written forms: mixing data from multiple countries that speak the same language, or using labels that came from old books Try to fix those label problems, or remove them from the training set if you have lots of data. Don't spend weeks just looking at the data, but have a small subset that you can trust is properly cleaned, even if that means manually labeling again. After you train the first couple of models, it's possible to use the model itself to help find problems in the training data. Writing the dataset/datamodule TODO: fill this section with the nemo manifest example load source load audio load text fix text Expand contractions ( I'm becomes I am ) Expand numbers ( 42 becomes forty two ) Optionally remove punctuation datamodule with sources First train For this first train, you should only try to overfit one batch. This is the most simple test, and if you can't get past it then anything more complex that you try will be wasted time. To do it, try to load a training dataset with only one batch worth of data. The validation/test sets can be as usual, you will ignore them at this step. As we are using pytorch lightning, there's a trainer flag to limit the number of training batches ( limit_train_batches=1 ) that can be used. Also, remember to disable any shuffle at the dataloader, to ensure the same batch will be used every epoch. Before you run the training, disable any augmentation, regularization and advanced stuff like learning rate scheduling. You can start with either a pretrained model or a clean new one, but either way don't freeze any parameters, just let it all train. Start the training, and you should see the loss follow a pattern where, the more time you let it run, the final value will be lower. This means that small bumps will happen, but it will always recover and keep going down. The ideal point is where you run the prediction on the batch that you overfit, and the model doesn't make a single mistake. Some problems that can happen: The loss is negative : There's a blank in the target text, find and remove it. Blanks should only be produced by the model, never at the labels. There's no predictions at all : let it train for more time Still, there's no predictions after a long time : Check if the target texts are being processed correctly. Inside the training step, decode the target text and assert that it returns what you expect The loss does a 'U' curve where it starts normally but then turns around and just keep increasing : try to lower the learning rate Second train Now repeat the first training, but with around 10 hours of data. This number depends on the hardware that you have available, but something that gives you 2 minute epochs is a good amount. This time, you're not trying to overfit anymore. The validation loss will start to get lower, and the metrics will improve compared to the first training. But, quickly, the model will reach the point where the data is enough and it will start to overfit to the training data. TODO: better graphs? Expected train loss: \\ \\ \\ \\ \\______ Expected val loss/metrics: \\ \\ \\ \\ / \\____/ Scaling to the whole dataset TODO: expand this section break long audios - more than 25s is usually bad Use the model to find problems Sort by loss descending and manually check the files Sort by CER descending and manually check the files Sort by CER ascending on the validation/test set to find possible data leak Watch for the loss spikes during training Reducing overfit TODO: expand this section fastai recipe https://youtu.be/4u8FxNEDUeg?t=1333 Add more data Add augmentation Regularization Deploy! TODO: expand this section torch jit Streamlit torchserve bentoml","title":"The ultimate guide to speech recognition (WIP)"},{"location":"Ultimate%20guide/#the-ultimate-guide-to-speech-recognition-wip","text":"This guide has the purpose to give you all the steps necessary to achieve a decent (but not necessarily state-of-the-art) speech recognition system in a new language.","title":"The ultimate guide to speech recognition (WIP)"},{"location":"Ultimate%20guide/#gathering-the-data","text":"Speech recognition systems are really sensitive to the quality of data used to train them. Also, they usually require from hundreads to thousands of hours depending on the quality expected. Some good sources for data are Mozilla commonvoice , the OpenSLR project or Tatoeba . After you download some initial data, there's a number of data quality problems that are expected and need to be fixed if you want to increase the performance of the trained models. First of all, list all of the audio files by increasing size and check if there's any corrupted file (usually they're very small). Remove them from the training data. Then install sox , that's the best tool to inspect and convert audio files. It should come with a basic tool to inspect any file in the terminal, called soxi . As an example: $ soxi example_file.wav Input File : 'example_file.wav' Channels : 1 Sample Rate : 16000 Precision : 16-bit Duration : 00:00:04.27 = 94053 samples ~ 319.908 CDDA sectors File Size : 188k Bit Rate : 353k Sample Encoding: 16-bit Signed Integer PCM That's the usual format of files used in speech recognition research. Wav files, encoded with a 16-bit PCM codec and a sample rate of 16 kHz. The file format and codec can vary and will only affect the quality of the audio, but the sample rate is the essential one. Trained models only work with a specific sample rate, and any file with a different one must be resampled either at the file level or directly after loading with torchaudio. Sox has more capabilities than just listing audio metadata. It can read almost any file format and convert to others. If you have an mp3 file at 44.1 kHz, and want to convert into the usual wav format above, you can use: sox input_file.mp3 -r 16000 -c 1 -b 16 output_file.wav The flags used represent: -r 16000 : 16 kHz sample rate -c 1 : convert to mono (1 channel) -b 16 : convert to PCM 16-bit output_file.wav : Sox understand that the output will be wav just by the file extension Ideally all of the training and inference audio files should have the same characteristics, so it's a good idea to transform them into a common format before training. As the wav format does not have any compression, the resulting data will demand a huge HDD space. If that's a problem, you can instead convert the files to mp3, that way you lose a small percentage of the performance but can achieve up to 10x smaller dataset sizes. Now take a look at the labels. We are searching for a number of different problems here: Strange symbols: can easily find if you list all unique characters in the dataset Text in another language: remove these files Additional info that should not be there, like speaker identification as part of the transcription (common in subtitles) Regional/temporal differences that can cause the same words to have multiple written forms: mixing data from multiple countries that speak the same language, or using labels that came from old books Try to fix those label problems, or remove them from the training set if you have lots of data. Don't spend weeks just looking at the data, but have a small subset that you can trust is properly cleaned, even if that means manually labeling again. After you train the first couple of models, it's possible to use the model itself to help find problems in the training data.","title":"Gathering the data"},{"location":"Ultimate%20guide/#writing-the-datasetdatamodule","text":"TODO: fill this section with the nemo manifest example load source load audio load text fix text Expand contractions ( I'm becomes I am ) Expand numbers ( 42 becomes forty two ) Optionally remove punctuation datamodule with sources","title":"Writing the dataset/datamodule"},{"location":"Ultimate%20guide/#first-train","text":"For this first train, you should only try to overfit one batch. This is the most simple test, and if you can't get past it then anything more complex that you try will be wasted time. To do it, try to load a training dataset with only one batch worth of data. The validation/test sets can be as usual, you will ignore them at this step. As we are using pytorch lightning, there's a trainer flag to limit the number of training batches ( limit_train_batches=1 ) that can be used. Also, remember to disable any shuffle at the dataloader, to ensure the same batch will be used every epoch. Before you run the training, disable any augmentation, regularization and advanced stuff like learning rate scheduling. You can start with either a pretrained model or a clean new one, but either way don't freeze any parameters, just let it all train. Start the training, and you should see the loss follow a pattern where, the more time you let it run, the final value will be lower. This means that small bumps will happen, but it will always recover and keep going down. The ideal point is where you run the prediction on the batch that you overfit, and the model doesn't make a single mistake. Some problems that can happen: The loss is negative : There's a blank in the target text, find and remove it. Blanks should only be produced by the model, never at the labels. There's no predictions at all : let it train for more time Still, there's no predictions after a long time : Check if the target texts are being processed correctly. Inside the training step, decode the target text and assert that it returns what you expect The loss does a 'U' curve where it starts normally but then turns around and just keep increasing : try to lower the learning rate","title":"First train"},{"location":"Ultimate%20guide/#second-train","text":"Now repeat the first training, but with around 10 hours of data. This number depends on the hardware that you have available, but something that gives you 2 minute epochs is a good amount. This time, you're not trying to overfit anymore. The validation loss will start to get lower, and the metrics will improve compared to the first training. But, quickly, the model will reach the point where the data is enough and it will start to overfit to the training data. TODO: better graphs? Expected train loss: \\ \\ \\ \\ \\______ Expected val loss/metrics: \\ \\ \\ \\ / \\____/","title":"Second train"},{"location":"Ultimate%20guide/#scaling-to-the-whole-dataset","text":"TODO: expand this section break long audios - more than 25s is usually bad Use the model to find problems Sort by loss descending and manually check the files Sort by CER descending and manually check the files Sort by CER ascending on the validation/test set to find possible data leak Watch for the loss spikes during training","title":"Scaling to the whole dataset"},{"location":"Ultimate%20guide/#reducing-overfit","text":"TODO: expand this section fastai recipe https://youtu.be/4u8FxNEDUeg?t=1333 Add more data Add augmentation Regularization","title":"Reducing overfit"},{"location":"Ultimate%20guide/#deploy","text":"TODO: expand this section torch jit Streamlit torchserve bentoml","title":"Deploy!"},{"location":"quick%20reference%20guide/","text":"Quick reference guide How to export a Quartznet .nemo file to a pure pytorch model? from thunder.quartznet.module import QuartznetModule module = QuartznetModule . load_from_nemo ( nemo_filepath = \"/path/to/checkpoint.nemo\" ) module . to_torchscript ( \"model_ready_for_inference.pt\" ) How to run inference on that exported file? import torch import torchaudio model = torch . jit . load ( \"model_ready_for_inference.pt\" ) audio , sr = torchaudio . load ( \"audio_file.wav\" ) # Optionally resample if sr is different from original model sample rate # tfm = torchaudio.transforms.Resample(orig_freq=sr, new_freq=16000) # audio = tfm(audio) transcriptions = model . predict ( audio ) # transcriptions is a list of strings with the captions. Note The exported model only depends on pytorch and torchaudio, and the later is only used to open the audio file into a tensor. If torchaudio.load could be compiled inside the model in the future, similar to what already happens with torchvision, the dependency can be removed and only the base pytorch will be necessary to run inferece! What if I want the probabilities instead of the captions? Instead of model.predict(audio) , use just model(audio) import torch import torchaudio model = torch . jit . load ( \"model_ready_for_inference.pt\" ) audio , sr = torchaudio . load ( audio_name ) probs = model ( audio ) # If you also want the transcriptions: transcriptions = model . text_transform . decode_prediction ( probs . argmax ( 1 )) How to finetune a model if I already have the nemo manifests prepared? import pytorch_lightning as pl from thunder.data.datamodule import ManifestDatamodule from thunder.quartznet.module import QuartznetModule , NemoCheckpoint dm = ManifestDatamodule ( train_manifest = \"/path/to/train_manifest.json\" , val_manifest = \"/path/to/val_manifest.json\" , test_manifest = \"/path/to/test_manifest.json\" , ) # Tab completion works to discover other Nemocheckpoint.* model = QuartznetModule . load_from_nemo ( checkpoint_name = NemoCheckpoint . QuartzNet5x5LS_En ) trainer = pl . Trainer ( gpus =- 1 , # Use all gpus max_epochs = 10 , ) trainer . fit ( model = model , datamodule = dm ) Danger This will probably have a subpar result right now, as I'm still working on properly fine tuning (freeze encoder at the start, learning rate scheduling, better defaults) How to get the initial_vocab_tokens from my dataset? from thunder.text_processing.tokenizer import char_tokenizer , get_most_frequent_tokens my_datamodule = CustomDatamodule ( ... ) my_datamodule . prepare_data () my_datamodule . setup ( None ) train_corpus = \" \" . join ( my_datamodule . train_dataset . all_outputs ()) initial_vocab_tokens = get_most_frequent_tokens ( train_corpus , char_tokenizer )","title":"Quick reference guide"},{"location":"quick%20reference%20guide/#quick-reference-guide","text":"","title":"Quick reference guide"},{"location":"quick%20reference%20guide/#how-to-export-a-quartznet-nemo-file-to-a-pure-pytorch-model","text":"from thunder.quartznet.module import QuartznetModule module = QuartznetModule . load_from_nemo ( nemo_filepath = \"/path/to/checkpoint.nemo\" ) module . to_torchscript ( \"model_ready_for_inference.pt\" )","title":"How to export a Quartznet .nemo file to a pure pytorch model?"},{"location":"quick%20reference%20guide/#how-to-run-inference-on-that-exported-file","text":"import torch import torchaudio model = torch . jit . load ( \"model_ready_for_inference.pt\" ) audio , sr = torchaudio . load ( \"audio_file.wav\" ) # Optionally resample if sr is different from original model sample rate # tfm = torchaudio.transforms.Resample(orig_freq=sr, new_freq=16000) # audio = tfm(audio) transcriptions = model . predict ( audio ) # transcriptions is a list of strings with the captions. Note The exported model only depends on pytorch and torchaudio, and the later is only used to open the audio file into a tensor. If torchaudio.load could be compiled inside the model in the future, similar to what already happens with torchvision, the dependency can be removed and only the base pytorch will be necessary to run inferece!","title":"How to run inference on that exported file?"},{"location":"quick%20reference%20guide/#what-if-i-want-the-probabilities-instead-of-the-captions","text":"Instead of model.predict(audio) , use just model(audio) import torch import torchaudio model = torch . jit . load ( \"model_ready_for_inference.pt\" ) audio , sr = torchaudio . load ( audio_name ) probs = model ( audio ) # If you also want the transcriptions: transcriptions = model . text_transform . decode_prediction ( probs . argmax ( 1 ))","title":"What if I want the probabilities instead of the captions?"},{"location":"quick%20reference%20guide/#how-to-finetune-a-model-if-i-already-have-the-nemo-manifests-prepared","text":"import pytorch_lightning as pl from thunder.data.datamodule import ManifestDatamodule from thunder.quartznet.module import QuartznetModule , NemoCheckpoint dm = ManifestDatamodule ( train_manifest = \"/path/to/train_manifest.json\" , val_manifest = \"/path/to/val_manifest.json\" , test_manifest = \"/path/to/test_manifest.json\" , ) # Tab completion works to discover other Nemocheckpoint.* model = QuartznetModule . load_from_nemo ( checkpoint_name = NemoCheckpoint . QuartzNet5x5LS_En ) trainer = pl . Trainer ( gpus =- 1 , # Use all gpus max_epochs = 10 , ) trainer . fit ( model = model , datamodule = dm ) Danger This will probably have a subpar result right now, as I'm still working on properly fine tuning (freeze encoder at the start, learning rate scheduling, better defaults)","title":"How to finetune a model if I already have the nemo manifests prepared?"},{"location":"quick%20reference%20guide/#how-to-get-the-initial_vocab_tokens-from-my-dataset","text":"from thunder.text_processing.tokenizer import char_tokenizer , get_most_frequent_tokens my_datamodule = CustomDatamodule ( ... ) my_datamodule . prepare_data () my_datamodule . setup ( None ) train_corpus = \" \" . join ( my_datamodule . train_dataset . all_outputs ()) initial_vocab_tokens = get_most_frequent_tokens ( train_corpus , char_tokenizer )","title":"How to get the initial_vocab_tokens from my dataset?"},{"location":"api/librosa%20compatibility/","text":"create_fb_matrix ( n_freqs , f_min , f_max , n_mels , sample_rate , norm = None , htk = True ) Create a frequency bin conversion matrix. This is a direct modification of torchaudio.functional.create_fb_matrix to also create the frequencies using the same formula as librosa Parameters: Name Type Description Default n_freqs int Number of frequencies to highlight/apply required f_min float Minimum frequency (Hz) required f_max float Maximum frequency (Hz) required n_mels int Number of mel filterbanks required sample_rate int Sample rate of the audio waveform required norm Optional[str] If 'slaney', divide the triangular mel weights by the width of the mel band (area normalization). None htk bool Use htk formula for mel scale or not. True Returns: Type Description Tensor Triangular filter banks (fb matrix) of size ( n_freqs , n_mels ) meaning number of frequencies to highlight/apply to x the number of filterbanks. Each column is a filterbank so that assuming there is a matrix A of size (..., n_freqs ), the applied result would be A * create_fb_matrix(A.size(-1), ...) . Source code in thunder/librosa_compat.py def create_fb_matrix ( n_freqs : int , f_min : float , f_max : float , n_mels : int , sample_rate : int , norm : Optional [ str ] = None , htk : bool = True , ) -> Tensor : \"\"\"Create a frequency bin conversion matrix. This is a direct modification of torchaudio.functional.create_fb_matrix to also create the frequencies using the same formula as librosa Args: n_freqs : Number of frequencies to highlight/apply f_min : Minimum frequency (Hz) f_max : Maximum frequency (Hz) n_mels : Number of mel filterbanks sample_rate : Sample rate of the audio waveform norm : If 'slaney', divide the triangular mel weights by the width of the mel band (area normalization). htk : Use htk formula for mel scale or not. Returns: Triangular filter banks (fb matrix) of size (``n_freqs``, ``n_mels``) meaning number of frequencies to highlight/apply to x the number of filterbanks. Each column is a filterbank so that assuming there is a matrix A of size (..., ``n_freqs``), the applied result would be ``A * create_fb_matrix(A.size(-1), ...)``. \"\"\" if norm is not None and norm != \"slaney\" : raise ValueError ( \"norm must be one of None or 'slaney'\" ) # freq bins # Equivalent filterbank construction by Librosa all_freqs = torch . linspace ( 0 , sample_rate // 2 , n_freqs ) f_pts = mel_frequencies ( f_min , f_max , n_mels , htk = htk ) # calculate the difference between each mel point and each stft freq point in hertz f_diff = f_pts [ 1 :] - f_pts [: - 1 ] # (n_mels + 1) slopes = f_pts . unsqueeze ( 0 ) - all_freqs . unsqueeze ( 1 ) # (n_freqs, n_mels + 2) # create overlapping triangles zero = torch . zeros ( 1 ) down_slopes = ( - 1.0 * slopes [:, : - 2 ]) / f_diff [: - 1 ] # (n_freqs, n_mels) up_slopes = slopes [:, 2 :] / f_diff [ 1 :] # (n_freqs, n_mels) fb = torch . max ( zero , torch . min ( down_slopes , up_slopes )) if norm is not None and norm == \"slaney\" : # Slaney-style mel is scaled to be approx constant energy per channel enorm = 2.0 / ( f_pts [ 2 : n_mels + 2 ] - f_pts [: n_mels ]) fb *= enorm . unsqueeze ( 0 ) if ( fb . max ( dim = 0 ) . values == 0.0 ) . any (): warnings . warn ( \"At least one mel filterbank has all zero values. \" f \"The value for `n_mels` ( { n_mels } ) may be set too high. \" f \"Or, the value for `n_freqs` ( { n_freqs } ) may be set too low.\" ) return fb hz_to_mel ( frequencies , htk = False ) This is a direct port of librosa.core.conver.hz_to_mel to work with torchaudio. Parameters: Name Type Description Default frequencies int Frequencies to convert required htk bool Use htk formula for conversion or not. Defaults to False. False Returns: Type Description int Frequencies in mel scale. Source code in thunder/librosa_compat.py def hz_to_mel ( frequencies : int , htk : bool = False ) -> int : \"\"\"This is a direct port of librosa.core.conver.hz_to_mel to work with torchaudio. Args: frequencies : Frequencies to convert htk : Use htk formula for conversion or not. Defaults to False. Returns: Frequencies in mel scale. \"\"\" if htk : return 2595.0 * math . log10 ( 1.0 + frequencies / 700.0 ) # Fill in the linear part f_min = 0.0 f_sp = 200.0 / 3 mels = ( frequencies - f_min ) / f_sp # Fill in the log-scale part min_log_hz = 1000.0 # beginning of log region (Hz) min_log_mel = ( min_log_hz - f_min ) / f_sp # same (Mels) logstep = math . log ( 6.4 ) / 27.0 # step size for log region if frequencies >= min_log_hz : # If we have scalar data, heck directly mels = min_log_mel + math . log ( frequencies / min_log_hz ) / logstep return mels mel_frequencies ( f_min , f_max , n_mels , htk ) Calculates the frequencies to create the mel scale filterbanks. This is a direct port of librosa.filters.mel_frequencies to work with pytorch. Parameters: Name Type Description Default f_min int Minimum frequency required f_max int Maximum frequency required n_mels int Number of mels required htk bool Use htk formula for mel scale or not required Returns: Type Description Tensor Tensor containing the corresponding frequencies. Source code in thunder/librosa_compat.py def mel_frequencies ( f_min : int , f_max : int , n_mels : int , htk : bool ) -> Tensor : \"\"\"Calculates the frequencies to create the mel scale filterbanks. This is a direct port of librosa.filters.mel_frequencies to work with pytorch. Args: f_min : Minimum frequency f_max : Maximum frequency n_mels : Number of mels htk : Use htk formula for mel scale or not Returns: Tensor containing the corresponding frequencies. \"\"\" m_min = hz_to_mel ( f_min , htk = htk ) m_max = hz_to_mel ( f_max , htk = htk ) m_pts = torch . linspace ( m_min , m_max , n_mels + 2 ) f_pts = mel_to_hz ( m_pts , htk = htk ) return f_pts mel_to_hz ( mels , htk = False ) This is a direct port of librosa.core.conver.mel_to_hz to work with torchaudio. Parameters: Name Type Description Default mels Tensor Frequencies to convert, in mel scale required htk bool Use htk formula for conversion or not. Defaults to False. False Returns: Type Description Tensor Frequencies in hertz. Source code in thunder/librosa_compat.py def mel_to_hz ( mels : Tensor , htk : bool = False ) -> Tensor : \"\"\"This is a direct port of librosa.core.conver.mel_to_hz to work with torchaudio. Args: mels : Frequencies to convert, in mel scale htk : Use htk formula for conversion or not. Defaults to False. Returns: Frequencies in hertz. \"\"\" if htk : return 700.0 * ( 10.0 ** ( mels / 2595.0 ) - 1.0 ) # Fill in the linear scale f_min = 0.0 f_sp = 200.0 / 3 freqs = f_min + f_sp * mels # And now the nonlinear scale min_log_hz = 1000.0 # beginning of log region (Hz) min_log_mel = ( min_log_hz - f_min ) / f_sp # same (Mels) logstep = math . log ( 6.4 ) / 27.0 # step size for log region log_t = mels >= min_log_mel freqs [ log_t ] = min_log_hz * ( logstep * ( mels [ log_t ] - min_log_mel )) . exp () return freqs","title":"Librosa compatibility"},{"location":"api/librosa%20compatibility/#thunder.librosa_compat.create_fb_matrix","text":"Create a frequency bin conversion matrix. This is a direct modification of torchaudio.functional.create_fb_matrix to also create the frequencies using the same formula as librosa Parameters: Name Type Description Default n_freqs int Number of frequencies to highlight/apply required f_min float Minimum frequency (Hz) required f_max float Maximum frequency (Hz) required n_mels int Number of mel filterbanks required sample_rate int Sample rate of the audio waveform required norm Optional[str] If 'slaney', divide the triangular mel weights by the width of the mel band (area normalization). None htk bool Use htk formula for mel scale or not. True Returns: Type Description Tensor Triangular filter banks (fb matrix) of size ( n_freqs , n_mels ) meaning number of frequencies to highlight/apply to x the number of filterbanks. Each column is a filterbank so that assuming there is a matrix A of size (..., n_freqs ), the applied result would be A * create_fb_matrix(A.size(-1), ...) . Source code in thunder/librosa_compat.py def create_fb_matrix ( n_freqs : int , f_min : float , f_max : float , n_mels : int , sample_rate : int , norm : Optional [ str ] = None , htk : bool = True , ) -> Tensor : \"\"\"Create a frequency bin conversion matrix. This is a direct modification of torchaudio.functional.create_fb_matrix to also create the frequencies using the same formula as librosa Args: n_freqs : Number of frequencies to highlight/apply f_min : Minimum frequency (Hz) f_max : Maximum frequency (Hz) n_mels : Number of mel filterbanks sample_rate : Sample rate of the audio waveform norm : If 'slaney', divide the triangular mel weights by the width of the mel band (area normalization). htk : Use htk formula for mel scale or not. Returns: Triangular filter banks (fb matrix) of size (``n_freqs``, ``n_mels``) meaning number of frequencies to highlight/apply to x the number of filterbanks. Each column is a filterbank so that assuming there is a matrix A of size (..., ``n_freqs``), the applied result would be ``A * create_fb_matrix(A.size(-1), ...)``. \"\"\" if norm is not None and norm != \"slaney\" : raise ValueError ( \"norm must be one of None or 'slaney'\" ) # freq bins # Equivalent filterbank construction by Librosa all_freqs = torch . linspace ( 0 , sample_rate // 2 , n_freqs ) f_pts = mel_frequencies ( f_min , f_max , n_mels , htk = htk ) # calculate the difference between each mel point and each stft freq point in hertz f_diff = f_pts [ 1 :] - f_pts [: - 1 ] # (n_mels + 1) slopes = f_pts . unsqueeze ( 0 ) - all_freqs . unsqueeze ( 1 ) # (n_freqs, n_mels + 2) # create overlapping triangles zero = torch . zeros ( 1 ) down_slopes = ( - 1.0 * slopes [:, : - 2 ]) / f_diff [: - 1 ] # (n_freqs, n_mels) up_slopes = slopes [:, 2 :] / f_diff [ 1 :] # (n_freqs, n_mels) fb = torch . max ( zero , torch . min ( down_slopes , up_slopes )) if norm is not None and norm == \"slaney\" : # Slaney-style mel is scaled to be approx constant energy per channel enorm = 2.0 / ( f_pts [ 2 : n_mels + 2 ] - f_pts [: n_mels ]) fb *= enorm . unsqueeze ( 0 ) if ( fb . max ( dim = 0 ) . values == 0.0 ) . any (): warnings . warn ( \"At least one mel filterbank has all zero values. \" f \"The value for `n_mels` ( { n_mels } ) may be set too high. \" f \"Or, the value for `n_freqs` ( { n_freqs } ) may be set too low.\" ) return fb","title":"create_fb_matrix()"},{"location":"api/librosa%20compatibility/#thunder.librosa_compat.hz_to_mel","text":"This is a direct port of librosa.core.conver.hz_to_mel to work with torchaudio. Parameters: Name Type Description Default frequencies int Frequencies to convert required htk bool Use htk formula for conversion or not. Defaults to False. False Returns: Type Description int Frequencies in mel scale. Source code in thunder/librosa_compat.py def hz_to_mel ( frequencies : int , htk : bool = False ) -> int : \"\"\"This is a direct port of librosa.core.conver.hz_to_mel to work with torchaudio. Args: frequencies : Frequencies to convert htk : Use htk formula for conversion or not. Defaults to False. Returns: Frequencies in mel scale. \"\"\" if htk : return 2595.0 * math . log10 ( 1.0 + frequencies / 700.0 ) # Fill in the linear part f_min = 0.0 f_sp = 200.0 / 3 mels = ( frequencies - f_min ) / f_sp # Fill in the log-scale part min_log_hz = 1000.0 # beginning of log region (Hz) min_log_mel = ( min_log_hz - f_min ) / f_sp # same (Mels) logstep = math . log ( 6.4 ) / 27.0 # step size for log region if frequencies >= min_log_hz : # If we have scalar data, heck directly mels = min_log_mel + math . log ( frequencies / min_log_hz ) / logstep return mels","title":"hz_to_mel()"},{"location":"api/librosa%20compatibility/#thunder.librosa_compat.mel_frequencies","text":"Calculates the frequencies to create the mel scale filterbanks. This is a direct port of librosa.filters.mel_frequencies to work with pytorch. Parameters: Name Type Description Default f_min int Minimum frequency required f_max int Maximum frequency required n_mels int Number of mels required htk bool Use htk formula for mel scale or not required Returns: Type Description Tensor Tensor containing the corresponding frequencies. Source code in thunder/librosa_compat.py def mel_frequencies ( f_min : int , f_max : int , n_mels : int , htk : bool ) -> Tensor : \"\"\"Calculates the frequencies to create the mel scale filterbanks. This is a direct port of librosa.filters.mel_frequencies to work with pytorch. Args: f_min : Minimum frequency f_max : Maximum frequency n_mels : Number of mels htk : Use htk formula for mel scale or not Returns: Tensor containing the corresponding frequencies. \"\"\" m_min = hz_to_mel ( f_min , htk = htk ) m_max = hz_to_mel ( f_max , htk = htk ) m_pts = torch . linspace ( m_min , m_max , n_mels + 2 ) f_pts = mel_to_hz ( m_pts , htk = htk ) return f_pts","title":"mel_frequencies()"},{"location":"api/librosa%20compatibility/#thunder.librosa_compat.mel_to_hz","text":"This is a direct port of librosa.core.conver.mel_to_hz to work with torchaudio. Parameters: Name Type Description Default mels Tensor Frequencies to convert, in mel scale required htk bool Use htk formula for conversion or not. Defaults to False. False Returns: Type Description Tensor Frequencies in hertz. Source code in thunder/librosa_compat.py def mel_to_hz ( mels : Tensor , htk : bool = False ) -> Tensor : \"\"\"This is a direct port of librosa.core.conver.mel_to_hz to work with torchaudio. Args: mels : Frequencies to convert, in mel scale htk : Use htk formula for conversion or not. Defaults to False. Returns: Frequencies in hertz. \"\"\" if htk : return 700.0 * ( 10.0 ** ( mels / 2595.0 ) - 1.0 ) # Fill in the linear scale f_min = 0.0 f_sp = 200.0 / 3 freqs = f_min + f_sp * mels # And now the nonlinear scale min_log_hz = 1000.0 # beginning of log region (Hz) min_log_mel = ( min_log_hz - f_min ) / f_sp # same (Mels) logstep = math . log ( 6.4 ) / 27.0 # step size for log region log_t = mels >= min_log_mel freqs [ log_t ] = min_log_hz * ( logstep * ( mels [ log_t ] - min_log_mel )) . exp () return freqs","title":"mel_to_hz()"},{"location":"api/metrics/","text":"CER Metric to compute the character error rate of predictions during the training loop. Accepts lists of predictions and references, correctly accumulating the metrics and computing the final value when requested. Check EditBaseMetric for more details on the possible methods. update_func ( self , predicted , reference ) Compute the statistics used to compare two strings using character error rate. Parameters: Name Type Description Default predicted str Single model prediction required reference str Corresponding reference required Returns: Type Description Tuple[int, int] Tuple containing the pure edit distance between predicted and reference, and the normalizing factor. Source code in thunder/metrics.py def update_func ( self , predicted : str , reference : str ) -> Tuple [ int , int ]: \"\"\"Compute the statistics used to compare two strings using character error rate. Args: predicted : Single model prediction reference : Corresponding reference Returns: Tuple containing the pure edit distance between predicted and reference, and the normalizing factor. \"\"\" return _cer_update ( predicted , reference ) EditBaseMetric __init__ ( self , compute_on_step = True , dist_sync_on_step = False , process_group = None , dist_sync_fn = None ) special Base metric for computations based on edit distance. Parameters: Name Type Description Default compute_on_step bool Forward only calls update() and returns None if this is set to False. True dist_sync_on_step bool Synchronize metric state across processes at each forward() before returning the value at the step. False process_group Optional[Any] Specify the process group on which synchronization is called. default: None (which selects the entire world) None dist_sync_fn Callable Callback that performs the allgather operation on the metric state. When None, DDP will be used to perform the allgather. None Source code in thunder/metrics.py def __init__ ( self , compute_on_step : bool = True , dist_sync_on_step : bool = False , process_group : Optional [ Any ] = None , dist_sync_fn : Callable = None , ): \"\"\"Base metric for computations based on edit distance. Args: compute_on_step: Forward only calls update() and returns None if this is set to False. dist_sync_on_step: Synchronize metric state across processes at each forward() before returning the value at the step. process_group: Specify the process group on which synchronization is called. default: None (which selects the entire world) dist_sync_fn: Callback that performs the allgather operation on the metric state. When None, DDP will be used to perform the allgather. \"\"\" super () . __init__ ( compute_on_step = compute_on_step , dist_sync_on_step = dist_sync_on_step , process_group = process_group , dist_sync_fn = dist_sync_fn , ) self . add_state ( \"distance\" , default = tensor ( 0 ), dist_reduce_fx = \"sum\" ) self . add_state ( \"total\" , default = tensor ( 0 ), dist_reduce_fx = \"sum\" ) compute ( self ) Uses the aggregated counters to calculate the final metric value. Returns: Type Description Tensor Float tensor between 0.0 and 1.0 representing the error rate. Source code in thunder/metrics.py def compute ( self ) -> Tensor : \"\"\"Uses the aggregated counters to calculate the final metric value. Returns: Float tensor between 0.0 and 1.0 representing the error rate. \"\"\" return tensor ( _edit_compute ( self . distance , self . total )) update ( self , preds , target ) Method used to update the internal counters of the metric at every batch. Subclasses should leave this function untouched and implement update_func instead. Parameters: Name Type Description Default preds List[str] List of predictions of the model, already decoded into string form. required target List[str] List of corresponding references. required Source code in thunder/metrics.py def update ( self , preds : List [ str ], target : List [ str ]): \"\"\"Method used to update the internal counters of the metric at every batch. Subclasses should leave this function untouched and implement update_func instead. Args: preds : List of predictions of the model, already decoded into string form. target : List of corresponding references. \"\"\" # fmt: off assert len ( target ) > 0 , \"You need to pass at least one pair\" assert len ( preds ) == len ( target ), \"The number of predictions and targets must be the same\" # fmt: on for predicted , reference in zip ( preds , target ): distance , total = self . update_func ( predicted , reference ) self . distance += distance self . total += total update_func ( self , predicted , reference ) Function to calculate the statistics from one pair of elements. This function should take the two strings, split them according to characters/words/phonemes based on the metric implemented, calculate the edit distance between the two splitted strings and return also the normalizing factor, that is the number of elements in the splitted reference. Parameters: Name Type Description Default predicted str Single model prediction required reference str Corresponding reference required Returns: Type Description Tuple[int, int] Tuple containing the pure edit distance between predicted and reference, and the normalizing factor. Source code in thunder/metrics.py def update_func ( self , predicted : str , reference : str ) -> Tuple [ int , int ]: \"\"\"Function to calculate the statistics from one pair of elements. This function should take the two strings, split them according to characters/words/phonemes based on the metric implemented, calculate the edit distance between the two splitted strings and return also the normalizing factor, that is the number of elements in the splitted reference. Args: predicted : Single model prediction reference : Corresponding reference Returns: Tuple containing the pure edit distance between predicted and reference, and the normalizing factor. \"\"\" pass WER Metric to compute the word error rate of predictions during the training loop. Accepts lists of predictions and references, correctly accumulating the metrics and computing the final value when requested. Check EditBaseMetric for more details on the possible methods. update_func ( self , predicted , reference ) Compute the statistics used to compare two strings using word error rate. Parameters: Name Type Description Default predicted str Single model prediction required reference str Corresponding reference required Returns: Type Description Tuple[int, int] Tuple containing the pure edit distance between predicted and reference, and the normalizing factor. Source code in thunder/metrics.py def update_func ( self , predicted : str , reference : str ) -> Tuple [ int , int ]: \"\"\"Compute the statistics used to compare two strings using word error rate. Args: predicted : Single model prediction reference : Corresponding reference Returns: Tuple containing the pure edit distance between predicted and reference, and the normalizing factor. \"\"\" return _wer_update ( predicted , reference ) single_cer ( predicted , reference ) Computes the character error rate between one prediction and the corresponding reference. This is the functional form, and it's recommended to use inside the training loop the class CER instead. Parameters: Name Type Description Default predicted str Model prediction after decoding back to string required reference str Reference text required Returns: Type Description float Value between 0.0 and 1.0 that measures the error rate. Source code in thunder/metrics.py def single_cer ( predicted : str , reference : str ) -> float : \"\"\"Computes the character error rate between one prediction and the corresponding reference. This is the functional form, and it's recommended to use inside the training loop the class [`CER`][thunder.metrics.CER] instead. Args: predicted : Model prediction after decoding back to string reference : Reference text Returns: Value between 0.0 and 1.0 that measures the error rate. \"\"\" distance , total = _cer_update ( predicted , reference ) return _edit_compute ( distance , total ) single_wer ( predicted , reference ) Computes the word error rate between one prediction and the corresponding reference. This is the functional form, and it's recommended to use inside the training loop the class WER instead. Parameters: Name Type Description Default predicted str Model prediction after decoding back to string required reference str Reference text required Returns: Type Description float Value between 0.0 and 1.0 that measures the error rate. Source code in thunder/metrics.py def single_wer ( predicted : str , reference : str ) -> float : \"\"\"Computes the word error rate between one prediction and the corresponding reference. This is the functional form, and it's recommended to use inside the training loop the class [`WER`][thunder.metrics.WER] instead. Args: predicted : Model prediction after decoding back to string reference : Reference text Returns: Value between 0.0 and 1.0 that measures the error rate. \"\"\" distance , total = _wer_update ( predicted , reference ) return _edit_compute ( distance , total )","title":"Metrics"},{"location":"api/metrics/#thunder.metrics.CER","text":"Metric to compute the character error rate of predictions during the training loop. Accepts lists of predictions and references, correctly accumulating the metrics and computing the final value when requested. Check EditBaseMetric for more details on the possible methods.","title":"CER"},{"location":"api/metrics/#thunder.metrics.CER.update_func","text":"Compute the statistics used to compare two strings using character error rate. Parameters: Name Type Description Default predicted str Single model prediction required reference str Corresponding reference required Returns: Type Description Tuple[int, int] Tuple containing the pure edit distance between predicted and reference, and the normalizing factor. Source code in thunder/metrics.py def update_func ( self , predicted : str , reference : str ) -> Tuple [ int , int ]: \"\"\"Compute the statistics used to compare two strings using character error rate. Args: predicted : Single model prediction reference : Corresponding reference Returns: Tuple containing the pure edit distance between predicted and reference, and the normalizing factor. \"\"\" return _cer_update ( predicted , reference )","title":"update_func()"},{"location":"api/metrics/#thunder.metrics.EditBaseMetric","text":"","title":"EditBaseMetric"},{"location":"api/metrics/#thunder.metrics.EditBaseMetric.__init__","text":"Base metric for computations based on edit distance. Parameters: Name Type Description Default compute_on_step bool Forward only calls update() and returns None if this is set to False. True dist_sync_on_step bool Synchronize metric state across processes at each forward() before returning the value at the step. False process_group Optional[Any] Specify the process group on which synchronization is called. default: None (which selects the entire world) None dist_sync_fn Callable Callback that performs the allgather operation on the metric state. When None, DDP will be used to perform the allgather. None Source code in thunder/metrics.py def __init__ ( self , compute_on_step : bool = True , dist_sync_on_step : bool = False , process_group : Optional [ Any ] = None , dist_sync_fn : Callable = None , ): \"\"\"Base metric for computations based on edit distance. Args: compute_on_step: Forward only calls update() and returns None if this is set to False. dist_sync_on_step: Synchronize metric state across processes at each forward() before returning the value at the step. process_group: Specify the process group on which synchronization is called. default: None (which selects the entire world) dist_sync_fn: Callback that performs the allgather operation on the metric state. When None, DDP will be used to perform the allgather. \"\"\" super () . __init__ ( compute_on_step = compute_on_step , dist_sync_on_step = dist_sync_on_step , process_group = process_group , dist_sync_fn = dist_sync_fn , ) self . add_state ( \"distance\" , default = tensor ( 0 ), dist_reduce_fx = \"sum\" ) self . add_state ( \"total\" , default = tensor ( 0 ), dist_reduce_fx = \"sum\" )","title":"__init__()"},{"location":"api/metrics/#thunder.metrics.EditBaseMetric.compute","text":"Uses the aggregated counters to calculate the final metric value. Returns: Type Description Tensor Float tensor between 0.0 and 1.0 representing the error rate. Source code in thunder/metrics.py def compute ( self ) -> Tensor : \"\"\"Uses the aggregated counters to calculate the final metric value. Returns: Float tensor between 0.0 and 1.0 representing the error rate. \"\"\" return tensor ( _edit_compute ( self . distance , self . total ))","title":"compute()"},{"location":"api/metrics/#thunder.metrics.EditBaseMetric.update","text":"Method used to update the internal counters of the metric at every batch. Subclasses should leave this function untouched and implement update_func instead. Parameters: Name Type Description Default preds List[str] List of predictions of the model, already decoded into string form. required target List[str] List of corresponding references. required Source code in thunder/metrics.py def update ( self , preds : List [ str ], target : List [ str ]): \"\"\"Method used to update the internal counters of the metric at every batch. Subclasses should leave this function untouched and implement update_func instead. Args: preds : List of predictions of the model, already decoded into string form. target : List of corresponding references. \"\"\" # fmt: off assert len ( target ) > 0 , \"You need to pass at least one pair\" assert len ( preds ) == len ( target ), \"The number of predictions and targets must be the same\" # fmt: on for predicted , reference in zip ( preds , target ): distance , total = self . update_func ( predicted , reference ) self . distance += distance self . total += total","title":"update()"},{"location":"api/metrics/#thunder.metrics.EditBaseMetric.update_func","text":"Function to calculate the statistics from one pair of elements. This function should take the two strings, split them according to characters/words/phonemes based on the metric implemented, calculate the edit distance between the two splitted strings and return also the normalizing factor, that is the number of elements in the splitted reference. Parameters: Name Type Description Default predicted str Single model prediction required reference str Corresponding reference required Returns: Type Description Tuple[int, int] Tuple containing the pure edit distance between predicted and reference, and the normalizing factor. Source code in thunder/metrics.py def update_func ( self , predicted : str , reference : str ) -> Tuple [ int , int ]: \"\"\"Function to calculate the statistics from one pair of elements. This function should take the two strings, split them according to characters/words/phonemes based on the metric implemented, calculate the edit distance between the two splitted strings and return also the normalizing factor, that is the number of elements in the splitted reference. Args: predicted : Single model prediction reference : Corresponding reference Returns: Tuple containing the pure edit distance between predicted and reference, and the normalizing factor. \"\"\" pass","title":"update_func()"},{"location":"api/metrics/#thunder.metrics.WER","text":"Metric to compute the word error rate of predictions during the training loop. Accepts lists of predictions and references, correctly accumulating the metrics and computing the final value when requested. Check EditBaseMetric for more details on the possible methods.","title":"WER"},{"location":"api/metrics/#thunder.metrics.WER.update_func","text":"Compute the statistics used to compare two strings using word error rate. Parameters: Name Type Description Default predicted str Single model prediction required reference str Corresponding reference required Returns: Type Description Tuple[int, int] Tuple containing the pure edit distance between predicted and reference, and the normalizing factor. Source code in thunder/metrics.py def update_func ( self , predicted : str , reference : str ) -> Tuple [ int , int ]: \"\"\"Compute the statistics used to compare two strings using word error rate. Args: predicted : Single model prediction reference : Corresponding reference Returns: Tuple containing the pure edit distance between predicted and reference, and the normalizing factor. \"\"\" return _wer_update ( predicted , reference )","title":"update_func()"},{"location":"api/metrics/#thunder.metrics.single_cer","text":"Computes the character error rate between one prediction and the corresponding reference. This is the functional form, and it's recommended to use inside the training loop the class CER instead. Parameters: Name Type Description Default predicted str Model prediction after decoding back to string required reference str Reference text required Returns: Type Description float Value between 0.0 and 1.0 that measures the error rate. Source code in thunder/metrics.py def single_cer ( predicted : str , reference : str ) -> float : \"\"\"Computes the character error rate between one prediction and the corresponding reference. This is the functional form, and it's recommended to use inside the training loop the class [`CER`][thunder.metrics.CER] instead. Args: predicted : Model prediction after decoding back to string reference : Reference text Returns: Value between 0.0 and 1.0 that measures the error rate. \"\"\" distance , total = _cer_update ( predicted , reference ) return _edit_compute ( distance , total )","title":"single_cer()"},{"location":"api/metrics/#thunder.metrics.single_wer","text":"Computes the word error rate between one prediction and the corresponding reference. This is the functional form, and it's recommended to use inside the training loop the class WER instead. Parameters: Name Type Description Default predicted str Model prediction after decoding back to string required reference str Reference text required Returns: Type Description float Value between 0.0 and 1.0 that measures the error rate. Source code in thunder/metrics.py def single_wer ( predicted : str , reference : str ) -> float : \"\"\"Computes the word error rate between one prediction and the corresponding reference. This is the functional form, and it's recommended to use inside the training loop the class [`WER`][thunder.metrics.WER] instead. Args: predicted : Model prediction after decoding back to string reference : Reference text Returns: Value between 0.0 and 1.0 that measures the error rate. \"\"\" distance , total = _wer_update ( predicted , reference ) return _edit_compute ( distance , total )","title":"single_wer()"},{"location":"api/utils/","text":"audio_len ( item ) Returns the length of the audio file Parameters: Name Type Description Default item Union[pathlib.Path, str] Audio path required Returns: Type Description float Lenght in seconds of the audio Source code in thunder/utils.py def audio_len ( item : Union [ Path , str ]) -> float : \"\"\"Returns the length of the audio file Args: item : Audio path Returns: Lenght in seconds of the audio \"\"\" metadata = torchaudio . info ( item ) return metadata . num_frames / metadata . sample_rate chain_calls ( * funcs ) Chain multiple functions that take only one argument, producing a new function that is the result of calling the individual functions in sequence. Examples: f1 = lambda x : 2 * x f2 = lambda x : 3 * x f3 = lambda x : 4 * x g = chain_calls ( f1 , f2 , f3 ) assert g ( 1 ) == 24 Returns: Type Description Callable Single chained function Source code in thunder/utils.py def chain_calls ( * funcs : List [ Callable ]) -> Callable : \"\"\"Chain multiple functions that take only one argument, producing a new function that is the result of calling the individual functions in sequence. Example: ```python f1 = lambda x: 2 * x f2 = lambda x: 3 * x f3 = lambda x: 4 * x g = chain_calls(f1, f2, f3) assert g(1) == 24 ``` Returns: Single chained function \"\"\" def call ( x , f ): return f ( x ) def _inner ( arg ): return functools . reduce ( call , funcs , arg ) return _inner get_default_cache_folder () Get the default folder where the cached stuff will be saved. Returns: Type Description Path Path of the cache folder. Source code in thunder/utils.py def get_default_cache_folder () -> Path : \"\"\"Get the default folder where the cached stuff will be saved. Returns: Path of the cache folder. \"\"\" folder = Path . home () / \".thunder\" folder . mkdir ( exist_ok = True ) return folder get_files ( directory , extension ) Find all files in directory with extension. Parameters: Name Type Description Default directory Union[str, pathlib.Path] Directory to recursively find the files required extension str File extension to search for required Returns: Type Description List[pathlib.Path] List of all the files that match the extension Source code in thunder/utils.py def get_files ( directory : Union [ str , Path ], extension : str ) -> List [ Path ]: \"\"\"Find all files in directory with extension. Args: directory : Directory to recursively find the files extension : File extension to search for Returns: List of all the files that match the extension \"\"\" files_found = [] for root , _ , files in os . walk ( directory , followlinks = True ): files_found += [ Path ( root ) / f for f in files if f . endswith ( extension )] return files_found","title":"Utils"},{"location":"api/utils/#thunder.utils.audio_len","text":"Returns the length of the audio file Parameters: Name Type Description Default item Union[pathlib.Path, str] Audio path required Returns: Type Description float Lenght in seconds of the audio Source code in thunder/utils.py def audio_len ( item : Union [ Path , str ]) -> float : \"\"\"Returns the length of the audio file Args: item : Audio path Returns: Lenght in seconds of the audio \"\"\" metadata = torchaudio . info ( item ) return metadata . num_frames / metadata . sample_rate","title":"audio_len()"},{"location":"api/utils/#thunder.utils.chain_calls","text":"Chain multiple functions that take only one argument, producing a new function that is the result of calling the individual functions in sequence. Examples: f1 = lambda x : 2 * x f2 = lambda x : 3 * x f3 = lambda x : 4 * x g = chain_calls ( f1 , f2 , f3 ) assert g ( 1 ) == 24 Returns: Type Description Callable Single chained function Source code in thunder/utils.py def chain_calls ( * funcs : List [ Callable ]) -> Callable : \"\"\"Chain multiple functions that take only one argument, producing a new function that is the result of calling the individual functions in sequence. Example: ```python f1 = lambda x: 2 * x f2 = lambda x: 3 * x f3 = lambda x: 4 * x g = chain_calls(f1, f2, f3) assert g(1) == 24 ``` Returns: Single chained function \"\"\" def call ( x , f ): return f ( x ) def _inner ( arg ): return functools . reduce ( call , funcs , arg ) return _inner","title":"chain_calls()"},{"location":"api/utils/#thunder.utils.get_default_cache_folder","text":"Get the default folder where the cached stuff will be saved. Returns: Type Description Path Path of the cache folder. Source code in thunder/utils.py def get_default_cache_folder () -> Path : \"\"\"Get the default folder where the cached stuff will be saved. Returns: Path of the cache folder. \"\"\" folder = Path . home () / \".thunder\" folder . mkdir ( exist_ok = True ) return folder","title":"get_default_cache_folder()"},{"location":"api/utils/#thunder.utils.get_files","text":"Find all files in directory with extension. Parameters: Name Type Description Default directory Union[str, pathlib.Path] Directory to recursively find the files required extension str File extension to search for required Returns: Type Description List[pathlib.Path] List of all the files that match the extension Source code in thunder/utils.py def get_files ( directory : Union [ str , Path ], extension : str ) -> List [ Path ]: \"\"\"Find all files in directory with extension. Args: directory : Directory to recursively find the files extension : File extension to search for Returns: List of all the files that match the extension \"\"\" files_found = [] for root , _ , files in os . walk ( directory , followlinks = True ): files_found += [ Path ( root ) / f for f in files if f . endswith ( extension )] return files_found","title":"get_files()"},{"location":"api/Data/dataloader%20utils/","text":"Helper functions used by the speech dataloaders. asr_collate ( samples ) Function that collect samples and adds padding. Parameters: Name Type Description Default samples List[Tuple[torch.Tensor, str]] Samples produced by dataloader required Returns: Type Description Tuple[torch.Tensor, torch.Tensor, List[str]] Tuple containing padded audios, audio lengths (normalized to 0.0 <-> 1.0 range) and the list of corresponding transcriptions in that order. Source code in thunder/data/dataloader_utils.py def asr_collate ( samples : List [ Tuple [ Tensor , str ]]) -> Tuple [ Tensor , Tensor , List [ str ]]: \"\"\"Function that collect samples and adds padding. Args: samples : Samples produced by dataloader Returns: Tuple containing padded audios, audio lengths (normalized to 0.0 <-> 1.0 range) and the list of corresponding transcriptions in that order. \"\"\" samples = sorted ( samples , key = lambda sample : sample [ 0 ] . size ( - 1 ), reverse = True ) padded_audios = pad_sequence ([ s [ 0 ] . squeeze () for s in samples ], batch_first = True ) audio_lengths = Tensor ([ s [ 0 ] . size ( - 1 ) for s in samples ]) audio_lengths = audio_lengths / audio_lengths . max () # Normalize by max length texts = [ s [ 1 ] for s in samples ] return ( padded_audios , audio_lengths , texts )","title":"Dataloader utils"},{"location":"api/Data/dataloader%20utils/#thunder.data.dataloader_utils.asr_collate","text":"Function that collect samples and adds padding. Parameters: Name Type Description Default samples List[Tuple[torch.Tensor, str]] Samples produced by dataloader required Returns: Type Description Tuple[torch.Tensor, torch.Tensor, List[str]] Tuple containing padded audios, audio lengths (normalized to 0.0 <-> 1.0 range) and the list of corresponding transcriptions in that order. Source code in thunder/data/dataloader_utils.py def asr_collate ( samples : List [ Tuple [ Tensor , str ]]) -> Tuple [ Tensor , Tensor , List [ str ]]: \"\"\"Function that collect samples and adds padding. Args: samples : Samples produced by dataloader Returns: Tuple containing padded audios, audio lengths (normalized to 0.0 <-> 1.0 range) and the list of corresponding transcriptions in that order. \"\"\" samples = sorted ( samples , key = lambda sample : sample [ 0 ] . size ( - 1 ), reverse = True ) padded_audios = pad_sequence ([ s [ 0 ] . squeeze () for s in samples ], batch_first = True ) audio_lengths = Tensor ([ s [ 0 ] . size ( - 1 ) for s in samples ]) audio_lengths = audio_lengths / audio_lengths . max () # Normalize by max length texts = [ s [ 1 ] for s in samples ] return ( padded_audios , audio_lengths , texts )","title":"asr_collate()"},{"location":"api/Data/datamodule/","text":"BaseDataModule steps_per_epoch : int property readonly Number of steps for each training epoch. Used for learning rate scheduling. Returns: Type Description int Number of steps get_dataset ( self , split ) Function to get the corresponding dataset to the specified split. This should be implemented by subclasses. Parameters: Name Type Description Default split str One of \"train\", \"valid\" or \"test\". required Returns: Type Description BaseSpeechDataset The corresponding dataset. Source code in thunder/data/datamodule.py def get_dataset ( self , split : str ) -> BaseSpeechDataset : \"\"\"Function to get the corresponding dataset to the specified split. This should be implemented by subclasses. Args: split : One of \"train\", \"valid\" or \"test\". Returns: The corresponding dataset. \"\"\" raise NotImplementedError () test_dataloader ( self ) Implement one or multiple PyTorch DataLoaders for testing. The dataloader you return will not be called every epoch unless you set :paramref: ~pytorch_lightning.trainer.Trainer.reload_dataloaders_every_epoch to True . For data processing use the following pattern: - download in :meth:`prepare_data` - process and split in :meth:`setup` However, the above are only necessary for distributed processing. .. warning:: do not assign state in prepare_data :meth: ~pytorch_lightning.trainer.Trainer.fit ... :meth: prepare_data :meth: setup :meth: train_dataloader :meth: val_dataloader :meth: test_dataloader Note Lightning adds the correct sampler for distributed and arbitrary hardware. There is no need to set it yourself. Returns: Type Description DataLoader Single or multiple PyTorch DataLoaders. Example:: def test_dataloader(self): transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (1.0,))]) dataset = MNIST(root='/path/to/mnist/', train=False, transform=transform, download=True) loader = torch.utils.data.DataLoader( dataset=dataset, batch_size=self.batch_size, shuffle=False ) return loader # can also return multiple dataloaders def test_dataloader(self): return [loader_a, loader_b, ..., loader_n] Note If you don't need a test dataset and a :meth: test_step , you don't need to implement this method. Note In the case where you return multiple test dataloaders, the :meth: test_step will have an argument dataloader_idx which matches the order here. Source code in thunder/data/datamodule.py def test_dataloader ( self ) -> DataLoader : return DataLoader ( self . test_dataset , batch_size = self . batch_size , shuffle = False , collate_fn = asr_collate , num_workers = self . num_workers , pin_memory = True , ) train_dataloader ( self ) Implement one or more PyTorch DataLoaders for training. Returns: Type Description DataLoader Either a single PyTorch :class: ~torch.utils.data.DataLoader or a collection of these (list, dict, nested lists and dicts). In the case of multiple dataloaders, please see this :ref: page <multiple-training-dataloaders> The dataloader you return will not be called every epoch unless you set :paramref: ~pytorch_lightning.trainer.Trainer.reload_dataloaders_every_epoch to True . For data processing use the following pattern: - download in :meth:`prepare_data` - process and split in :meth:`setup` However, the above are only necessary for distributed processing. .. warning:: do not assign state in prepare_data :meth: ~pytorch_lightning.trainer.Trainer.fit ... :meth: prepare_data :meth: setup :meth: train_dataloader Note Lightning adds the correct sampler for distributed and arbitrary hardware. There is no need to set it yourself. Example:: # single dataloader def train_dataloader(self): transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (1.0,))]) dataset = MNIST(root='/path/to/mnist/', train=True, transform=transform, download=True) loader = torch.utils.data.DataLoader( dataset=dataset, batch_size=self.batch_size, shuffle=True ) return loader # multiple dataloaders, return as list def train_dataloader(self): mnist = MNIST(...) cifar = CIFAR(...) mnist_loader = torch.utils.data.DataLoader( dataset=mnist, batch_size=self.batch_size, shuffle=True ) cifar_loader = torch.utils.data.DataLoader( dataset=cifar, batch_size=self.batch_size, shuffle=True ) # each batch will be a list of tensors: [batch_mnist, batch_cifar] return [mnist_loader, cifar_loader] # multiple dataloader, return as dict def train_dataloader(self): mnist = MNIST(...) cifar = CIFAR(...) mnist_loader = torch.utils.data.DataLoader( dataset=mnist, batch_size=self.batch_size, shuffle=True ) cifar_loader = torch.utils.data.DataLoader( dataset=cifar, batch_size=self.batch_size, shuffle=True ) # each batch will be a dict of tensors: {'mnist': batch_mnist, 'cifar': batch_cifar} return {'mnist': mnist_loader, 'cifar': cifar_loader} Source code in thunder/data/datamodule.py def train_dataloader ( self ) -> DataLoader : return DataLoader ( self . train_dataset , batch_size = self . batch_size , collate_fn = asr_collate , num_workers = self . num_workers , shuffle = True , pin_memory = True , ) val_dataloader ( self ) Implement one or multiple PyTorch DataLoaders for validation. The dataloader you return will not be called every epoch unless you set :paramref: ~pytorch_lightning.trainer.Trainer.reload_dataloaders_every_epoch to True . It's recommended that all data downloads and preparation happen in :meth: prepare_data . :meth: ~pytorch_lightning.trainer.Trainer.fit ... :meth: prepare_data :meth: train_dataloader :meth: val_dataloader :meth: test_dataloader Note Lightning adds the correct sampler for distributed and arbitrary hardware There is no need to set it yourself. Returns: Type Description DataLoader Single or multiple PyTorch DataLoaders. Examples:: def val_dataloader(self): transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (1.0,))]) dataset = MNIST(root='/path/to/mnist/', train=False, transform=transform, download=True) loader = torch.utils.data.DataLoader( dataset=dataset, batch_size=self.batch_size, shuffle=False ) return loader # can also return multiple dataloaders def val_dataloader(self): return [loader_a, loader_b, ..., loader_n] Note If you don't need a validation dataset and a :meth: validation_step , you don't need to implement this method. Note In the case where you return multiple validation dataloaders, the :meth: validation_step will have an argument dataloader_idx which matches the order here. Source code in thunder/data/datamodule.py def val_dataloader ( self ) -> DataLoader : return DataLoader ( self . val_dataset , batch_size = self . batch_size , shuffle = False , collate_fn = asr_collate , num_workers = self . num_workers , pin_memory = True , ) ManifestDatamodule get_dataset ( self , split ) Function to get the corresponding dataset to the specified split. This should be implemented by subclasses. Parameters: Name Type Description Default split str One of \"train\", \"valid\" or \"test\". required Returns: Type Description ManifestSpeechDataset The corresponding dataset. Source code in thunder/data/datamodule.py def get_dataset ( self , split : str ) -> ManifestSpeechDataset : return ManifestSpeechDataset ( self . manifest_mapping [ split ], self . force_mono , self . sample_rate )","title":"Datamodule"},{"location":"api/Data/datamodule/#thunder.data.datamodule.BaseDataModule","text":"","title":"BaseDataModule"},{"location":"api/Data/datamodule/#thunder.data.datamodule.BaseDataModule.steps_per_epoch","text":"Number of steps for each training epoch. Used for learning rate scheduling. Returns: Type Description int Number of steps","title":"steps_per_epoch"},{"location":"api/Data/datamodule/#thunder.data.datamodule.BaseDataModule.get_dataset","text":"Function to get the corresponding dataset to the specified split. This should be implemented by subclasses. Parameters: Name Type Description Default split str One of \"train\", \"valid\" or \"test\". required Returns: Type Description BaseSpeechDataset The corresponding dataset. Source code in thunder/data/datamodule.py def get_dataset ( self , split : str ) -> BaseSpeechDataset : \"\"\"Function to get the corresponding dataset to the specified split. This should be implemented by subclasses. Args: split : One of \"train\", \"valid\" or \"test\". Returns: The corresponding dataset. \"\"\" raise NotImplementedError ()","title":"get_dataset()"},{"location":"api/Data/datamodule/#thunder.data.datamodule.BaseDataModule.test_dataloader","text":"Implement one or multiple PyTorch DataLoaders for testing. The dataloader you return will not be called every epoch unless you set :paramref: ~pytorch_lightning.trainer.Trainer.reload_dataloaders_every_epoch to True . For data processing use the following pattern: - download in :meth:`prepare_data` - process and split in :meth:`setup` However, the above are only necessary for distributed processing. .. warning:: do not assign state in prepare_data :meth: ~pytorch_lightning.trainer.Trainer.fit ... :meth: prepare_data :meth: setup :meth: train_dataloader :meth: val_dataloader :meth: test_dataloader Note Lightning adds the correct sampler for distributed and arbitrary hardware. There is no need to set it yourself. Returns: Type Description DataLoader Single or multiple PyTorch DataLoaders. Example:: def test_dataloader(self): transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (1.0,))]) dataset = MNIST(root='/path/to/mnist/', train=False, transform=transform, download=True) loader = torch.utils.data.DataLoader( dataset=dataset, batch_size=self.batch_size, shuffle=False ) return loader # can also return multiple dataloaders def test_dataloader(self): return [loader_a, loader_b, ..., loader_n] Note If you don't need a test dataset and a :meth: test_step , you don't need to implement this method. Note In the case where you return multiple test dataloaders, the :meth: test_step will have an argument dataloader_idx which matches the order here. Source code in thunder/data/datamodule.py def test_dataloader ( self ) -> DataLoader : return DataLoader ( self . test_dataset , batch_size = self . batch_size , shuffle = False , collate_fn = asr_collate , num_workers = self . num_workers , pin_memory = True , )","title":"test_dataloader()"},{"location":"api/Data/datamodule/#thunder.data.datamodule.BaseDataModule.train_dataloader","text":"Implement one or more PyTorch DataLoaders for training. Returns: Type Description DataLoader Either a single PyTorch :class: ~torch.utils.data.DataLoader or a collection of these (list, dict, nested lists and dicts). In the case of multiple dataloaders, please see this :ref: page <multiple-training-dataloaders> The dataloader you return will not be called every epoch unless you set :paramref: ~pytorch_lightning.trainer.Trainer.reload_dataloaders_every_epoch to True . For data processing use the following pattern: - download in :meth:`prepare_data` - process and split in :meth:`setup` However, the above are only necessary for distributed processing. .. warning:: do not assign state in prepare_data :meth: ~pytorch_lightning.trainer.Trainer.fit ... :meth: prepare_data :meth: setup :meth: train_dataloader Note Lightning adds the correct sampler for distributed and arbitrary hardware. There is no need to set it yourself. Example:: # single dataloader def train_dataloader(self): transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (1.0,))]) dataset = MNIST(root='/path/to/mnist/', train=True, transform=transform, download=True) loader = torch.utils.data.DataLoader( dataset=dataset, batch_size=self.batch_size, shuffle=True ) return loader # multiple dataloaders, return as list def train_dataloader(self): mnist = MNIST(...) cifar = CIFAR(...) mnist_loader = torch.utils.data.DataLoader( dataset=mnist, batch_size=self.batch_size, shuffle=True ) cifar_loader = torch.utils.data.DataLoader( dataset=cifar, batch_size=self.batch_size, shuffle=True ) # each batch will be a list of tensors: [batch_mnist, batch_cifar] return [mnist_loader, cifar_loader] # multiple dataloader, return as dict def train_dataloader(self): mnist = MNIST(...) cifar = CIFAR(...) mnist_loader = torch.utils.data.DataLoader( dataset=mnist, batch_size=self.batch_size, shuffle=True ) cifar_loader = torch.utils.data.DataLoader( dataset=cifar, batch_size=self.batch_size, shuffle=True ) # each batch will be a dict of tensors: {'mnist': batch_mnist, 'cifar': batch_cifar} return {'mnist': mnist_loader, 'cifar': cifar_loader} Source code in thunder/data/datamodule.py def train_dataloader ( self ) -> DataLoader : return DataLoader ( self . train_dataset , batch_size = self . batch_size , collate_fn = asr_collate , num_workers = self . num_workers , shuffle = True , pin_memory = True , )","title":"train_dataloader()"},{"location":"api/Data/datamodule/#thunder.data.datamodule.BaseDataModule.val_dataloader","text":"Implement one or multiple PyTorch DataLoaders for validation. The dataloader you return will not be called every epoch unless you set :paramref: ~pytorch_lightning.trainer.Trainer.reload_dataloaders_every_epoch to True . It's recommended that all data downloads and preparation happen in :meth: prepare_data . :meth: ~pytorch_lightning.trainer.Trainer.fit ... :meth: prepare_data :meth: train_dataloader :meth: val_dataloader :meth: test_dataloader Note Lightning adds the correct sampler for distributed and arbitrary hardware There is no need to set it yourself. Returns: Type Description DataLoader Single or multiple PyTorch DataLoaders. Examples:: def val_dataloader(self): transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (1.0,))]) dataset = MNIST(root='/path/to/mnist/', train=False, transform=transform, download=True) loader = torch.utils.data.DataLoader( dataset=dataset, batch_size=self.batch_size, shuffle=False ) return loader # can also return multiple dataloaders def val_dataloader(self): return [loader_a, loader_b, ..., loader_n] Note If you don't need a validation dataset and a :meth: validation_step , you don't need to implement this method. Note In the case where you return multiple validation dataloaders, the :meth: validation_step will have an argument dataloader_idx which matches the order here. Source code in thunder/data/datamodule.py def val_dataloader ( self ) -> DataLoader : return DataLoader ( self . val_dataset , batch_size = self . batch_size , shuffle = False , collate_fn = asr_collate , num_workers = self . num_workers , pin_memory = True , )","title":"val_dataloader()"},{"location":"api/Data/datamodule/#thunder.data.datamodule.ManifestDatamodule","text":"","title":"ManifestDatamodule"},{"location":"api/Data/datamodule/#thunder.data.datamodule.ManifestDatamodule.get_dataset","text":"Function to get the corresponding dataset to the specified split. This should be implemented by subclasses. Parameters: Name Type Description Default split str One of \"train\", \"valid\" or \"test\". required Returns: Type Description ManifestSpeechDataset The corresponding dataset. Source code in thunder/data/datamodule.py def get_dataset ( self , split : str ) -> ManifestSpeechDataset : return ManifestSpeechDataset ( self . manifest_mapping [ split ], self . force_mono , self . sample_rate )","title":"get_dataset()"},{"location":"api/Data/dataset/","text":"BaseSpeechDataset __init__ ( self , items , force_mono = True , sample_rate = 16000 ) special This is the base class that implements the minimal functionality to have a compatible speech dataset, in a way that can be easily customized by subclassing. Parameters: Name Type Description Default items Iterable Source of items in the dataset, sorted by audio duration. This can be a list of files, a pandas dataframe or any other iterable structure where you record your data. required force_mono bool If true, convert all the loaded samples to mono. True sample_rate int Sample rate used by the dataset. All of the samples that have different rate will be resampled. 16000 Source code in thunder/data/dataset.py def __init__ ( self , items : Iterable , force_mono : bool = True , sample_rate : int = 16000 ): \"\"\"This is the base class that implements the minimal functionality to have a compatible speech dataset, in a way that can be easily customized by subclassing. Args: items : Source of items in the dataset, sorted by audio duration. This can be a list of files, a pandas dataframe or any other iterable structure where you record your data. force_mono : If true, convert all the loaded samples to mono. sample_rate : Sample rate used by the dataset. All of the samples that have different rate will be resampled. \"\"\" super () . __init__ () self . items = items self . sample_rate = sample_rate self . force_mono = force_mono all_outputs ( self ) Return a list with just the outputs for the whole dataset. Useful when creating the initial vocab tokens, or to train a language model. Returns: Type Description List[str] All of the outputs of the dataset, with the corresponding preprocessing applied. Source code in thunder/data/dataset.py def all_outputs ( self ) -> List [ str ]: \"\"\"Return a list with just the outputs for the whole dataset. Useful when creating the initial vocab tokens, or to train a language model. Returns: All of the outputs of the dataset, with the corresponding preprocessing applied. \"\"\" outputs = [] for index in range ( len ( self )): item = self . get_item ( index ) text = self . open_text ( item ) text = self . preprocess_text ( text ) outputs . append ( text ) return outputs get_item ( self , index ) Get the item source specified by the index. Parameters: Name Type Description Default index int Indicates what item it needs to return information about. required Returns: Type Description Any Whatever data necessary to open the audio and text corresponding to this index. Source code in thunder/data/dataset.py def get_item ( self , index : int ) -> Any : \"\"\"Get the item source specified by the index. Args: index : Indicates what item it needs to return information about. Returns: Whatever data necessary to open the audio and text corresponding to this index. \"\"\" return self . items [ index ] open_audio ( self , item ) Uses the data returned by get_item to open the audio Parameters: Name Type Description Default item Any Data returned by get_item(index) required Returns: Type Description Tuple[torch.Tensor, int] Tuple containing the audio tensor with shape (channels, time), and the corresponding sample rate. Source code in thunder/data/dataset.py def open_audio ( self , item : Any ) -> Tuple [ Tensor , int ]: \"\"\"Uses the data returned by get_item to open the audio Args: item : Data returned by get_item(index) Returns: Tuple containing the audio tensor with shape (channels, time), and the corresponding sample rate. \"\"\" return torchaudio . load ( item ) open_text ( self , item ) Opens the transcription based on the data returned by get_item(index) Parameters: Name Type Description Default item Any The data returned by get_item. required Returns: Type Description str The transcription corresponding to the item. Source code in thunder/data/dataset.py def open_text ( self , item : Any ) -> str : \"\"\"Opens the transcription based on the data returned by get_item(index) Args: item : The data returned by get_item. Returns: The transcription corresponding to the item. \"\"\" raise NotImplementedError () preprocess_audio ( self , audio , sample_rate ) Apply some base transforms to the audio, that fix silent problems. It transforms all the audios to mono (depending on class creation parameter), remove the possible DC bias present and then resamples the audios to a common sample rate. Parameters: Name Type Description Default audio Tensor Audio tensor required sample_rate int Sample rate required Returns: Type Description Tensor Audio tensor after the transforms. Source code in thunder/data/dataset.py def preprocess_audio ( self , audio : Tensor , sample_rate : int ) -> Tensor : \"\"\"Apply some base transforms to the audio, that fix silent problems. It transforms all the audios to mono (depending on class creation parameter), remove the possible DC bias present and then resamples the audios to a common sample rate. Args: audio : Audio tensor sample_rate : Sample rate Returns: Audio tensor after the transforms. \"\"\" if self . force_mono and ( audio . shape [ 0 ] > 1 ): audio = audio . mean ( 0 , keepdim = True ) # Removing the dc component from the audio # It happens when a faulty capture device introduce # an offset into the recorded waveform, and this can # cause problems with later transforms. # https://en.wikipedia.org/wiki/DC_bias audio = audio - audio . mean ( 1 ) if self . sample_rate != sample_rate : tfm = torchaudio . transforms . Resample ( orig_freq = sample_rate , new_freq = self . sample_rate ) audio = tfm ( audio ) return audio ManifestSpeechDataset __init__ ( self , file , force_mono , sample_rate ) special Dataset that loads from nemo manifest files. Parameters: Name Type Description Default file Union[str, pathlib.Path] Nemo manifest file. required force_mono bool If true, convert all the loaded samples to mono. required sample_rate int Sample rate used by the dataset. All of the samples that have different rate will be resampled. required Source code in thunder/data/dataset.py def __init__ ( self , file : Union [ str , Path ], force_mono : bool , sample_rate : int ): \"\"\"Dataset that loads from nemo manifest files. Args: file : Nemo manifest file. force_mono : If true, convert all the loaded samples to mono. sample_rate : Sample rate used by the dataset. All of the samples that have different rate will be resampled. \"\"\" file = Path ( file ) # Reading from the manifest file items = [ json . loads ( line ) for line in file . read_text () . strip () . splitlines ()] super () . __init__ ( items , force_mono = force_mono , sample_rate = sample_rate ) open_audio ( self , item ) Uses the data returned by get_item to open the audio Parameters: Name Type Description Default item dict Data returned by get_item(index) required Returns: Type Description Tuple[torch.Tensor, int] Tuple containing the audio tensor with shape (channels, time), and the corresponding sample rate. Source code in thunder/data/dataset.py def open_audio ( self , item : dict ) -> Tuple [ Tensor , int ]: return torchaudio . load ( item [ \"audio_filepath\" ]) open_text ( self , item ) Opens the transcription based on the data returned by get_item(index) Parameters: Name Type Description Default item dict The data returned by get_item. required Returns: Type Description str The transcription corresponding to the item. Source code in thunder/data/dataset.py def open_text ( self , item : dict ) -> str : return item [ \"text\" ]","title":"Dataset"},{"location":"api/Data/dataset/#thunder.data.dataset.BaseSpeechDataset","text":"","title":"BaseSpeechDataset"},{"location":"api/Data/dataset/#thunder.data.dataset.BaseSpeechDataset.__init__","text":"This is the base class that implements the minimal functionality to have a compatible speech dataset, in a way that can be easily customized by subclassing. Parameters: Name Type Description Default items Iterable Source of items in the dataset, sorted by audio duration. This can be a list of files, a pandas dataframe or any other iterable structure where you record your data. required force_mono bool If true, convert all the loaded samples to mono. True sample_rate int Sample rate used by the dataset. All of the samples that have different rate will be resampled. 16000 Source code in thunder/data/dataset.py def __init__ ( self , items : Iterable , force_mono : bool = True , sample_rate : int = 16000 ): \"\"\"This is the base class that implements the minimal functionality to have a compatible speech dataset, in a way that can be easily customized by subclassing. Args: items : Source of items in the dataset, sorted by audio duration. This can be a list of files, a pandas dataframe or any other iterable structure where you record your data. force_mono : If true, convert all the loaded samples to mono. sample_rate : Sample rate used by the dataset. All of the samples that have different rate will be resampled. \"\"\" super () . __init__ () self . items = items self . sample_rate = sample_rate self . force_mono = force_mono","title":"__init__()"},{"location":"api/Data/dataset/#thunder.data.dataset.BaseSpeechDataset.all_outputs","text":"Return a list with just the outputs for the whole dataset. Useful when creating the initial vocab tokens, or to train a language model. Returns: Type Description List[str] All of the outputs of the dataset, with the corresponding preprocessing applied. Source code in thunder/data/dataset.py def all_outputs ( self ) -> List [ str ]: \"\"\"Return a list with just the outputs for the whole dataset. Useful when creating the initial vocab tokens, or to train a language model. Returns: All of the outputs of the dataset, with the corresponding preprocessing applied. \"\"\" outputs = [] for index in range ( len ( self )): item = self . get_item ( index ) text = self . open_text ( item ) text = self . preprocess_text ( text ) outputs . append ( text ) return outputs","title":"all_outputs()"},{"location":"api/Data/dataset/#thunder.data.dataset.BaseSpeechDataset.get_item","text":"Get the item source specified by the index. Parameters: Name Type Description Default index int Indicates what item it needs to return information about. required Returns: Type Description Any Whatever data necessary to open the audio and text corresponding to this index. Source code in thunder/data/dataset.py def get_item ( self , index : int ) -> Any : \"\"\"Get the item source specified by the index. Args: index : Indicates what item it needs to return information about. Returns: Whatever data necessary to open the audio and text corresponding to this index. \"\"\" return self . items [ index ]","title":"get_item()"},{"location":"api/Data/dataset/#thunder.data.dataset.BaseSpeechDataset.open_audio","text":"Uses the data returned by get_item to open the audio Parameters: Name Type Description Default item Any Data returned by get_item(index) required Returns: Type Description Tuple[torch.Tensor, int] Tuple containing the audio tensor with shape (channels, time), and the corresponding sample rate. Source code in thunder/data/dataset.py def open_audio ( self , item : Any ) -> Tuple [ Tensor , int ]: \"\"\"Uses the data returned by get_item to open the audio Args: item : Data returned by get_item(index) Returns: Tuple containing the audio tensor with shape (channels, time), and the corresponding sample rate. \"\"\" return torchaudio . load ( item )","title":"open_audio()"},{"location":"api/Data/dataset/#thunder.data.dataset.BaseSpeechDataset.open_text","text":"Opens the transcription based on the data returned by get_item(index) Parameters: Name Type Description Default item Any The data returned by get_item. required Returns: Type Description str The transcription corresponding to the item. Source code in thunder/data/dataset.py def open_text ( self , item : Any ) -> str : \"\"\"Opens the transcription based on the data returned by get_item(index) Args: item : The data returned by get_item. Returns: The transcription corresponding to the item. \"\"\" raise NotImplementedError ()","title":"open_text()"},{"location":"api/Data/dataset/#thunder.data.dataset.BaseSpeechDataset.preprocess_audio","text":"Apply some base transforms to the audio, that fix silent problems. It transforms all the audios to mono (depending on class creation parameter), remove the possible DC bias present and then resamples the audios to a common sample rate. Parameters: Name Type Description Default audio Tensor Audio tensor required sample_rate int Sample rate required Returns: Type Description Tensor Audio tensor after the transforms. Source code in thunder/data/dataset.py def preprocess_audio ( self , audio : Tensor , sample_rate : int ) -> Tensor : \"\"\"Apply some base transforms to the audio, that fix silent problems. It transforms all the audios to mono (depending on class creation parameter), remove the possible DC bias present and then resamples the audios to a common sample rate. Args: audio : Audio tensor sample_rate : Sample rate Returns: Audio tensor after the transforms. \"\"\" if self . force_mono and ( audio . shape [ 0 ] > 1 ): audio = audio . mean ( 0 , keepdim = True ) # Removing the dc component from the audio # It happens when a faulty capture device introduce # an offset into the recorded waveform, and this can # cause problems with later transforms. # https://en.wikipedia.org/wiki/DC_bias audio = audio - audio . mean ( 1 ) if self . sample_rate != sample_rate : tfm = torchaudio . transforms . Resample ( orig_freq = sample_rate , new_freq = self . sample_rate ) audio = tfm ( audio ) return audio","title":"preprocess_audio()"},{"location":"api/Data/dataset/#thunder.data.dataset.ManifestSpeechDataset","text":"","title":"ManifestSpeechDataset"},{"location":"api/Data/dataset/#thunder.data.dataset.ManifestSpeechDataset.__init__","text":"Dataset that loads from nemo manifest files. Parameters: Name Type Description Default file Union[str, pathlib.Path] Nemo manifest file. required force_mono bool If true, convert all the loaded samples to mono. required sample_rate int Sample rate used by the dataset. All of the samples that have different rate will be resampled. required Source code in thunder/data/dataset.py def __init__ ( self , file : Union [ str , Path ], force_mono : bool , sample_rate : int ): \"\"\"Dataset that loads from nemo manifest files. Args: file : Nemo manifest file. force_mono : If true, convert all the loaded samples to mono. sample_rate : Sample rate used by the dataset. All of the samples that have different rate will be resampled. \"\"\" file = Path ( file ) # Reading from the manifest file items = [ json . loads ( line ) for line in file . read_text () . strip () . splitlines ()] super () . __init__ ( items , force_mono = force_mono , sample_rate = sample_rate )","title":"__init__()"},{"location":"api/Data/dataset/#thunder.data.dataset.ManifestSpeechDataset.open_audio","text":"Uses the data returned by get_item to open the audio Parameters: Name Type Description Default item dict Data returned by get_item(index) required Returns: Type Description Tuple[torch.Tensor, int] Tuple containing the audio tensor with shape (channels, time), and the corresponding sample rate. Source code in thunder/data/dataset.py def open_audio ( self , item : dict ) -> Tuple [ Tensor , int ]: return torchaudio . load ( item [ \"audio_filepath\" ])","title":"open_audio()"},{"location":"api/Data/dataset/#thunder.data.dataset.ManifestSpeechDataset.open_text","text":"Opens the transcription based on the data returned by get_item(index) Parameters: Name Type Description Default item dict The data returned by get_item. required Returns: Type Description str The transcription corresponding to the item. Source code in thunder/data/dataset.py def open_text ( self , item : dict ) -> str : return item [ \"text\" ]","title":"open_text()"},{"location":"api/Quartznet/blocks/","text":"Basic building blocks to create the Quartznet model InitMode Weight init methods. Used by init_weights . Note Possible values are xavier_uniform , xavier_normal , kaiming_uniform and kaiming_normal Masked forward ( self , x , lens ) Defines the computation performed at every call. Should be overridden by all subclasses. .. note:: Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them. Source code in thunder/quartznet/blocks.py def forward ( self , x : torch . Tensor , lens : torch . Tensor ): return self . layer ( x ), lens MaskedConv1d __init__ ( self , in_channels , out_channels , kernel_size , stride = 1 , padding = 0 , dilation = 1 , groups = 1 , bias = False , use_mask = True ) special Masked Convolution. This module correspond to a 1d convolution with input masking. Arguments to create are the same as nn.Conv1d, but with the addition of use_mask for special behaviour. Parameters: Name Type Description Default in_channels int Same as nn.Conv1d required out_channels int Same as nn.Conv1d required kernel_size int Same as nn.Conv1d required stride int Same as nn.Conv1d 1 padding int Same as nn.Conv1d 0 dilation int Same as nn.Conv1d 1 groups int Same as nn.Conv1d 1 bias bool Same as nn.Conv1d False use_mask bool Controls the masking of input before the convolution during the forward. True Source code in thunder/quartznet/blocks.py def __init__ ( self , in_channels : int , out_channels : int , kernel_size : int , stride : int = 1 , padding : int = 0 , dilation : int = 1 , groups : int = 1 , bias : bool = False , use_mask : bool = True , ): \"\"\"Masked Convolution. This module correspond to a 1d convolution with input masking. Arguments to create are the same as nn.Conv1d, but with the addition of use_mask for special behaviour. Args: in_channels : Same as nn.Conv1d out_channels : Same as nn.Conv1d kernel_size : Same as nn.Conv1d stride : Same as nn.Conv1d padding : Same as nn.Conv1d dilation : Same as nn.Conv1d groups : Same as nn.Conv1d bias : Same as nn.Conv1d use_mask : Controls the masking of input before the convolution during the forward. \"\"\" super ( MaskedConv1d , self ) . __init__ () self . use_mask = use_mask self . conv = nn . Conv1d ( in_channels , out_channels , kernel_size , stride = stride , padding = padding , dilation = dilation , groups = groups , bias = bias , ) self . padding = self . conv . padding [ 0 ] self . dilation = self . conv . dilation [ 0 ] self . kernel_size = self . conv . kernel_size [ 0 ] self . stride = self . conv . stride [ 0 ] forward ( self , x , lens ) Forward method Parameters: Name Type Description Default x Tensor Signal to be processed, of shape (batch, features, time) required lens Tensor Lenghts of each element in the batch of x, with shape (batch) required Returns: Type Description Tuple[torch.Tensor, torch.Tensor] Both the signal processed by the convolution and the resulting lengths Source code in thunder/quartznet/blocks.py def forward ( self , x : torch . Tensor , lens : torch . Tensor ) -> Tuple [ torch . Tensor , torch . Tensor ]: \"\"\"Forward method Args: x : Signal to be processed, of shape (batch, features, time) lens : Lenghts of each element in the batch of x, with shape (batch) Returns: Both the signal processed by the convolution and the resulting lengths \"\"\" if self . use_mask : x = self . mask_fill ( x , lens ) out = self . conv ( x ) return out , self . get_seq_len ( lens ) get_seq_len ( self , lens ) Get the lengths of the inputs after the convolution operation is applied. Parameters: Name Type Description Default lens Tensor Original lengths of the inputs required Returns: Type Description Tensor Resulting lengths after the convolution Source code in thunder/quartznet/blocks.py def get_seq_len ( self , lens : torch . Tensor ) -> torch . Tensor : \"\"\"Get the lengths of the inputs after the convolution operation is applied. Args: lens : Original lengths of the inputs Returns: Resulting lengths after the convolution \"\"\" return ( lens + 2 * self . padding - self . dilation * ( self . kernel_size - 1 ) - 1 ) // self . stride + 1 mask_fill ( self , x , lens ) Mask the input based on it's respective lengths. Parameters: Name Type Description Default x Tensor Signal to be processed, of shape (batch, features, time) required lens Tensor Lenghts of each element in the batch of x, with shape (batch) required Returns: Type Description Tensor The masked signal Source code in thunder/quartznet/blocks.py def mask_fill ( self , x : torch . Tensor , lens : torch . Tensor ) -> torch . Tensor : \"\"\"Mask the input based on it's respective lengths. Args: x : Signal to be processed, of shape (batch, features, time) lens : Lenghts of each element in the batch of x, with shape (batch) Returns: The masked signal \"\"\" lens = lens . to ( dtype = torch . long ) max_len = x . size ( 2 ) mask = torch . arange ( max_len , device = lens . device ) . expand ( lens . shape [ 0 ], max_len ) >= lens . unsqueeze ( 1 ) x = x . masked_fill ( mask . unsqueeze ( 1 ) . to ( device = x . device ), 0 ) return x MultiSequential forward ( self , x1 , x2 ) Defines the computation performed at every call. Should be overridden by all subclasses. .. note:: Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them. Source code in thunder/quartznet/blocks.py def forward ( self , x1 , x2 ): for module in self . children (): x1 , x2 = module ( x1 , x2 ) return x1 , x2 QuartznetBlock __init__ ( self , in_channels , out_channels , repeat = 5 , kernel_size = [ 11 ], stride = [ 1 ], dilation = [ 1 ], dropout = 0.0 , residual = True , separable = False ) special Quartznet block. This is a refactoring of the Jasperblock present on the NeMo toolkit, but simplified to only support the new quartznet model. Biggest change is that dense residual used on Jasper is not supported here, and masked convolutions were also removed. Parameters: Name Type Description Default in_channels int Number of input channels required out_channels int Number of output channels required repeat int Repetitions inside block. 5 kernel_size List[int] Kernel size. [11] stride List[int] Stride of each repetition. [1] dilation List[int] Dilation of each repetition. [1] dropout float Dropout used before each activation. 0.0 residual bool Controls the use of residual connection. True separable bool Controls the use of separable convolutions. False Source code in thunder/quartznet/blocks.py def __init__ ( self , in_channels : int , out_channels : int , repeat : int = 5 , kernel_size : List [ int ] = [ 11 ], stride : List [ int ] = [ 1 ], dilation : List [ int ] = [ 1 ], dropout : float = 0.0 , residual : bool = True , separable : bool = False , ): \"\"\"Quartznet block. This is a refactoring of the Jasperblock present on the NeMo toolkit, but simplified to only support the new quartznet model. Biggest change is that dense residual used on Jasper is not supported here, and masked convolutions were also removed. Args: in_channels : Number of input channels out_channels : Number of output channels repeat : Repetitions inside block. kernel_size : Kernel size. stride : Stride of each repetition. dilation : Dilation of each repetition. dropout : Dropout used before each activation. residual : Controls the use of residual connection. separable : Controls the use of separable convolutions. \"\"\" super () . __init__ () padding_val = get_same_padding ( kernel_size [ 0 ], stride [ 0 ], dilation [ 0 ]) inplanes_loop = in_channels conv = [] for _ in range ( repeat - 1 ): conv . extend ( self . _get_conv_bn_layer ( inplanes_loop , out_channels , kernel_size = kernel_size , stride = stride , dilation = dilation , padding = padding_val , separable = separable , bias = False , ) ) conv . extend ( self . _get_act_dropout_layer ( drop_prob = dropout )) inplanes_loop = out_channels conv . extend ( self . _get_conv_bn_layer ( inplanes_loop , out_channels , kernel_size = kernel_size , stride = stride , dilation = dilation , padding = padding_val , separable = separable , bias = False , ) ) self . mconv = MultiSequential ( * conv ) if residual : stride_residual = stride if stride [ 0 ] == 1 else stride [ 0 ] ** repeat self . res = MultiSequential ( * self . _get_conv_bn_layer ( in_channels , out_channels , kernel_size = 1 , stride = stride_residual , bias = False , ) ) else : self . res = None self . mout = MultiSequential ( * self . _get_act_dropout_layer ( drop_prob = dropout )) forward ( self , x , lens ) Parameters: Name Type Description Default x Tensor Tensor of shape (batch, features, time) where #features == inplanes required Returns: Type Description Tuple[torch.Tensor, torch.Tensor] Result of applying the block on the input Source code in thunder/quartznet/blocks.py def forward ( self , x : torch . Tensor , lens : torch . Tensor ) -> Tuple [ torch . Tensor , torch . Tensor ]: \"\"\" Args: x : Tensor of shape (batch, features, time) where #features == inplanes Returns: Result of applying the block on the input \"\"\" # compute forward convolutions out , lens_out = self . mconv ( x , lens ) # compute the residuals if self . res is not None : res_out , _ = self . res ( x , lens ) out = out + res_out # compute the output out , lens_out = self . mout ( out , lens_out ) return out , lens_out body ( filters , kernel_size , repeat_blocks = 1 ) Creates the body of the Quartznet model. That is the middle part. Parameters: Name Type Description Default filters List[int] List of filters inside each block in the body. required kernel_size List[int] Corresponding list of kernel sizes for each block. Should have the same length as the first argument. required repeat_blocks int Number of repetitions of each block inside the body. 1 Returns: Type Description List[thunder.quartznet.blocks.QuartznetBlock] List of layers that form the body of the network. Source code in thunder/quartznet/blocks.py def body ( filters : List [ int ], kernel_size : List [ int ], repeat_blocks : int = 1 ) -> List [ QuartznetBlock ]: \"\"\"Creates the body of the Quartznet model. That is the middle part. Args: filters : List of filters inside each block in the body. kernel_size : Corresponding list of kernel sizes for each block. Should have the same length as the first argument. repeat_blocks : Number of repetitions of each block inside the body. Returns: List of layers that form the body of the network. \"\"\" layers = [] f_in = 256 for f , k in zip ( filters , kernel_size ): for _ in range ( repeat_blocks ): layers . append ( QuartznetBlock ( f_in , f , kernel_size = [ k ], separable = True )) f_in = f layers . extend ( [ QuartznetBlock ( f_in , 512 , repeat = 1 , dilation = [ 2 ], kernel_size = [ 87 ], residual = False , separable = True , ), QuartznetBlock ( 512 , 1024 , repeat = 1 , kernel_size = [ 1 ], residual = False , separable = False ), ] ) return layers get_same_padding ( kernel_size , stride , dilation ) Calculates the padding size to obtain same padding. Same padding means that the output will have the shape input_shape / stride. That means, for stride = 1 the output shape is the same as the input, and stride = 2 gives an output that is half of the input shape. Parameters: Name Type Description Default kernel_size int convolution kernel size. Only tested to be correct with odd values. required stride int convolution stride required dilation int convolution dilation required Exceptions: Type Description ValueError Only stride or dilation may be greater than 1 Returns: Type Description int padding value to obtain same padding. Source code in thunder/quartznet/blocks.py def get_same_padding ( kernel_size : int , stride : int , dilation : int ) -> int : \"\"\"Calculates the padding size to obtain same padding. Same padding means that the output will have the shape input_shape / stride. That means, for stride = 1 the output shape is the same as the input, and stride = 2 gives an output that is half of the input shape. Args: kernel_size : convolution kernel size. Only tested to be correct with odd values. stride : convolution stride dilation : convolution dilation Raises: ValueError: Only stride or dilation may be greater than 1 Returns: padding value to obtain same padding. \"\"\" if stride > 1 and dilation > 1 : raise ValueError ( \"Only stride OR dilation may be greater than 1\" ) if dilation > 1 : return ( dilation * ( kernel_size - 1 ) + 1 ) // 2 return kernel_size // 2 init_weights ( m , mode =< InitMode . xavier_uniform : 'xavier_uniform' > ) Initialize Linear, MaskedConv1d/Conv1d or BatchNorm1d weights. There's no return, the operation occurs inplace. Parameters: Name Type Description Default m Module The layer to be initialized required mode InitMode Weight initialization mode. Only applicable to linear and conv layers. <InitMode.xavier_uniform: 'xavier_uniform'> Exceptions: Type Description ValueError Raised when the initial mode is not one of the possible options. Source code in thunder/quartznet/blocks.py def init_weights ( m : nn . Module , mode : InitMode = InitMode . xavier_uniform ): \"\"\"Initialize Linear, MaskedConv1d/Conv1d or BatchNorm1d weights. There's no return, the operation occurs inplace. Args: m: The layer to be initialized mode: Weight initialization mode. Only applicable to linear and conv layers. Raises: ValueError: Raised when the initial mode is not one of the possible options. \"\"\" if isinstance ( m , MaskedConv1d ): init_weights ( m . conv , mode ) if isinstance ( m , ( nn . Conv1d , nn . Linear )): if mode == InitMode . xavier_uniform : nn . init . xavier_uniform_ ( m . weight , gain = 1.0 ) elif mode == InitMode . xavier_normal : nn . init . xavier_normal_ ( m . weight , gain = 1.0 ) elif mode == InitMode . kaiming_uniform : nn . init . kaiming_uniform_ ( m . weight , nonlinearity = \"relu\" ) elif mode == InitMode . kaiming_normal : nn . init . kaiming_normal_ ( m . weight , nonlinearity = \"relu\" ) else : raise ValueError ( \"Unknown Initialization mode: {0} \" . format ( mode )) elif isinstance ( m , nn . BatchNorm1d ): if m . track_running_stats : m . running_mean . zero_ () m . running_var . fill_ ( 1 ) m . num_batches_tracked . zero_ () if m . affine : nn . init . ones_ ( m . weight ) nn . init . zeros_ ( m . bias ) Quartznet_decoder ( num_classes , input_channels = 1024 ) Build the Quartznet decoder. Parameters: Name Type Description Default num_classes int Number of output classes of the model. It's the size of the vocabulary, excluding the blank symbol. required Returns: Type Description Module Pytorch model of the decoder Source code in thunder/quartznet/blocks.py def Quartznet_decoder ( num_classes : int , input_channels : int = 1024 ) -> nn . Module : \"\"\"Build the Quartznet decoder. Args: num_classes : Number of output classes of the model. It's the size of the vocabulary, excluding the blank symbol. Returns: Pytorch model of the decoder \"\"\" decoder = nn . Conv1d ( input_channels , num_classes , kernel_size = 1 , bias = True , ) decoder . apply ( init_weights ) return decoder Quartznet_encoder ( feat_in , filters = [ 256 , 256 , 512 , 512 , 512 ], kernel_sizes = [ 33 , 39 , 51 , 63 , 75 ], repeat_blocks = 1 ) Basic Quartznet encoder setup. Can be used to build either Quartznet5x5 (repeat_blocks=1) or Quartznet15x5 (repeat_blocks=3) Parameters: Name Type Description Default feat_in int Number of input features to the model. required filters List[int] List of filter sizes used to create the encoder blocks [256, 256, 512, 512, 512] kernel_sizes List[int] List of kernel sizes corresponding to each filter size [33, 39, 51, 63, 75] repeat_blocks int Number of repetitions of each block. 1 Returns: Type Description Module Pytorch model corresponding to the encoder. Source code in thunder/quartznet/blocks.py def Quartznet_encoder ( feat_in : int , filters : List [ int ] = [ 256 , 256 , 512 , 512 , 512 ], kernel_sizes : List [ int ] = [ 33 , 39 , 51 , 63 , 75 ], repeat_blocks : int = 1 , ) -> nn . Module : \"\"\"Basic Quartznet encoder setup. Can be used to build either Quartznet5x5 (repeat_blocks=1) or Quartznet15x5 (repeat_blocks=3) Args: feat_in : Number of input features to the model. filters: List of filter sizes used to create the encoder blocks kernel_sizes: List of kernel sizes corresponding to each filter size repeat_blocks : Number of repetitions of each block. Returns: Pytorch model corresponding to the encoder. \"\"\" return MultiSequential ( stem ( feat_in ), * body ( filters , kernel_sizes , repeat_blocks ), ) stem ( feat_in ) Creates the Quartznet stem. That is the first block of the model, that process the input directly. Parameters: Name Type Description Default feat_in int Number of input features required Returns: Type Description QuartznetBlock Quartznet stem block Source code in thunder/quartznet/blocks.py def stem ( feat_in : int ) -> QuartznetBlock : \"\"\"Creates the Quartznet stem. That is the first block of the model, that process the input directly. Args: feat_in : Number of input features Returns: Quartznet stem block \"\"\" return QuartznetBlock ( feat_in , 256 , repeat = 1 , stride = [ 2 ], kernel_size = [ 33 ], residual = False , separable = True , )","title":"Blocks"},{"location":"api/Quartznet/blocks/#thunder.quartznet.blocks.InitMode","text":"Weight init methods. Used by init_weights . Note Possible values are xavier_uniform , xavier_normal , kaiming_uniform and kaiming_normal","title":"InitMode"},{"location":"api/Quartznet/blocks/#thunder.quartznet.blocks.Masked","text":"","title":"Masked"},{"location":"api/Quartznet/blocks/#thunder.quartznet.blocks.Masked.forward","text":"Defines the computation performed at every call. Should be overridden by all subclasses. .. note:: Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them. Source code in thunder/quartznet/blocks.py def forward ( self , x : torch . Tensor , lens : torch . Tensor ): return self . layer ( x ), lens","title":"forward()"},{"location":"api/Quartznet/blocks/#thunder.quartznet.blocks.MaskedConv1d","text":"","title":"MaskedConv1d"},{"location":"api/Quartznet/blocks/#thunder.quartznet.blocks.MaskedConv1d.__init__","text":"Masked Convolution. This module correspond to a 1d convolution with input masking. Arguments to create are the same as nn.Conv1d, but with the addition of use_mask for special behaviour. Parameters: Name Type Description Default in_channels int Same as nn.Conv1d required out_channels int Same as nn.Conv1d required kernel_size int Same as nn.Conv1d required stride int Same as nn.Conv1d 1 padding int Same as nn.Conv1d 0 dilation int Same as nn.Conv1d 1 groups int Same as nn.Conv1d 1 bias bool Same as nn.Conv1d False use_mask bool Controls the masking of input before the convolution during the forward. True Source code in thunder/quartznet/blocks.py def __init__ ( self , in_channels : int , out_channels : int , kernel_size : int , stride : int = 1 , padding : int = 0 , dilation : int = 1 , groups : int = 1 , bias : bool = False , use_mask : bool = True , ): \"\"\"Masked Convolution. This module correspond to a 1d convolution with input masking. Arguments to create are the same as nn.Conv1d, but with the addition of use_mask for special behaviour. Args: in_channels : Same as nn.Conv1d out_channels : Same as nn.Conv1d kernel_size : Same as nn.Conv1d stride : Same as nn.Conv1d padding : Same as nn.Conv1d dilation : Same as nn.Conv1d groups : Same as nn.Conv1d bias : Same as nn.Conv1d use_mask : Controls the masking of input before the convolution during the forward. \"\"\" super ( MaskedConv1d , self ) . __init__ () self . use_mask = use_mask self . conv = nn . Conv1d ( in_channels , out_channels , kernel_size , stride = stride , padding = padding , dilation = dilation , groups = groups , bias = bias , ) self . padding = self . conv . padding [ 0 ] self . dilation = self . conv . dilation [ 0 ] self . kernel_size = self . conv . kernel_size [ 0 ] self . stride = self . conv . stride [ 0 ]","title":"__init__()"},{"location":"api/Quartznet/blocks/#thunder.quartznet.blocks.MaskedConv1d.forward","text":"Forward method Parameters: Name Type Description Default x Tensor Signal to be processed, of shape (batch, features, time) required lens Tensor Lenghts of each element in the batch of x, with shape (batch) required Returns: Type Description Tuple[torch.Tensor, torch.Tensor] Both the signal processed by the convolution and the resulting lengths Source code in thunder/quartznet/blocks.py def forward ( self , x : torch . Tensor , lens : torch . Tensor ) -> Tuple [ torch . Tensor , torch . Tensor ]: \"\"\"Forward method Args: x : Signal to be processed, of shape (batch, features, time) lens : Lenghts of each element in the batch of x, with shape (batch) Returns: Both the signal processed by the convolution and the resulting lengths \"\"\" if self . use_mask : x = self . mask_fill ( x , lens ) out = self . conv ( x ) return out , self . get_seq_len ( lens )","title":"forward()"},{"location":"api/Quartznet/blocks/#thunder.quartznet.blocks.MaskedConv1d.get_seq_len","text":"Get the lengths of the inputs after the convolution operation is applied. Parameters: Name Type Description Default lens Tensor Original lengths of the inputs required Returns: Type Description Tensor Resulting lengths after the convolution Source code in thunder/quartznet/blocks.py def get_seq_len ( self , lens : torch . Tensor ) -> torch . Tensor : \"\"\"Get the lengths of the inputs after the convolution operation is applied. Args: lens : Original lengths of the inputs Returns: Resulting lengths after the convolution \"\"\" return ( lens + 2 * self . padding - self . dilation * ( self . kernel_size - 1 ) - 1 ) // self . stride + 1","title":"get_seq_len()"},{"location":"api/Quartznet/blocks/#thunder.quartznet.blocks.MaskedConv1d.mask_fill","text":"Mask the input based on it's respective lengths. Parameters: Name Type Description Default x Tensor Signal to be processed, of shape (batch, features, time) required lens Tensor Lenghts of each element in the batch of x, with shape (batch) required Returns: Type Description Tensor The masked signal Source code in thunder/quartznet/blocks.py def mask_fill ( self , x : torch . Tensor , lens : torch . Tensor ) -> torch . Tensor : \"\"\"Mask the input based on it's respective lengths. Args: x : Signal to be processed, of shape (batch, features, time) lens : Lenghts of each element in the batch of x, with shape (batch) Returns: The masked signal \"\"\" lens = lens . to ( dtype = torch . long ) max_len = x . size ( 2 ) mask = torch . arange ( max_len , device = lens . device ) . expand ( lens . shape [ 0 ], max_len ) >= lens . unsqueeze ( 1 ) x = x . masked_fill ( mask . unsqueeze ( 1 ) . to ( device = x . device ), 0 ) return x","title":"mask_fill()"},{"location":"api/Quartznet/blocks/#thunder.quartznet.blocks.MultiSequential","text":"","title":"MultiSequential"},{"location":"api/Quartznet/blocks/#thunder.quartznet.blocks.MultiSequential.forward","text":"Defines the computation performed at every call. Should be overridden by all subclasses. .. note:: Although the recipe for forward pass needs to be defined within this function, one should call the :class: Module instance afterwards instead of this since the former takes care of running the registered hooks while the latter silently ignores them. Source code in thunder/quartznet/blocks.py def forward ( self , x1 , x2 ): for module in self . children (): x1 , x2 = module ( x1 , x2 ) return x1 , x2","title":"forward()"},{"location":"api/Quartznet/blocks/#thunder.quartznet.blocks.QuartznetBlock","text":"","title":"QuartznetBlock"},{"location":"api/Quartznet/blocks/#thunder.quartznet.blocks.QuartznetBlock.__init__","text":"Quartznet block. This is a refactoring of the Jasperblock present on the NeMo toolkit, but simplified to only support the new quartznet model. Biggest change is that dense residual used on Jasper is not supported here, and masked convolutions were also removed. Parameters: Name Type Description Default in_channels int Number of input channels required out_channels int Number of output channels required repeat int Repetitions inside block. 5 kernel_size List[int] Kernel size. [11] stride List[int] Stride of each repetition. [1] dilation List[int] Dilation of each repetition. [1] dropout float Dropout used before each activation. 0.0 residual bool Controls the use of residual connection. True separable bool Controls the use of separable convolutions. False Source code in thunder/quartznet/blocks.py def __init__ ( self , in_channels : int , out_channels : int , repeat : int = 5 , kernel_size : List [ int ] = [ 11 ], stride : List [ int ] = [ 1 ], dilation : List [ int ] = [ 1 ], dropout : float = 0.0 , residual : bool = True , separable : bool = False , ): \"\"\"Quartznet block. This is a refactoring of the Jasperblock present on the NeMo toolkit, but simplified to only support the new quartznet model. Biggest change is that dense residual used on Jasper is not supported here, and masked convolutions were also removed. Args: in_channels : Number of input channels out_channels : Number of output channels repeat : Repetitions inside block. kernel_size : Kernel size. stride : Stride of each repetition. dilation : Dilation of each repetition. dropout : Dropout used before each activation. residual : Controls the use of residual connection. separable : Controls the use of separable convolutions. \"\"\" super () . __init__ () padding_val = get_same_padding ( kernel_size [ 0 ], stride [ 0 ], dilation [ 0 ]) inplanes_loop = in_channels conv = [] for _ in range ( repeat - 1 ): conv . extend ( self . _get_conv_bn_layer ( inplanes_loop , out_channels , kernel_size = kernel_size , stride = stride , dilation = dilation , padding = padding_val , separable = separable , bias = False , ) ) conv . extend ( self . _get_act_dropout_layer ( drop_prob = dropout )) inplanes_loop = out_channels conv . extend ( self . _get_conv_bn_layer ( inplanes_loop , out_channels , kernel_size = kernel_size , stride = stride , dilation = dilation , padding = padding_val , separable = separable , bias = False , ) ) self . mconv = MultiSequential ( * conv ) if residual : stride_residual = stride if stride [ 0 ] == 1 else stride [ 0 ] ** repeat self . res = MultiSequential ( * self . _get_conv_bn_layer ( in_channels , out_channels , kernel_size = 1 , stride = stride_residual , bias = False , ) ) else : self . res = None self . mout = MultiSequential ( * self . _get_act_dropout_layer ( drop_prob = dropout ))","title":"__init__()"},{"location":"api/Quartznet/blocks/#thunder.quartznet.blocks.QuartznetBlock.forward","text":"Parameters: Name Type Description Default x Tensor Tensor of shape (batch, features, time) where #features == inplanes required Returns: Type Description Tuple[torch.Tensor, torch.Tensor] Result of applying the block on the input Source code in thunder/quartznet/blocks.py def forward ( self , x : torch . Tensor , lens : torch . Tensor ) -> Tuple [ torch . Tensor , torch . Tensor ]: \"\"\" Args: x : Tensor of shape (batch, features, time) where #features == inplanes Returns: Result of applying the block on the input \"\"\" # compute forward convolutions out , lens_out = self . mconv ( x , lens ) # compute the residuals if self . res is not None : res_out , _ = self . res ( x , lens ) out = out + res_out # compute the output out , lens_out = self . mout ( out , lens_out ) return out , lens_out","title":"forward()"},{"location":"api/Quartznet/blocks/#thunder.quartznet.blocks.body","text":"Creates the body of the Quartznet model. That is the middle part. Parameters: Name Type Description Default filters List[int] List of filters inside each block in the body. required kernel_size List[int] Corresponding list of kernel sizes for each block. Should have the same length as the first argument. required repeat_blocks int Number of repetitions of each block inside the body. 1 Returns: Type Description List[thunder.quartznet.blocks.QuartznetBlock] List of layers that form the body of the network. Source code in thunder/quartznet/blocks.py def body ( filters : List [ int ], kernel_size : List [ int ], repeat_blocks : int = 1 ) -> List [ QuartznetBlock ]: \"\"\"Creates the body of the Quartznet model. That is the middle part. Args: filters : List of filters inside each block in the body. kernel_size : Corresponding list of kernel sizes for each block. Should have the same length as the first argument. repeat_blocks : Number of repetitions of each block inside the body. Returns: List of layers that form the body of the network. \"\"\" layers = [] f_in = 256 for f , k in zip ( filters , kernel_size ): for _ in range ( repeat_blocks ): layers . append ( QuartznetBlock ( f_in , f , kernel_size = [ k ], separable = True )) f_in = f layers . extend ( [ QuartznetBlock ( f_in , 512 , repeat = 1 , dilation = [ 2 ], kernel_size = [ 87 ], residual = False , separable = True , ), QuartznetBlock ( 512 , 1024 , repeat = 1 , kernel_size = [ 1 ], residual = False , separable = False ), ] ) return layers","title":"body()"},{"location":"api/Quartznet/blocks/#thunder.quartznet.blocks.get_same_padding","text":"Calculates the padding size to obtain same padding. Same padding means that the output will have the shape input_shape / stride. That means, for stride = 1 the output shape is the same as the input, and stride = 2 gives an output that is half of the input shape. Parameters: Name Type Description Default kernel_size int convolution kernel size. Only tested to be correct with odd values. required stride int convolution stride required dilation int convolution dilation required Exceptions: Type Description ValueError Only stride or dilation may be greater than 1 Returns: Type Description int padding value to obtain same padding. Source code in thunder/quartznet/blocks.py def get_same_padding ( kernel_size : int , stride : int , dilation : int ) -> int : \"\"\"Calculates the padding size to obtain same padding. Same padding means that the output will have the shape input_shape / stride. That means, for stride = 1 the output shape is the same as the input, and stride = 2 gives an output that is half of the input shape. Args: kernel_size : convolution kernel size. Only tested to be correct with odd values. stride : convolution stride dilation : convolution dilation Raises: ValueError: Only stride or dilation may be greater than 1 Returns: padding value to obtain same padding. \"\"\" if stride > 1 and dilation > 1 : raise ValueError ( \"Only stride OR dilation may be greater than 1\" ) if dilation > 1 : return ( dilation * ( kernel_size - 1 ) + 1 ) // 2 return kernel_size // 2","title":"get_same_padding()"},{"location":"api/Quartznet/blocks/#thunder.quartznet.blocks.init_weights","text":"Initialize Linear, MaskedConv1d/Conv1d or BatchNorm1d weights. There's no return, the operation occurs inplace. Parameters: Name Type Description Default m Module The layer to be initialized required mode InitMode Weight initialization mode. Only applicable to linear and conv layers. <InitMode.xavier_uniform: 'xavier_uniform'> Exceptions: Type Description ValueError Raised when the initial mode is not one of the possible options. Source code in thunder/quartznet/blocks.py def init_weights ( m : nn . Module , mode : InitMode = InitMode . xavier_uniform ): \"\"\"Initialize Linear, MaskedConv1d/Conv1d or BatchNorm1d weights. There's no return, the operation occurs inplace. Args: m: The layer to be initialized mode: Weight initialization mode. Only applicable to linear and conv layers. Raises: ValueError: Raised when the initial mode is not one of the possible options. \"\"\" if isinstance ( m , MaskedConv1d ): init_weights ( m . conv , mode ) if isinstance ( m , ( nn . Conv1d , nn . Linear )): if mode == InitMode . xavier_uniform : nn . init . xavier_uniform_ ( m . weight , gain = 1.0 ) elif mode == InitMode . xavier_normal : nn . init . xavier_normal_ ( m . weight , gain = 1.0 ) elif mode == InitMode . kaiming_uniform : nn . init . kaiming_uniform_ ( m . weight , nonlinearity = \"relu\" ) elif mode == InitMode . kaiming_normal : nn . init . kaiming_normal_ ( m . weight , nonlinearity = \"relu\" ) else : raise ValueError ( \"Unknown Initialization mode: {0} \" . format ( mode )) elif isinstance ( m , nn . BatchNorm1d ): if m . track_running_stats : m . running_mean . zero_ () m . running_var . fill_ ( 1 ) m . num_batches_tracked . zero_ () if m . affine : nn . init . ones_ ( m . weight ) nn . init . zeros_ ( m . bias )","title":"init_weights()"},{"location":"api/Quartznet/blocks/#thunder.quartznet.blocks.Quartznet_decoder","text":"Build the Quartznet decoder. Parameters: Name Type Description Default num_classes int Number of output classes of the model. It's the size of the vocabulary, excluding the blank symbol. required Returns: Type Description Module Pytorch model of the decoder Source code in thunder/quartznet/blocks.py def Quartznet_decoder ( num_classes : int , input_channels : int = 1024 ) -> nn . Module : \"\"\"Build the Quartznet decoder. Args: num_classes : Number of output classes of the model. It's the size of the vocabulary, excluding the blank symbol. Returns: Pytorch model of the decoder \"\"\" decoder = nn . Conv1d ( input_channels , num_classes , kernel_size = 1 , bias = True , ) decoder . apply ( init_weights ) return decoder","title":"Quartznet_decoder()"},{"location":"api/Quartznet/blocks/#thunder.quartznet.blocks.Quartznet_encoder","text":"Basic Quartznet encoder setup. Can be used to build either Quartznet5x5 (repeat_blocks=1) or Quartznet15x5 (repeat_blocks=3) Parameters: Name Type Description Default feat_in int Number of input features to the model. required filters List[int] List of filter sizes used to create the encoder blocks [256, 256, 512, 512, 512] kernel_sizes List[int] List of kernel sizes corresponding to each filter size [33, 39, 51, 63, 75] repeat_blocks int Number of repetitions of each block. 1 Returns: Type Description Module Pytorch model corresponding to the encoder. Source code in thunder/quartznet/blocks.py def Quartznet_encoder ( feat_in : int , filters : List [ int ] = [ 256 , 256 , 512 , 512 , 512 ], kernel_sizes : List [ int ] = [ 33 , 39 , 51 , 63 , 75 ], repeat_blocks : int = 1 , ) -> nn . Module : \"\"\"Basic Quartznet encoder setup. Can be used to build either Quartznet5x5 (repeat_blocks=1) or Quartznet15x5 (repeat_blocks=3) Args: feat_in : Number of input features to the model. filters: List of filter sizes used to create the encoder blocks kernel_sizes: List of kernel sizes corresponding to each filter size repeat_blocks : Number of repetitions of each block. Returns: Pytorch model corresponding to the encoder. \"\"\" return MultiSequential ( stem ( feat_in ), * body ( filters , kernel_sizes , repeat_blocks ), )","title":"Quartznet_encoder()"},{"location":"api/Quartznet/blocks/#thunder.quartznet.blocks.stem","text":"Creates the Quartznet stem. That is the first block of the model, that process the input directly. Parameters: Name Type Description Default feat_in int Number of input features required Returns: Type Description QuartznetBlock Quartznet stem block Source code in thunder/quartznet/blocks.py def stem ( feat_in : int ) -> QuartznetBlock : \"\"\"Creates the Quartznet stem. That is the first block of the model, that process the input directly. Args: feat_in : Number of input features Returns: Quartznet stem block \"\"\" return QuartznetBlock ( feat_in , 256 , repeat = 1 , stride = [ 2 ], kernel_size = [ 33 ], residual = False , separable = True , )","title":"stem()"},{"location":"api/Quartznet/compatibility/","text":"Helper functions to load the Quartznet model from original Nemo released checkpoint files. NemoCheckpoint Trained model weight checkpoints. Used by download_checkpoint and QuartznetModule.load_from_nemo . Note Possible values are QuartzNet15x5Base_En , QuartzNet15x5Base_Zh , QuartzNet5x5LS_En , QuartzNet15x5NR_En , stt_ca_quartznet15x5 , stt_it_quartznet15x5 , stt_fr_quartznet15x5 , stt_es_quartznet15x5 , stt_de_quartznet15x5 , stt_pl_quartznet15x5 , stt_ru_quartznet15x5 , stt_en_quartznet15x5 , stt_zh_quartznet15x5 download_checkpoint ( name , checkpoint_folder = None ) Download quartznet checkpoint by identifier. Parameters: Name Type Description Default name NemoCheckpoint Model identifier. Check checkpoint_archives.keys() required checkpoint_folder str Folder where the checkpoint will be saved to. None Returns: Type Description Path Path to the saved checkpoint file. Source code in thunder/quartznet/compatibility.py def download_checkpoint ( name : NemoCheckpoint , checkpoint_folder : str = None ) -> Path : \"\"\"Download quartznet checkpoint by identifier. Args: name: Model identifier. Check checkpoint_archives.keys() checkpoint_folder: Folder where the checkpoint will be saved to. Returns: Path to the saved checkpoint file. \"\"\" if checkpoint_folder is None : checkpoint_folder = get_default_cache_folder () url = name . value filename = url . split ( \"/\" )[ - 1 ] checkpoint_path = Path ( checkpoint_folder ) / filename if not checkpoint_path . exists (): wget . download ( url , out = str ( checkpoint_path )) return checkpoint_path load_quartznet_weights ( encoder , decoder , weights_path ) Load Quartznet model weights from data present inside .nemo file Parameters: Name Type Description Default encoder Module Encoder module to load the weights into required decoder Module Decoder module to load the weights into required weights_path str Path to the pytorch weights checkpoint required Source code in thunder/quartznet/compatibility.py def load_quartznet_weights ( encoder : nn . Module , decoder : nn . Module , weights_path : str ): \"\"\"Load Quartznet model weights from data present inside .nemo file Args: encoder: Encoder module to load the weights into decoder: Decoder module to load the weights into weights_path : Path to the pytorch weights checkpoint \"\"\" weights = torch . load ( weights_path ) def fix_encoder_name ( x : str ) -> str : x = x . replace ( \"encoder.\" , \"\" ) . replace ( \".res.0\" , \".res\" ) # Add another abstraction layer if it's not a masked conv # This is caused by the new Masked wrapper if \".conv\" not in x : parts = x . split ( \".\" ) x = \".\" . join ( parts [: 3 ] + [ \"layer\" , \"0\" ] + parts [ 3 :]) return x # We remove the 'encoder.' and 'decoder.' prefix from the weights to enable # compatibility to load with plain nn.Modules created by reading the config encoder_weights = { fix_encoder_name ( k ): v for k , v in weights . items () if \"encoder\" in k } encoder . load_state_dict ( encoder_weights , strict = True ) decoder_weights = { k . replace ( \"decoder.decoder_layers.0.\" , \"\" ): v for k , v in weights . items () if \"decoder\" in k } decoder . load_state_dict ( decoder_weights , strict = True ) read_params_from_config ( config_path ) Read the important parameters from the config stored inside the .nemo checkpoint. Parameters: Name Type Description Default config_path str Path to the .yaml file, usually called model_config.yaml required Returns: Type Description Tuple[Dict, List[str], Dict] A tuple containing, in this order, the encoder hyperparameters, the vocabulary, and the preprocessing hyperparameters Source code in thunder/quartznet/compatibility.py def read_params_from_config ( config_path : str ) -> Tuple [ Dict , List [ str ], Dict ]: \"\"\"Read the important parameters from the config stored inside the .nemo checkpoint. Args: config_path : Path to the .yaml file, usually called model_config.yaml Returns: A tuple containing, in this order, the encoder hyperparameters, the vocabulary, and the preprocessing hyperparameters \"\"\" conf = OmegaConf . load ( config_path ) encoder_params = conf [ \"encoder\" ][ \"params\" ] quartznet_conf = OmegaConf . to_container ( encoder_params [ \"jasper\" ]) body_config = quartznet_conf [ 1 : - 2 ] filters = [ cfg [ \"filters\" ] for cfg in body_config ] kernel_sizes = [ cfg [ \"kernel\" ][ 0 ] for cfg in body_config ] encoder_cfg = { \"filters\" : filters , \"kernel_sizes\" : kernel_sizes , } preprocess = conf [ \"preprocessor\" ][ \"params\" ] preprocess_cfg = { \"sample_rate\" : preprocess [ \"sample_rate\" ], \"n_window_size\" : int ( preprocess [ \"window_size\" ] * preprocess [ \"sample_rate\" ]), \"n_window_stride\" : int ( preprocess [ \"window_stride\" ] * preprocess [ \"sample_rate\" ]), \"n_fft\" : preprocess [ \"n_fft\" ], \"nfilt\" : preprocess [ \"features\" ], \"dither\" : preprocess [ \"dither\" ], } labels = ( conf [ \"labels\" ] if \"labels\" in conf else conf [ \"decoder\" ][ \"params\" ][ \"vocabulary\" ] ) return ( encoder_cfg , OmegaConf . to_container ( labels ), preprocess_cfg , )","title":"Compatibility"},{"location":"api/Quartznet/compatibility/#thunder.quartznet.compatibility.NemoCheckpoint","text":"Trained model weight checkpoints. Used by download_checkpoint and QuartznetModule.load_from_nemo . Note Possible values are QuartzNet15x5Base_En , QuartzNet15x5Base_Zh , QuartzNet5x5LS_En , QuartzNet15x5NR_En , stt_ca_quartznet15x5 , stt_it_quartznet15x5 , stt_fr_quartznet15x5 , stt_es_quartznet15x5 , stt_de_quartznet15x5 , stt_pl_quartznet15x5 , stt_ru_quartznet15x5 , stt_en_quartznet15x5 , stt_zh_quartznet15x5","title":"NemoCheckpoint"},{"location":"api/Quartznet/compatibility/#thunder.quartznet.compatibility.download_checkpoint","text":"Download quartznet checkpoint by identifier. Parameters: Name Type Description Default name NemoCheckpoint Model identifier. Check checkpoint_archives.keys() required checkpoint_folder str Folder where the checkpoint will be saved to. None Returns: Type Description Path Path to the saved checkpoint file. Source code in thunder/quartznet/compatibility.py def download_checkpoint ( name : NemoCheckpoint , checkpoint_folder : str = None ) -> Path : \"\"\"Download quartznet checkpoint by identifier. Args: name: Model identifier. Check checkpoint_archives.keys() checkpoint_folder: Folder where the checkpoint will be saved to. Returns: Path to the saved checkpoint file. \"\"\" if checkpoint_folder is None : checkpoint_folder = get_default_cache_folder () url = name . value filename = url . split ( \"/\" )[ - 1 ] checkpoint_path = Path ( checkpoint_folder ) / filename if not checkpoint_path . exists (): wget . download ( url , out = str ( checkpoint_path )) return checkpoint_path","title":"download_checkpoint()"},{"location":"api/Quartznet/compatibility/#thunder.quartznet.compatibility.load_quartznet_weights","text":"Load Quartznet model weights from data present inside .nemo file Parameters: Name Type Description Default encoder Module Encoder module to load the weights into required decoder Module Decoder module to load the weights into required weights_path str Path to the pytorch weights checkpoint required Source code in thunder/quartznet/compatibility.py def load_quartznet_weights ( encoder : nn . Module , decoder : nn . Module , weights_path : str ): \"\"\"Load Quartznet model weights from data present inside .nemo file Args: encoder: Encoder module to load the weights into decoder: Decoder module to load the weights into weights_path : Path to the pytorch weights checkpoint \"\"\" weights = torch . load ( weights_path ) def fix_encoder_name ( x : str ) -> str : x = x . replace ( \"encoder.\" , \"\" ) . replace ( \".res.0\" , \".res\" ) # Add another abstraction layer if it's not a masked conv # This is caused by the new Masked wrapper if \".conv\" not in x : parts = x . split ( \".\" ) x = \".\" . join ( parts [: 3 ] + [ \"layer\" , \"0\" ] + parts [ 3 :]) return x # We remove the 'encoder.' and 'decoder.' prefix from the weights to enable # compatibility to load with plain nn.Modules created by reading the config encoder_weights = { fix_encoder_name ( k ): v for k , v in weights . items () if \"encoder\" in k } encoder . load_state_dict ( encoder_weights , strict = True ) decoder_weights = { k . replace ( \"decoder.decoder_layers.0.\" , \"\" ): v for k , v in weights . items () if \"decoder\" in k } decoder . load_state_dict ( decoder_weights , strict = True )","title":"load_quartznet_weights()"},{"location":"api/Quartznet/compatibility/#thunder.quartznet.compatibility.read_params_from_config","text":"Read the important parameters from the config stored inside the .nemo checkpoint. Parameters: Name Type Description Default config_path str Path to the .yaml file, usually called model_config.yaml required Returns: Type Description Tuple[Dict, List[str], Dict] A tuple containing, in this order, the encoder hyperparameters, the vocabulary, and the preprocessing hyperparameters Source code in thunder/quartznet/compatibility.py def read_params_from_config ( config_path : str ) -> Tuple [ Dict , List [ str ], Dict ]: \"\"\"Read the important parameters from the config stored inside the .nemo checkpoint. Args: config_path : Path to the .yaml file, usually called model_config.yaml Returns: A tuple containing, in this order, the encoder hyperparameters, the vocabulary, and the preprocessing hyperparameters \"\"\" conf = OmegaConf . load ( config_path ) encoder_params = conf [ \"encoder\" ][ \"params\" ] quartznet_conf = OmegaConf . to_container ( encoder_params [ \"jasper\" ]) body_config = quartznet_conf [ 1 : - 2 ] filters = [ cfg [ \"filters\" ] for cfg in body_config ] kernel_sizes = [ cfg [ \"kernel\" ][ 0 ] for cfg in body_config ] encoder_cfg = { \"filters\" : filters , \"kernel_sizes\" : kernel_sizes , } preprocess = conf [ \"preprocessor\" ][ \"params\" ] preprocess_cfg = { \"sample_rate\" : preprocess [ \"sample_rate\" ], \"n_window_size\" : int ( preprocess [ \"window_size\" ] * preprocess [ \"sample_rate\" ]), \"n_window_stride\" : int ( preprocess [ \"window_stride\" ] * preprocess [ \"sample_rate\" ]), \"n_fft\" : preprocess [ \"n_fft\" ], \"nfilt\" : preprocess [ \"features\" ], \"dither\" : preprocess [ \"dither\" ], } labels = ( conf [ \"labels\" ] if \"labels\" in conf else conf [ \"decoder\" ][ \"params\" ][ \"vocabulary\" ] ) return ( encoder_cfg , OmegaConf . to_container ( labels ), preprocess_cfg , )","title":"read_params_from_config()"},{"location":"api/Quartznet/module/","text":"QuartznetModule __init__ ( self , initial_vocab_tokens , sample_rate = 16000 , n_window_size = 320 , n_window_stride = 160 , n_fft = 512 , preemph = 0.97 , nfilt = 64 , dither = 1e-05 , filters = [ 256 , 256 , 512 , 512 , 512 ], kernel_sizes = [ 33 , 39 , 51 , 63 , 75 ], repeat_blocks = 1 , learning_rate = 0.0003 , nemo_compat_vocab = False ) special Module containing both the basic quartznet model and helper functionality, such as feature creation and text processing. Parameters: Name Type Description Default initial_vocab_tokens List[str] List of tokens to be used in the vocab, special tokens should not be included here. Check docs required sample_rate int Check FilterbankFeatures 16000 n_window_size int Check FilterbankFeatures 320 n_window_stride int Check FilterbankFeatures 160 n_fft int Check FilterbankFeatures 512 preemph float Check FilterbankFeatures 0.97 nfilt int Check FilterbankFeatures 64 dither float Check FilterbankFeatures 1e-05 filters List[int] Check Quartznet_encoder [256, 256, 512, 512, 512] kernel_sizes List[int] Check Quartznet_encoder [33, 39, 51, 63, 75] repeat_blocks int Check Quartznet_encoder 1 learning_rate float Learning rate used by the optimizer 0.0003 nemo_compat_vocab bool Controls if the used vocabulary will be compatible with the original nemo implementation. False Source code in thunder/quartznet/module.py def __init__ ( self , initial_vocab_tokens : List [ str ], sample_rate : int = 16000 , n_window_size : int = 320 , n_window_stride : int = 160 , n_fft : int = 512 , preemph : float = 0.97 , nfilt : int = 64 , dither : float = 1e-5 , filters : List [ int ] = [ 256 , 256 , 512 , 512 , 512 ], kernel_sizes : List [ int ] = [ 33 , 39 , 51 , 63 , 75 ], repeat_blocks : int = 1 , learning_rate : float = 3e-4 , nemo_compat_vocab : bool = False , ): \"\"\"Module containing both the basic quartznet model and helper functionality, such as feature creation and text processing. Args: initial_vocab_tokens : List of tokens to be used in the vocab, special tokens should not be included here. Check [`docs`](https://scart97.github.io/thunder-speech/quick%20reference%20guide/#how-to-get-the-initial_vocab_tokens-from-my-dataset) sample_rate : Check [`FilterbankFeatures`][thunder.quartznet.transform.FilterbankFeatures] n_window_size : Check [`FilterbankFeatures`][thunder.quartznet.transform.FilterbankFeatures] n_window_stride : Check [`FilterbankFeatures`][thunder.quartznet.transform.FilterbankFeatures] n_fft : Check [`FilterbankFeatures`][thunder.quartznet.transform.FilterbankFeatures] preemph : Check [`FilterbankFeatures`][thunder.quartznet.transform.FilterbankFeatures] nfilt : Check [`FilterbankFeatures`][thunder.quartznet.transform.FilterbankFeatures] dither : Check [`FilterbankFeatures`][thunder.quartznet.transform.FilterbankFeatures] filters : Check [`Quartznet_encoder`][thunder.quartznet.blocks.Quartznet_encoder] kernel_sizes : Check [`Quartznet_encoder`][thunder.quartznet.blocks.Quartznet_encoder] repeat_blocks : Check [`Quartznet_encoder`][thunder.quartznet.blocks.Quartznet_encoder] learning_rate : Learning rate used by the optimizer nemo_compat_vocab : Controls if the used vocabulary will be compatible with the original nemo implementation. \"\"\" super () . __init__ () self . save_hyperparameters () self . audio_transform = FilterbankFeatures ( sample_rate = sample_rate , n_window_size = n_window_size , n_window_stride = n_window_stride , n_fft = n_fft , preemph = preemph , nfilt = nfilt , dither = dither , ) self . encoder = Quartznet_encoder ( nfilt , filters , kernel_sizes , repeat_blocks ) self . text_transform = self . build_text_transform ( initial_vocab_tokens , nemo_compat_vocab ) self . decoder = self . build_decoder ( 1024 , len ( self . text_transform . vocab )) # Metrics self . val_cer = CER () self . val_wer = WER () # Example input is one second of fake audio self . example_input_array = ( torch . randn (( 10 , sample_rate )), torch . randint ( 10 , sample_rate , ( 10 ,)), ) build_decoder ( self , decoder_input_channels , vocab_size ) Overwrite this function if you want to change the model decoder. Parameters: Name Type Description Default decoder_input_channels int Number of input channels of the decoder. That is the number of channels of the features created by the encoder. required vocab_size int Number of output classes required Returns: Type Description Module Module that represents the decoder. Source code in thunder/quartznet/module.py def build_decoder ( self , decoder_input_channels : int , vocab_size : int ) -> nn . Module : \"\"\"Overwrite this function if you want to change the model decoder. Args: decoder_input_channels : Number of input channels of the decoder. That is the number of channels of the features created by the encoder. vocab_size : Number of output classes Returns: Module that represents the decoder. \"\"\" return Quartznet_decoder ( num_classes = vocab_size , input_channels = decoder_input_channels ) build_text_transform ( self , initial_vocab_tokens , nemo_compat_vocab ) Overwrite this function if you want to change how the text processing happens inside the model. Parameters: Name Type Description Default initial_vocab_tokens List[str] List of tokens to create the vocabulary, special tokens should not be included here. required nemo_compat_vocab bool Controls if the used vocabulary will be compatible with the original nemo implementation. required Returns: Type Description BatchTextTransformer The transform that will both encode the text and decode_prediction . Source code in thunder/quartznet/module.py def build_text_transform ( self , initial_vocab_tokens : List [ str ], nemo_compat_vocab : bool ) -> BatchTextTransformer : \"\"\"Overwrite this function if you want to change how the text processing happens inside the model. Args: initial_vocab_tokens : List of tokens to create the vocabulary, special tokens should not be included here. nemo_compat_vocab : Controls if the used vocabulary will be compatible with the original nemo implementation. Returns: The transform that will both `encode` the text and `decode_prediction`. \"\"\" vocab = Vocab ( initial_vocab_tokens , nemo_compat = nemo_compat_vocab ) return BatchTextTransformer ( vocab = vocab ) change_vocab ( self , new_vocab_tokens , nemo_compat = False ) Changes the vocabulary of the model. useful when finetuning to another language. Parameters: Name Type Description Default new_vocab_tokens List[str] List of tokens to be used in the vocabulary, special tokens should not be included here. required nemo_compat Controls if the used vocabulary will be compatible with the original nemo implementation. False Source code in thunder/quartznet/module.py def change_vocab ( self , new_vocab_tokens : List [ str ], nemo_compat = False ): \"\"\"Changes the vocabulary of the model. useful when finetuning to another language. Args: new_vocab_tokens : List of tokens to be used in the vocabulary, special tokens should not be included here. nemo_compat : Controls if the used vocabulary will be compatible with the original nemo implementation. \"\"\" # Updating hparams so that the saved model can be correctly loaded self . hparams . initial_vocab_tokens = new_vocab_tokens self . hparams . nemo_compat_vocab = nemo_compat self . text_transform = self . build_text_transform ( new_vocab_tokens , nemo_compat ) self . decoder = self . build_decoder ( 1024 , len ( self . text_transform . vocab )) configure_optimizers ( self ) Configuring optimizers. Check the original lightning docs for more info. Returns: Type Description Optimizer Optimizer, and optionally the learning rate scheduler. Source code in thunder/quartznet/module.py def configure_optimizers ( self ) -> torch . optim . Optimizer : \"\"\"Configuring optimizers. Check the original lightning docs for more info. Returns: Optimizer, and optionally the learning rate scheduler. \"\"\" return torch . optim . Adam ( filter ( lambda p : p . requires_grad , self . parameters ()), lr = self . hparams . learning_rate , betas = [ 0.8 , 0.5 ], ) forward ( self , x , lens ) Process the audio tensor to create the predictions. Parameters: Name Type Description Default x Tensor Audio tensor of shape [batch_size, time] required lens Tensor Normalized original lengths of each element in the audio tensor. It should be a float tensor where the biggest element has length 1.0, and all the others are relative to it. required Returns: Type Description Tuple[torch.Tensor, torch.Tensor] Tuple with the predictions and output lengths. Notice that the ouput lengths are not normalized, they are a long tensor. Source code in thunder/quartznet/module.py def forward ( self , x : Tensor , lens : Tensor ) -> Tuple [ Tensor , Tensor ]: \"\"\"Process the audio tensor to create the predictions. Args: x : Audio tensor of shape [batch_size, time] lens : Normalized original lengths of each element in the audio tensor. It should be a float tensor where the biggest element has length 1.0, and all the others are relative to it. Returns: Tuple with the predictions and output lengths. Notice that the ouput lengths are not normalized, they are a long tensor. \"\"\" features = self . audio_transform ( x ) # TODO: maybe mask features feature_lens = ( features . shape [ - 1 ] * lens ) . long () encoded , output_lens = self . encoder ( features , feature_lens ) return self . decoder ( encoded ), output_lens . long () load_from_nemo ( * , nemo_filepath = None , checkpoint_name = None ) classmethod Load from the original nemo checkpoint. Parameters: Name Type Description Default nemo_filepath str Path to local .nemo file. None checkpoint_name NemoCheckpoint Name of checkpoint to be downloaded locally and lodaded. None Exceptions: Type Description ValueError You need to pass only one of the two parameters. Returns: Type Description QuartznetModule The model loaded from the checkpoint Source code in thunder/quartznet/module.py @classmethod def load_from_nemo ( cls , * , nemo_filepath : str = None , checkpoint_name : NemoCheckpoint = None ) -> \"QuartznetModule\" : \"\"\"Load from the original nemo checkpoint. Args: nemo_filepath : Path to local .nemo file. checkpoint_name : Name of checkpoint to be downloaded locally and lodaded. Raises: ValueError: You need to pass only one of the two parameters. Returns: The model loaded from the checkpoint \"\"\" if checkpoint_name is not None : nemo_filepath = download_checkpoint ( checkpoint_name ) if nemo_filepath is None and checkpoint_name is None : raise ValueError ( \"Either nemo_filepath or checkpoint_name must be passed\" ) with TemporaryDirectory () as extract_path : extract_path = Path ( extract_path ) extract_archive ( str ( nemo_filepath ), extract_path ) config_path = extract_path / \"model_config.yaml\" encoder_params , initial_vocab , preprocess_params = read_params_from_config ( config_path ) module = cls ( initial_vocab_tokens = initial_vocab , ** encoder_params , ** preprocess_params , nemo_compat_vocab = True , ) weights_path = extract_path / \"model_weights.ckpt\" load_quartznet_weights ( module . encoder , module . decoder , weights_path ) # Here we set it in eval mode, so it correctly works during inference # Supposing that the majority of applications will be either (1) load a checkpoint # and directly run inference, or (2) fine-tuning. Either way this will prevent a silent # bug (case 1) or will be ignored (case 2). module . eval () return module predict ( self , x ) Use this function during inference to predict. Parameters: Name Type Description Default x Tensor Audio tensor of shape [batch_size, time] required Returns: Type Description List[str] A list of strings, each one contains the corresponding transcription to the original batch element. Source code in thunder/quartznet/module.py @torch . jit . export def predict ( self , x : Tensor ) -> List [ str ]: \"\"\"Use this function during inference to predict. Args: x : Audio tensor of shape [batch_size, time] Returns: A list of strings, each one contains the corresponding transcription to the original batch element. \"\"\" lens = torch . tensor ([ x . shape [ - 1 ]], device = x . device ) pred , _ = self ( x , lens ) return self . text_transform . decode_prediction ( pred . argmax ( 1 )) training_step ( self , batch , batch_idx ) Training step. Check the original lightning docs for more information. Parameters: Name Type Description Default batch Tuple[torch.Tensor, torch.Tensor, List[str]] Tuple containing the batched audios, normalized lengths and the corresponding text labels. required batch_idx int Batch index required Returns: Type Description Tensor Training loss for that batch Source code in thunder/quartznet/module.py def training_step ( self , batch : Tuple [ torch . Tensor , torch . Tensor , List [ str ]], batch_idx : int ) -> torch . Tensor : \"\"\"Training step. Check the original lightning docs for more information. Args: batch : Tuple containing the batched audios, normalized lengths and the corresponding text labels. batch_idx : Batch index Returns: Training loss for that batch \"\"\" audio , audio_lens , texts = batch y , y_lens = self . text_transform . encode ( texts , device = self . device ) probabilities , out_lens = self ( audio , audio_lens ) loss = self . calculate_loss ( probabilities , y , out_lens , y_lens ) self . log ( \"loss/train_loss\" , loss ) return loss validation_step ( self , batch , batch_idx ) Validation step. Check the original lightning docs for more information. Parameters: Name Type Description Default batch Tuple[torch.Tensor, torch.Tensor, List[str]] Tuple containing the batched audios, normalized lengths and the corresponding text labels. required batch_idx int Batch index required Returns: Type Description Tensor Validation loss for that batch Source code in thunder/quartznet/module.py def validation_step ( self , batch : Tuple [ torch . Tensor , torch . Tensor , List [ str ]], batch_idx : int ) -> torch . Tensor : \"\"\"Validation step. Check the original lightning docs for more information. Args: batch : Tuple containing the batched audios, normalized lengths and the corresponding text labels. batch_idx : Batch index Returns: Validation loss for that batch \"\"\" audio , audio_lens , texts = batch y , y_lens = self . text_transform . encode ( texts , device = self . device ) probabilities , out_lens = self ( audio , audio_lens ) loss = self . calculate_loss ( probabilities , y , out_lens , y_lens ) decoded_preds = self . text_transform . decode_prediction ( probabilities . argmax ( 1 )) decoded_targets = self . text_transform . decode_prediction ( y ) self . val_cer ( decoded_preds , decoded_targets ) self . val_wer ( decoded_preds , decoded_targets ) self . log ( \"loss/val_loss\" , loss ) self . log ( \"metrics/cer\" , self . val_cer , on_epoch = True ) self . log ( \"metrics/wer\" , self . val_wer , on_epoch = True ) return loss","title":"Module"},{"location":"api/Quartznet/module/#thunder.quartznet.module.QuartznetModule","text":"","title":"QuartznetModule"},{"location":"api/Quartznet/module/#thunder.quartznet.module.QuartznetModule.__init__","text":"Module containing both the basic quartznet model and helper functionality, such as feature creation and text processing. Parameters: Name Type Description Default initial_vocab_tokens List[str] List of tokens to be used in the vocab, special tokens should not be included here. Check docs required sample_rate int Check FilterbankFeatures 16000 n_window_size int Check FilterbankFeatures 320 n_window_stride int Check FilterbankFeatures 160 n_fft int Check FilterbankFeatures 512 preemph float Check FilterbankFeatures 0.97 nfilt int Check FilterbankFeatures 64 dither float Check FilterbankFeatures 1e-05 filters List[int] Check Quartznet_encoder [256, 256, 512, 512, 512] kernel_sizes List[int] Check Quartznet_encoder [33, 39, 51, 63, 75] repeat_blocks int Check Quartznet_encoder 1 learning_rate float Learning rate used by the optimizer 0.0003 nemo_compat_vocab bool Controls if the used vocabulary will be compatible with the original nemo implementation. False Source code in thunder/quartznet/module.py def __init__ ( self , initial_vocab_tokens : List [ str ], sample_rate : int = 16000 , n_window_size : int = 320 , n_window_stride : int = 160 , n_fft : int = 512 , preemph : float = 0.97 , nfilt : int = 64 , dither : float = 1e-5 , filters : List [ int ] = [ 256 , 256 , 512 , 512 , 512 ], kernel_sizes : List [ int ] = [ 33 , 39 , 51 , 63 , 75 ], repeat_blocks : int = 1 , learning_rate : float = 3e-4 , nemo_compat_vocab : bool = False , ): \"\"\"Module containing both the basic quartznet model and helper functionality, such as feature creation and text processing. Args: initial_vocab_tokens : List of tokens to be used in the vocab, special tokens should not be included here. Check [`docs`](https://scart97.github.io/thunder-speech/quick%20reference%20guide/#how-to-get-the-initial_vocab_tokens-from-my-dataset) sample_rate : Check [`FilterbankFeatures`][thunder.quartznet.transform.FilterbankFeatures] n_window_size : Check [`FilterbankFeatures`][thunder.quartznet.transform.FilterbankFeatures] n_window_stride : Check [`FilterbankFeatures`][thunder.quartznet.transform.FilterbankFeatures] n_fft : Check [`FilterbankFeatures`][thunder.quartznet.transform.FilterbankFeatures] preemph : Check [`FilterbankFeatures`][thunder.quartznet.transform.FilterbankFeatures] nfilt : Check [`FilterbankFeatures`][thunder.quartznet.transform.FilterbankFeatures] dither : Check [`FilterbankFeatures`][thunder.quartznet.transform.FilterbankFeatures] filters : Check [`Quartznet_encoder`][thunder.quartznet.blocks.Quartznet_encoder] kernel_sizes : Check [`Quartznet_encoder`][thunder.quartznet.blocks.Quartznet_encoder] repeat_blocks : Check [`Quartznet_encoder`][thunder.quartznet.blocks.Quartznet_encoder] learning_rate : Learning rate used by the optimizer nemo_compat_vocab : Controls if the used vocabulary will be compatible with the original nemo implementation. \"\"\" super () . __init__ () self . save_hyperparameters () self . audio_transform = FilterbankFeatures ( sample_rate = sample_rate , n_window_size = n_window_size , n_window_stride = n_window_stride , n_fft = n_fft , preemph = preemph , nfilt = nfilt , dither = dither , ) self . encoder = Quartznet_encoder ( nfilt , filters , kernel_sizes , repeat_blocks ) self . text_transform = self . build_text_transform ( initial_vocab_tokens , nemo_compat_vocab ) self . decoder = self . build_decoder ( 1024 , len ( self . text_transform . vocab )) # Metrics self . val_cer = CER () self . val_wer = WER () # Example input is one second of fake audio self . example_input_array = ( torch . randn (( 10 , sample_rate )), torch . randint ( 10 , sample_rate , ( 10 ,)), )","title":"__init__()"},{"location":"api/Quartznet/module/#thunder.quartznet.module.QuartznetModule.build_decoder","text":"Overwrite this function if you want to change the model decoder. Parameters: Name Type Description Default decoder_input_channels int Number of input channels of the decoder. That is the number of channels of the features created by the encoder. required vocab_size int Number of output classes required Returns: Type Description Module Module that represents the decoder. Source code in thunder/quartznet/module.py def build_decoder ( self , decoder_input_channels : int , vocab_size : int ) -> nn . Module : \"\"\"Overwrite this function if you want to change the model decoder. Args: decoder_input_channels : Number of input channels of the decoder. That is the number of channels of the features created by the encoder. vocab_size : Number of output classes Returns: Module that represents the decoder. \"\"\" return Quartznet_decoder ( num_classes = vocab_size , input_channels = decoder_input_channels )","title":"build_decoder()"},{"location":"api/Quartznet/module/#thunder.quartznet.module.QuartznetModule.build_text_transform","text":"Overwrite this function if you want to change how the text processing happens inside the model. Parameters: Name Type Description Default initial_vocab_tokens List[str] List of tokens to create the vocabulary, special tokens should not be included here. required nemo_compat_vocab bool Controls if the used vocabulary will be compatible with the original nemo implementation. required Returns: Type Description BatchTextTransformer The transform that will both encode the text and decode_prediction . Source code in thunder/quartznet/module.py def build_text_transform ( self , initial_vocab_tokens : List [ str ], nemo_compat_vocab : bool ) -> BatchTextTransformer : \"\"\"Overwrite this function if you want to change how the text processing happens inside the model. Args: initial_vocab_tokens : List of tokens to create the vocabulary, special tokens should not be included here. nemo_compat_vocab : Controls if the used vocabulary will be compatible with the original nemo implementation. Returns: The transform that will both `encode` the text and `decode_prediction`. \"\"\" vocab = Vocab ( initial_vocab_tokens , nemo_compat = nemo_compat_vocab ) return BatchTextTransformer ( vocab = vocab )","title":"build_text_transform()"},{"location":"api/Quartznet/module/#thunder.quartznet.module.QuartznetModule.change_vocab","text":"Changes the vocabulary of the model. useful when finetuning to another language. Parameters: Name Type Description Default new_vocab_tokens List[str] List of tokens to be used in the vocabulary, special tokens should not be included here. required nemo_compat Controls if the used vocabulary will be compatible with the original nemo implementation. False Source code in thunder/quartznet/module.py def change_vocab ( self , new_vocab_tokens : List [ str ], nemo_compat = False ): \"\"\"Changes the vocabulary of the model. useful when finetuning to another language. Args: new_vocab_tokens : List of tokens to be used in the vocabulary, special tokens should not be included here. nemo_compat : Controls if the used vocabulary will be compatible with the original nemo implementation. \"\"\" # Updating hparams so that the saved model can be correctly loaded self . hparams . initial_vocab_tokens = new_vocab_tokens self . hparams . nemo_compat_vocab = nemo_compat self . text_transform = self . build_text_transform ( new_vocab_tokens , nemo_compat ) self . decoder = self . build_decoder ( 1024 , len ( self . text_transform . vocab ))","title":"change_vocab()"},{"location":"api/Quartznet/module/#thunder.quartznet.module.QuartznetModule.configure_optimizers","text":"Configuring optimizers. Check the original lightning docs for more info. Returns: Type Description Optimizer Optimizer, and optionally the learning rate scheduler. Source code in thunder/quartznet/module.py def configure_optimizers ( self ) -> torch . optim . Optimizer : \"\"\"Configuring optimizers. Check the original lightning docs for more info. Returns: Optimizer, and optionally the learning rate scheduler. \"\"\" return torch . optim . Adam ( filter ( lambda p : p . requires_grad , self . parameters ()), lr = self . hparams . learning_rate , betas = [ 0.8 , 0.5 ], )","title":"configure_optimizers()"},{"location":"api/Quartznet/module/#thunder.quartznet.module.QuartznetModule.forward","text":"Process the audio tensor to create the predictions. Parameters: Name Type Description Default x Tensor Audio tensor of shape [batch_size, time] required lens Tensor Normalized original lengths of each element in the audio tensor. It should be a float tensor where the biggest element has length 1.0, and all the others are relative to it. required Returns: Type Description Tuple[torch.Tensor, torch.Tensor] Tuple with the predictions and output lengths. Notice that the ouput lengths are not normalized, they are a long tensor. Source code in thunder/quartznet/module.py def forward ( self , x : Tensor , lens : Tensor ) -> Tuple [ Tensor , Tensor ]: \"\"\"Process the audio tensor to create the predictions. Args: x : Audio tensor of shape [batch_size, time] lens : Normalized original lengths of each element in the audio tensor. It should be a float tensor where the biggest element has length 1.0, and all the others are relative to it. Returns: Tuple with the predictions and output lengths. Notice that the ouput lengths are not normalized, they are a long tensor. \"\"\" features = self . audio_transform ( x ) # TODO: maybe mask features feature_lens = ( features . shape [ - 1 ] * lens ) . long () encoded , output_lens = self . encoder ( features , feature_lens ) return self . decoder ( encoded ), output_lens . long ()","title":"forward()"},{"location":"api/Quartznet/module/#thunder.quartznet.module.QuartznetModule.load_from_nemo","text":"Load from the original nemo checkpoint. Parameters: Name Type Description Default nemo_filepath str Path to local .nemo file. None checkpoint_name NemoCheckpoint Name of checkpoint to be downloaded locally and lodaded. None Exceptions: Type Description ValueError You need to pass only one of the two parameters. Returns: Type Description QuartznetModule The model loaded from the checkpoint Source code in thunder/quartznet/module.py @classmethod def load_from_nemo ( cls , * , nemo_filepath : str = None , checkpoint_name : NemoCheckpoint = None ) -> \"QuartznetModule\" : \"\"\"Load from the original nemo checkpoint. Args: nemo_filepath : Path to local .nemo file. checkpoint_name : Name of checkpoint to be downloaded locally and lodaded. Raises: ValueError: You need to pass only one of the two parameters. Returns: The model loaded from the checkpoint \"\"\" if checkpoint_name is not None : nemo_filepath = download_checkpoint ( checkpoint_name ) if nemo_filepath is None and checkpoint_name is None : raise ValueError ( \"Either nemo_filepath or checkpoint_name must be passed\" ) with TemporaryDirectory () as extract_path : extract_path = Path ( extract_path ) extract_archive ( str ( nemo_filepath ), extract_path ) config_path = extract_path / \"model_config.yaml\" encoder_params , initial_vocab , preprocess_params = read_params_from_config ( config_path ) module = cls ( initial_vocab_tokens = initial_vocab , ** encoder_params , ** preprocess_params , nemo_compat_vocab = True , ) weights_path = extract_path / \"model_weights.ckpt\" load_quartznet_weights ( module . encoder , module . decoder , weights_path ) # Here we set it in eval mode, so it correctly works during inference # Supposing that the majority of applications will be either (1) load a checkpoint # and directly run inference, or (2) fine-tuning. Either way this will prevent a silent # bug (case 1) or will be ignored (case 2). module . eval () return module","title":"load_from_nemo()"},{"location":"api/Quartznet/module/#thunder.quartznet.module.QuartznetModule.predict","text":"Use this function during inference to predict. Parameters: Name Type Description Default x Tensor Audio tensor of shape [batch_size, time] required Returns: Type Description List[str] A list of strings, each one contains the corresponding transcription to the original batch element. Source code in thunder/quartznet/module.py @torch . jit . export def predict ( self , x : Tensor ) -> List [ str ]: \"\"\"Use this function during inference to predict. Args: x : Audio tensor of shape [batch_size, time] Returns: A list of strings, each one contains the corresponding transcription to the original batch element. \"\"\" lens = torch . tensor ([ x . shape [ - 1 ]], device = x . device ) pred , _ = self ( x , lens ) return self . text_transform . decode_prediction ( pred . argmax ( 1 ))","title":"predict()"},{"location":"api/Quartznet/module/#thunder.quartznet.module.QuartznetModule.training_step","text":"Training step. Check the original lightning docs for more information. Parameters: Name Type Description Default batch Tuple[torch.Tensor, torch.Tensor, List[str]] Tuple containing the batched audios, normalized lengths and the corresponding text labels. required batch_idx int Batch index required Returns: Type Description Tensor Training loss for that batch Source code in thunder/quartznet/module.py def training_step ( self , batch : Tuple [ torch . Tensor , torch . Tensor , List [ str ]], batch_idx : int ) -> torch . Tensor : \"\"\"Training step. Check the original lightning docs for more information. Args: batch : Tuple containing the batched audios, normalized lengths and the corresponding text labels. batch_idx : Batch index Returns: Training loss for that batch \"\"\" audio , audio_lens , texts = batch y , y_lens = self . text_transform . encode ( texts , device = self . device ) probabilities , out_lens = self ( audio , audio_lens ) loss = self . calculate_loss ( probabilities , y , out_lens , y_lens ) self . log ( \"loss/train_loss\" , loss ) return loss","title":"training_step()"},{"location":"api/Quartznet/module/#thunder.quartznet.module.QuartznetModule.validation_step","text":"Validation step. Check the original lightning docs for more information. Parameters: Name Type Description Default batch Tuple[torch.Tensor, torch.Tensor, List[str]] Tuple containing the batched audios, normalized lengths and the corresponding text labels. required batch_idx int Batch index required Returns: Type Description Tensor Validation loss for that batch Source code in thunder/quartznet/module.py def validation_step ( self , batch : Tuple [ torch . Tensor , torch . Tensor , List [ str ]], batch_idx : int ) -> torch . Tensor : \"\"\"Validation step. Check the original lightning docs for more information. Args: batch : Tuple containing the batched audios, normalized lengths and the corresponding text labels. batch_idx : Batch index Returns: Validation loss for that batch \"\"\" audio , audio_lens , texts = batch y , y_lens = self . text_transform . encode ( texts , device = self . device ) probabilities , out_lens = self ( audio , audio_lens ) loss = self . calculate_loss ( probabilities , y , out_lens , y_lens ) decoded_preds = self . text_transform . decode_prediction ( probabilities . argmax ( 1 )) decoded_targets = self . text_transform . decode_prediction ( y ) self . val_cer ( decoded_preds , decoded_targets ) self . val_wer ( decoded_preds , decoded_targets ) self . log ( \"loss/val_loss\" , loss ) self . log ( \"metrics/cer\" , self . val_cer , on_epoch = True ) self . log ( \"metrics/wer\" , self . val_wer , on_epoch = True ) return loss","title":"validation_step()"},{"location":"api/Quartznet/transform/","text":"Functionality to transform the audio input in the same way that the Quartznet model expects it. DitherAudio __init__ ( self , dither = 1e-05 ) special Add some dithering to the audio tensor. Note From wikipedia: Dither is an intentionally applied form of noise used to randomize quantization error. Parameters: Name Type Description Default dither float Amount of dither to add. 1e-05 Source code in thunder/quartznet/transform.py def __init__ ( self , dither : float = 1e-5 ): \"\"\"Add some dithering to the audio tensor. Note: From wikipedia: Dither is an intentionally applied form of noise used to randomize quantization error. Args: dither : Amount of dither to add. \"\"\" super () . __init__ () self . dither = dither forward ( self , x ) Parameters: Name Type Description Default x Tensor Tensor of shape (batch, time) required Source code in thunder/quartznet/transform.py @torch . no_grad () def forward ( self , x : torch . Tensor ) -> torch . Tensor : \"\"\" Args: x : Tensor of shape (batch, time) \"\"\" if self . training : return x + self . dither * torch . randn_like ( x ) else : return x FeatureBatchNormalizer __init__ ( self ) special Normalize batch at the feature dimension. Source code in thunder/quartznet/transform.py def __init__ ( self ): \"\"\"Normalize batch at the feature dimension.\"\"\" super () . __init__ () self . div_guard = 1e-5 forward ( self , x ) Parameters: Name Type Description Default x Tensor Tensor of shape (batch, features, time) required Source code in thunder/quartznet/transform.py @torch . no_grad () def forward ( self , x : torch . Tensor ) -> torch . Tensor : \"\"\" Args: x : Tensor of shape (batch, features, time) \"\"\" x_mean = x . mean ( dim = 2 , keepdim = True ) . detach () x_std = x . std ( dim = 2 , keepdim = True ) . detach () # make sure x_std is not zero x_std += self . div_guard return ( x - x_mean ) / x_std MelScale __init__ ( self , sample_rate , n_fft , nfilt , log_scale = True ) special Convert a spectrogram to Mel scale, following the default formula of librosa instead of the one used by torchaudio. Also converts to log scale. Parameters: Name Type Description Default sample_rate int Sampling rate of the signal required n_fft int Number of fourier features required nfilt int Number of output mel filters to use required log_scale bool Controls if the output should also be applied a log scale. True Source code in thunder/quartznet/transform.py def __init__ ( self , sample_rate : int , n_fft : int , nfilt : int , log_scale : bool = True ): \"\"\"Convert a spectrogram to Mel scale, following the default formula of librosa instead of the one used by torchaudio. Also converts to log scale. Args: sample_rate : Sampling rate of the signal n_fft : Number of fourier features nfilt : Number of output mel filters to use log_scale : Controls if the output should also be applied a log scale. \"\"\" super () . __init__ () filterbanks = ( create_fb_matrix ( int ( 1 + n_fft // 2 ), n_mels = nfilt , sample_rate = sample_rate , f_min = 0 , f_max = sample_rate / 2 , norm = \"slaney\" , htk = False , ) . transpose ( 0 , 1 ) . unsqueeze ( 0 ) ) self . register_buffer ( \"fb\" , filterbanks ) self . log_scale = log_scale forward ( self , x ) Parameters: Name Type Description Default x Tensor Tensor of shape (batch, features, time) required Source code in thunder/quartznet/transform.py @torch . no_grad () def forward ( self , x : torch . Tensor ) -> torch . Tensor : \"\"\" Args: x : Tensor of shape (batch, features, time) \"\"\" # dot with filterbank energies x = torch . matmul ( self . fb . to ( x . dtype ), x ) # log features # We want to avoid taking the log of zero if self . log_scale : x = torch . log ( x + 2 ** - 24 ) return x PowerSpectrum __init__ ( self , n_window_size = 320 , n_window_stride = 160 , n_fft = None ) special Calculates the power spectrum of the audio signal, following the same method as used in NEMO. Parameters: Name Type Description Default n_window_size int Number of elements in the window size. 320 n_window_stride int Number of elements in the window stride. 160 n_fft Optional[int] Number of fourier features. None Exceptions: Type Description ValueError Raised when incompatible parameters are passed. Source code in thunder/quartznet/transform.py def __init__ ( self , n_window_size : int = 320 , n_window_stride : int = 160 , n_fft : Optional [ int ] = None , ): \"\"\"Calculates the power spectrum of the audio signal, following the same method as used in NEMO. Args: n_window_size : Number of elements in the window size. n_window_stride : Number of elements in the window stride. n_fft : Number of fourier features. Raises: ValueError: Raised when incompatible parameters are passed. \"\"\" super () . __init__ () if n_window_size <= 0 or n_window_stride <= 0 : raise ValueError ( f \" { self } got an invalid value for either n_window_size or \" f \"n_window_stride. Both must be positive ints.\" ) self . win_length = n_window_size self . hop_length = n_window_stride self . n_fft = n_fft or 2 ** math . ceil ( math . log2 ( self . win_length )) window_tensor = torch . hann_window ( self . win_length , periodic = False ) self . register_buffer ( \"window\" , window_tensor ) forward ( self , x ) Parameters: Name Type Description Default x Tensor Tensor of shape (batch, time) required Source code in thunder/quartznet/transform.py @torch . no_grad () def forward ( self , x : torch . Tensor ) -> torch . Tensor : \"\"\" Args: x : Tensor of shape (batch, time) \"\"\" x = torch . stft ( x , n_fft = self . n_fft , hop_length = self . hop_length , win_length = self . win_length , center = True , window = self . window . to ( dtype = torch . float ), return_complex = False , ) # torch returns real, imag; so convert to magnitude x = torch . sqrt ( x . pow ( 2 ) . sum ( - 1 )) # get power spectrum x = x . pow ( 2.0 ) return x PreEmphasisFilter __init__ ( self , preemph = 0.97 ) special Applies preemphasis filtering to the audio signal. This is a classic signal processing function to emphasise the high frequency portion of the content compared to the low frequency. It applies a FIR filter of the form: y[n] = y[n] - preemph * y[n-1] Parameters: Name Type Description Default preemph float Filter control factor. 0.97 Source code in thunder/quartznet/transform.py def __init__ ( self , preemph : float = 0.97 ): \"\"\"Applies preemphasis filtering to the audio signal. This is a classic signal processing function to emphasise the high frequency portion of the content compared to the low frequency. It applies a FIR filter of the form: `y[n] = y[n] - preemph * y[n-1]` Args: preemph : Filter control factor. \"\"\" super () . __init__ () self . preemph = preemph forward ( self , x ) Parameters: Name Type Description Default x Tensor Tensor of shape (batch, time) required Source code in thunder/quartznet/transform.py @torch . no_grad () def forward ( self , x : torch . Tensor ) -> torch . Tensor : \"\"\" Args: x : Tensor of shape (batch, time) \"\"\" return torch . cat ( ( x [:, 0 ] . unsqueeze ( 1 ), x [:, 1 :] - self . preemph * x [:, : - 1 ]), dim = 1 ) FilterbankFeatures ( sample_rate = 16000 , n_window_size = 320 , n_window_stride = 160 , n_fft = 512 , preemph = 0.97 , nfilt = 64 , dither = 1e-05 , ** kwargs ) Creates the Filterbank features used in the Quartznet model. Parameters: Name Type Description Default sample_rate int Sampling rate of the signal 16000 n_window_size int Number of elements in the window size. 320 n_window_stride int Number of elements in the window stride. 160 n_fft int Number of fourier features. 512 preemph float Preemphasis filtering control factor. 0.97 nfilt int Number of output mel filters to use 64 dither float Amount of dither to add. 1e-05 Returns: Type Description Module Module that computes the features based on raw audio tensor. Source code in thunder/quartznet/transform.py def FilterbankFeatures ( sample_rate : int = 16000 , n_window_size : int = 320 , n_window_stride : int = 160 , n_fft : int = 512 , preemph : float = 0.97 , nfilt : int = 64 , dither : float = 1e-5 , ** kwargs , ) -> nn . Module : \"\"\"Creates the Filterbank features used in the Quartznet model. Args: sample_rate : Sampling rate of the signal n_window_size : Number of elements in the window size. n_window_stride : Number of elements in the window stride. n_fft : Number of fourier features. preemph : Preemphasis filtering control factor. nfilt : Number of output mel filters to use dither : Amount of dither to add. Returns: Module that computes the features based on raw audio tensor. \"\"\" return nn . Sequential ( DitherAudio ( dither = dither ), PreEmphasisFilter ( preemph = preemph ), PowerSpectrum ( n_window_size = n_window_size , n_window_stride = n_window_stride , n_fft = n_fft , ), MelScale ( sample_rate = sample_rate , n_fft = n_fft , nfilt = nfilt ), FeatureBatchNormalizer (), )","title":"Transform"},{"location":"api/Quartznet/transform/#thunder.quartznet.transform.DitherAudio","text":"","title":"DitherAudio"},{"location":"api/Quartznet/transform/#thunder.quartznet.transform.DitherAudio.__init__","text":"Add some dithering to the audio tensor. Note From wikipedia: Dither is an intentionally applied form of noise used to randomize quantization error. Parameters: Name Type Description Default dither float Amount of dither to add. 1e-05 Source code in thunder/quartznet/transform.py def __init__ ( self , dither : float = 1e-5 ): \"\"\"Add some dithering to the audio tensor. Note: From wikipedia: Dither is an intentionally applied form of noise used to randomize quantization error. Args: dither : Amount of dither to add. \"\"\" super () . __init__ () self . dither = dither","title":"__init__()"},{"location":"api/Quartznet/transform/#thunder.quartznet.transform.DitherAudio.forward","text":"Parameters: Name Type Description Default x Tensor Tensor of shape (batch, time) required Source code in thunder/quartznet/transform.py @torch . no_grad () def forward ( self , x : torch . Tensor ) -> torch . Tensor : \"\"\" Args: x : Tensor of shape (batch, time) \"\"\" if self . training : return x + self . dither * torch . randn_like ( x ) else : return x","title":"forward()"},{"location":"api/Quartznet/transform/#thunder.quartznet.transform.FeatureBatchNormalizer","text":"","title":"FeatureBatchNormalizer"},{"location":"api/Quartznet/transform/#thunder.quartznet.transform.FeatureBatchNormalizer.__init__","text":"Normalize batch at the feature dimension. Source code in thunder/quartznet/transform.py def __init__ ( self ): \"\"\"Normalize batch at the feature dimension.\"\"\" super () . __init__ () self . div_guard = 1e-5","title":"__init__()"},{"location":"api/Quartznet/transform/#thunder.quartznet.transform.FeatureBatchNormalizer.forward","text":"Parameters: Name Type Description Default x Tensor Tensor of shape (batch, features, time) required Source code in thunder/quartznet/transform.py @torch . no_grad () def forward ( self , x : torch . Tensor ) -> torch . Tensor : \"\"\" Args: x : Tensor of shape (batch, features, time) \"\"\" x_mean = x . mean ( dim = 2 , keepdim = True ) . detach () x_std = x . std ( dim = 2 , keepdim = True ) . detach () # make sure x_std is not zero x_std += self . div_guard return ( x - x_mean ) / x_std","title":"forward()"},{"location":"api/Quartznet/transform/#thunder.quartznet.transform.MelScale","text":"","title":"MelScale"},{"location":"api/Quartznet/transform/#thunder.quartznet.transform.MelScale.__init__","text":"Convert a spectrogram to Mel scale, following the default formula of librosa instead of the one used by torchaudio. Also converts to log scale. Parameters: Name Type Description Default sample_rate int Sampling rate of the signal required n_fft int Number of fourier features required nfilt int Number of output mel filters to use required log_scale bool Controls if the output should also be applied a log scale. True Source code in thunder/quartznet/transform.py def __init__ ( self , sample_rate : int , n_fft : int , nfilt : int , log_scale : bool = True ): \"\"\"Convert a spectrogram to Mel scale, following the default formula of librosa instead of the one used by torchaudio. Also converts to log scale. Args: sample_rate : Sampling rate of the signal n_fft : Number of fourier features nfilt : Number of output mel filters to use log_scale : Controls if the output should also be applied a log scale. \"\"\" super () . __init__ () filterbanks = ( create_fb_matrix ( int ( 1 + n_fft // 2 ), n_mels = nfilt , sample_rate = sample_rate , f_min = 0 , f_max = sample_rate / 2 , norm = \"slaney\" , htk = False , ) . transpose ( 0 , 1 ) . unsqueeze ( 0 ) ) self . register_buffer ( \"fb\" , filterbanks ) self . log_scale = log_scale","title":"__init__()"},{"location":"api/Quartznet/transform/#thunder.quartznet.transform.MelScale.forward","text":"Parameters: Name Type Description Default x Tensor Tensor of shape (batch, features, time) required Source code in thunder/quartznet/transform.py @torch . no_grad () def forward ( self , x : torch . Tensor ) -> torch . Tensor : \"\"\" Args: x : Tensor of shape (batch, features, time) \"\"\" # dot with filterbank energies x = torch . matmul ( self . fb . to ( x . dtype ), x ) # log features # We want to avoid taking the log of zero if self . log_scale : x = torch . log ( x + 2 ** - 24 ) return x","title":"forward()"},{"location":"api/Quartznet/transform/#thunder.quartznet.transform.PowerSpectrum","text":"","title":"PowerSpectrum"},{"location":"api/Quartznet/transform/#thunder.quartznet.transform.PowerSpectrum.__init__","text":"Calculates the power spectrum of the audio signal, following the same method as used in NEMO. Parameters: Name Type Description Default n_window_size int Number of elements in the window size. 320 n_window_stride int Number of elements in the window stride. 160 n_fft Optional[int] Number of fourier features. None Exceptions: Type Description ValueError Raised when incompatible parameters are passed. Source code in thunder/quartznet/transform.py def __init__ ( self , n_window_size : int = 320 , n_window_stride : int = 160 , n_fft : Optional [ int ] = None , ): \"\"\"Calculates the power spectrum of the audio signal, following the same method as used in NEMO. Args: n_window_size : Number of elements in the window size. n_window_stride : Number of elements in the window stride. n_fft : Number of fourier features. Raises: ValueError: Raised when incompatible parameters are passed. \"\"\" super () . __init__ () if n_window_size <= 0 or n_window_stride <= 0 : raise ValueError ( f \" { self } got an invalid value for either n_window_size or \" f \"n_window_stride. Both must be positive ints.\" ) self . win_length = n_window_size self . hop_length = n_window_stride self . n_fft = n_fft or 2 ** math . ceil ( math . log2 ( self . win_length )) window_tensor = torch . hann_window ( self . win_length , periodic = False ) self . register_buffer ( \"window\" , window_tensor )","title":"__init__()"},{"location":"api/Quartznet/transform/#thunder.quartznet.transform.PowerSpectrum.forward","text":"Parameters: Name Type Description Default x Tensor Tensor of shape (batch, time) required Source code in thunder/quartznet/transform.py @torch . no_grad () def forward ( self , x : torch . Tensor ) -> torch . Tensor : \"\"\" Args: x : Tensor of shape (batch, time) \"\"\" x = torch . stft ( x , n_fft = self . n_fft , hop_length = self . hop_length , win_length = self . win_length , center = True , window = self . window . to ( dtype = torch . float ), return_complex = False , ) # torch returns real, imag; so convert to magnitude x = torch . sqrt ( x . pow ( 2 ) . sum ( - 1 )) # get power spectrum x = x . pow ( 2.0 ) return x","title":"forward()"},{"location":"api/Quartznet/transform/#thunder.quartznet.transform.PreEmphasisFilter","text":"","title":"PreEmphasisFilter"},{"location":"api/Quartznet/transform/#thunder.quartznet.transform.PreEmphasisFilter.__init__","text":"Applies preemphasis filtering to the audio signal. This is a classic signal processing function to emphasise the high frequency portion of the content compared to the low frequency. It applies a FIR filter of the form: y[n] = y[n] - preemph * y[n-1] Parameters: Name Type Description Default preemph float Filter control factor. 0.97 Source code in thunder/quartznet/transform.py def __init__ ( self , preemph : float = 0.97 ): \"\"\"Applies preemphasis filtering to the audio signal. This is a classic signal processing function to emphasise the high frequency portion of the content compared to the low frequency. It applies a FIR filter of the form: `y[n] = y[n] - preemph * y[n-1]` Args: preemph : Filter control factor. \"\"\" super () . __init__ () self . preemph = preemph","title":"__init__()"},{"location":"api/Quartznet/transform/#thunder.quartznet.transform.PreEmphasisFilter.forward","text":"Parameters: Name Type Description Default x Tensor Tensor of shape (batch, time) required Source code in thunder/quartznet/transform.py @torch . no_grad () def forward ( self , x : torch . Tensor ) -> torch . Tensor : \"\"\" Args: x : Tensor of shape (batch, time) \"\"\" return torch . cat ( ( x [:, 0 ] . unsqueeze ( 1 ), x [:, 1 :] - self . preemph * x [:, : - 1 ]), dim = 1 )","title":"forward()"},{"location":"api/Quartznet/transform/#thunder.quartznet.transform.FilterbankFeatures","text":"Creates the Filterbank features used in the Quartznet model. Parameters: Name Type Description Default sample_rate int Sampling rate of the signal 16000 n_window_size int Number of elements in the window size. 320 n_window_stride int Number of elements in the window stride. 160 n_fft int Number of fourier features. 512 preemph float Preemphasis filtering control factor. 0.97 nfilt int Number of output mel filters to use 64 dither float Amount of dither to add. 1e-05 Returns: Type Description Module Module that computes the features based on raw audio tensor. Source code in thunder/quartznet/transform.py def FilterbankFeatures ( sample_rate : int = 16000 , n_window_size : int = 320 , n_window_stride : int = 160 , n_fft : int = 512 , preemph : float = 0.97 , nfilt : int = 64 , dither : float = 1e-5 , ** kwargs , ) -> nn . Module : \"\"\"Creates the Filterbank features used in the Quartznet model. Args: sample_rate : Sampling rate of the signal n_window_size : Number of elements in the window size. n_window_stride : Number of elements in the window stride. n_fft : Number of fourier features. preemph : Preemphasis filtering control factor. nfilt : Number of output mel filters to use dither : Amount of dither to add. Returns: Module that computes the features based on raw audio tensor. \"\"\" return nn . Sequential ( DitherAudio ( dither = dither ), PreEmphasisFilter ( preemph = preemph ), PowerSpectrum ( n_window_size = n_window_size , n_window_stride = n_window_stride , n_fft = n_fft , ), MelScale ( sample_rate = sample_rate , n_fft = n_fft , nfilt = nfilt ), FeatureBatchNormalizer (), )","title":"FilterbankFeatures()"},{"location":"api/Text%20Processing/preprocess/","text":"expand_numbers ( text , language = 'en' ) Expand the numbers present inside the text. That means converting \"42\" into \"forty two\". It also detects if the number is ordinal automatically. Parameters: Name Type Description Default text str Input text required language str Language used to expand the numbers. Defaults to \"en\". 'en' Returns: Type Description str Output text Source code in thunder/text_processing/preprocess.py def expand_numbers ( text : str , language : str = \"en\" ) -> str : \"\"\"Expand the numbers present inside the text. That means converting \"42\" into \"forty two\". It also detects if the number is ordinal automatically. Args: text : Input text language : Language used to expand the numbers. Defaults to \"en\". Returns: Output text \"\"\" number_regex = re . compile ( r \"\\d+\u00ba*\" ) all_numbers = number_regex . findall ( text ) for num in all_numbers : if \"\u00ba\" in num : pure_number = num . replace ( \"\u00ba\" , \"\" ) . strip () expanded = num2words ( int ( pure_number ), lang = language , to = \"ordinal\" ) else : expanded = num2words ( int ( num ), lang = language ) text = text . replace ( num , expanded ) return text lower_text ( text ) Transform all the text to lowercase. Parameters: Name Type Description Default text str Input text required Returns: Type Description str Output text Source code in thunder/text_processing/preprocess.py def lower_text ( text : str ) -> str : \"\"\"Transform all the text to lowercase. Args: text : Input text Returns: Output text \"\"\" return text . lower () normalize_text ( text ) Normalize the text to remove accents and ensure all the characters are valid ascii symbols. Parameters: Name Type Description Default text str Input text required Returns: Type Description str Output text Source code in thunder/text_processing/preprocess.py def normalize_text ( text : str ) -> str : \"\"\"Normalize the text to remove accents and ensure all the characters are valid ascii symbols. Args: text : Input text Returns: Output text \"\"\" nfkd_form = unicodedata . normalize ( \"NFKD\" , text ) only_ascii = nfkd_form . encode ( \"ASCII\" , \"ignore\" ) return only_ascii . decode ()","title":"Preprocess"},{"location":"api/Text%20Processing/preprocess/#thunder.text_processing.preprocess.expand_numbers","text":"Expand the numbers present inside the text. That means converting \"42\" into \"forty two\". It also detects if the number is ordinal automatically. Parameters: Name Type Description Default text str Input text required language str Language used to expand the numbers. Defaults to \"en\". 'en' Returns: Type Description str Output text Source code in thunder/text_processing/preprocess.py def expand_numbers ( text : str , language : str = \"en\" ) -> str : \"\"\"Expand the numbers present inside the text. That means converting \"42\" into \"forty two\". It also detects if the number is ordinal automatically. Args: text : Input text language : Language used to expand the numbers. Defaults to \"en\". Returns: Output text \"\"\" number_regex = re . compile ( r \"\\d+\u00ba*\" ) all_numbers = number_regex . findall ( text ) for num in all_numbers : if \"\u00ba\" in num : pure_number = num . replace ( \"\u00ba\" , \"\" ) . strip () expanded = num2words ( int ( pure_number ), lang = language , to = \"ordinal\" ) else : expanded = num2words ( int ( num ), lang = language ) text = text . replace ( num , expanded ) return text","title":"expand_numbers()"},{"location":"api/Text%20Processing/preprocess/#thunder.text_processing.preprocess.lower_text","text":"Transform all the text to lowercase. Parameters: Name Type Description Default text str Input text required Returns: Type Description str Output text Source code in thunder/text_processing/preprocess.py def lower_text ( text : str ) -> str : \"\"\"Transform all the text to lowercase. Args: text : Input text Returns: Output text \"\"\" return text . lower ()","title":"lower_text()"},{"location":"api/Text%20Processing/preprocess/#thunder.text_processing.preprocess.normalize_text","text":"Normalize the text to remove accents and ensure all the characters are valid ascii symbols. Parameters: Name Type Description Default text str Input text required Returns: Type Description str Output text Source code in thunder/text_processing/preprocess.py def normalize_text ( text : str ) -> str : \"\"\"Normalize the text to remove accents and ensure all the characters are valid ascii symbols. Args: text : Input text Returns: Output text \"\"\" nfkd_form = unicodedata . normalize ( \"NFKD\" , text ) only_ascii = nfkd_form . encode ( \"ASCII\" , \"ignore\" ) return only_ascii . decode ()","title":"normalize_text()"},{"location":"api/Text%20Processing/tokenize/","text":"char_tokenizer ( text ) Tokenize input text splitting into characters Parameters: Name Type Description Default text str Input text required Returns: Type Description List[str] Tokenized text Source code in thunder/text_processing/tokenizer.py def char_tokenizer ( text : str ) -> List [ str ]: \"\"\"Tokenize input text splitting into characters Args: text : Input text Returns: Tokenized text \"\"\" return list ( text ) get_most_frequent_tokens ( corpus , tokenize_function , minimum_frequency = 1 , max_number_of_tokens = None ) Helper function to get the most frequent tokens from a text corpus. Parameters: Name Type Description Default corpus str Text corpus to be used, this is a long string containing all of your text required tokenize_function Callable Same tokenizer function that will be used during training required minimum_frequency int Remove any token with frequency less than that. Defaults to 1. 1 max_number_of_tokens Optional[int] Optionally limit to the K most frequent tokens. Defaults to None. None Returns: Type Description List[str] All of the unique, most frequent tokens, ordered by frequency. Source code in thunder/text_processing/tokenizer.py def get_most_frequent_tokens ( corpus : str , tokenize_function : Callable , minimum_frequency : int = 1 , max_number_of_tokens : Optional [ int ] = None , ) -> List [ str ]: \"\"\"Helper function to get the most frequent tokens from a text corpus. Args: corpus : Text corpus to be used, this is a long string containing all of your text tokenize_function : Same tokenizer function that will be used during training minimum_frequency : Remove any token with frequency less than that. Defaults to 1. max_number_of_tokens : Optionally limit to the K most frequent tokens. Defaults to None. Returns: All of the unique, most frequent tokens, ordered by frequency. \"\"\" tokenized = tokenize_function ( corpus ) token_counter = Counter ( tokenized ) output_tokens = [] for token , count in token_counter . most_common ( max_number_of_tokens ): if count >= minimum_frequency : output_tokens . append ( token ) return output_tokens word_tokenizer ( text ) Tokenize input text splitting into words Parameters: Name Type Description Default text str Input text required Returns: Type Description List[str] Tokenized text Source code in thunder/text_processing/tokenizer.py def word_tokenizer ( text : str ) -> List [ str ]: \"\"\"Tokenize input text splitting into words Args: text : Input text Returns: Tokenized text \"\"\" return text . split ()","title":"Tokenize"},{"location":"api/Text%20Processing/tokenize/#thunder.text_processing.tokenizer.char_tokenizer","text":"Tokenize input text splitting into characters Parameters: Name Type Description Default text str Input text required Returns: Type Description List[str] Tokenized text Source code in thunder/text_processing/tokenizer.py def char_tokenizer ( text : str ) -> List [ str ]: \"\"\"Tokenize input text splitting into characters Args: text : Input text Returns: Tokenized text \"\"\" return list ( text )","title":"char_tokenizer()"},{"location":"api/Text%20Processing/tokenize/#thunder.text_processing.tokenizer.get_most_frequent_tokens","text":"Helper function to get the most frequent tokens from a text corpus. Parameters: Name Type Description Default corpus str Text corpus to be used, this is a long string containing all of your text required tokenize_function Callable Same tokenizer function that will be used during training required minimum_frequency int Remove any token with frequency less than that. Defaults to 1. 1 max_number_of_tokens Optional[int] Optionally limit to the K most frequent tokens. Defaults to None. None Returns: Type Description List[str] All of the unique, most frequent tokens, ordered by frequency. Source code in thunder/text_processing/tokenizer.py def get_most_frequent_tokens ( corpus : str , tokenize_function : Callable , minimum_frequency : int = 1 , max_number_of_tokens : Optional [ int ] = None , ) -> List [ str ]: \"\"\"Helper function to get the most frequent tokens from a text corpus. Args: corpus : Text corpus to be used, this is a long string containing all of your text tokenize_function : Same tokenizer function that will be used during training minimum_frequency : Remove any token with frequency less than that. Defaults to 1. max_number_of_tokens : Optionally limit to the K most frequent tokens. Defaults to None. Returns: All of the unique, most frequent tokens, ordered by frequency. \"\"\" tokenized = tokenize_function ( corpus ) token_counter = Counter ( tokenized ) output_tokens = [] for token , count in token_counter . most_common ( max_number_of_tokens ): if count >= minimum_frequency : output_tokens . append ( token ) return output_tokens","title":"get_most_frequent_tokens()"},{"location":"api/Text%20Processing/tokenize/#thunder.text_processing.tokenizer.word_tokenizer","text":"Tokenize input text splitting into words Parameters: Name Type Description Default text str Input text required Returns: Type Description List[str] Tokenized text Source code in thunder/text_processing/tokenizer.py def word_tokenizer ( text : str ) -> List [ str ]: \"\"\"Tokenize input text splitting into words Args: text : Input text Returns: Tokenized text \"\"\" return text . split ()","title":"word_tokenizer()"},{"location":"api/Text%20Processing/transform/","text":"BatchTextTransformer __init__ ( self , vocab , tokenize_func =< function char_tokenizer at 0x7f74dda1d5e0 > , after_tokenize = None , after_numericalize = None ) special That class is the glue code that uses all of the text processing stuff to encode an entire batch of text at once. Parameters: Name Type Description Default vocab Vocab Vocabulary to be used required tokenize_func Function that will perform the tokenization of each individual text sample. Defaults to char_tokenizer. <function char_tokenizer at 0x7f74dda1d5e0> after_tokenize Functions to be applied after the tokenization but before numericalization. Defaults to None. None after_numericalize Functions to be applied at the end of the pipeline. Defaults to torch.LongTensor. None Source code in thunder/text_processing/transform.py def __init__ ( self , vocab : Vocab , tokenize_func = char_tokenizer , after_tokenize = None , after_numericalize = None , ): \"\"\"That class is the glue code that uses all of the text processing stuff to encode an entire batch of text at once. Args: vocab : Vocabulary to be used tokenize_func : Function that will perform the tokenization of each individual text sample. Defaults to char_tokenizer. after_tokenize : Functions to be applied after the tokenization but before numericalization. Defaults to None. after_numericalize : Functions to be applied at the end of the pipeline. Defaults to torch.LongTensor. \"\"\" super () . __init__ () self . vocab = vocab self . tokenize_func = tokenize_func self . after_tokenize = after_tokenize self . after_numericalize = after_numericalize decode_prediction ( self , predictions ) Parameters: Name Type Description Default predictions Tensor Tensor of shape (batch, time) required Returns: Type Description List[str] A list of decoded strings, one for each element in the batch. Source code in thunder/text_processing/transform.py @torch . jit . export def decode_prediction ( self , predictions : torch . Tensor ) -> List [ str ]: \"\"\" Args: predictions : Tensor of shape (batch, time) Returns: A list of decoded strings, one for each element in the batch. \"\"\" out_list : List [ str ] = [] for element in predictions : # Remove consecutive repeated elements element = torch . unique_consecutive ( element ) # Map back to string out = self . vocab . decode_into_text ( element ) # Join prediction into one string out = \"\" . join ( out ) # Remove the blank and pad token from output out = out . replace ( self . vocab . blank_token , \"\" ) out = out . replace ( self . vocab . pad_token , \"\" ) out = out . replace ( self . vocab . start_token , \"\" ) out = out . replace ( self . vocab . end_token , \"\" ) out_list . append ( out ) return out_list","title":"Transform"},{"location":"api/Text%20Processing/transform/#thunder.text_processing.transform.BatchTextTransformer","text":"","title":"BatchTextTransformer"},{"location":"api/Text%20Processing/transform/#thunder.text_processing.transform.BatchTextTransformer.__init__","text":"That class is the glue code that uses all of the text processing stuff to encode an entire batch of text at once. Parameters: Name Type Description Default vocab Vocab Vocabulary to be used required tokenize_func Function that will perform the tokenization of each individual text sample. Defaults to char_tokenizer. <function char_tokenizer at 0x7f74dda1d5e0> after_tokenize Functions to be applied after the tokenization but before numericalization. Defaults to None. None after_numericalize Functions to be applied at the end of the pipeline. Defaults to torch.LongTensor. None Source code in thunder/text_processing/transform.py def __init__ ( self , vocab : Vocab , tokenize_func = char_tokenizer , after_tokenize = None , after_numericalize = None , ): \"\"\"That class is the glue code that uses all of the text processing stuff to encode an entire batch of text at once. Args: vocab : Vocabulary to be used tokenize_func : Function that will perform the tokenization of each individual text sample. Defaults to char_tokenizer. after_tokenize : Functions to be applied after the tokenization but before numericalization. Defaults to None. after_numericalize : Functions to be applied at the end of the pipeline. Defaults to torch.LongTensor. \"\"\" super () . __init__ () self . vocab = vocab self . tokenize_func = tokenize_func self . after_tokenize = after_tokenize self . after_numericalize = after_numericalize","title":"__init__()"},{"location":"api/Text%20Processing/transform/#thunder.text_processing.transform.BatchTextTransformer.decode_prediction","text":"Parameters: Name Type Description Default predictions Tensor Tensor of shape (batch, time) required Returns: Type Description List[str] A list of decoded strings, one for each element in the batch. Source code in thunder/text_processing/transform.py @torch . jit . export def decode_prediction ( self , predictions : torch . Tensor ) -> List [ str ]: \"\"\" Args: predictions : Tensor of shape (batch, time) Returns: A list of decoded strings, one for each element in the batch. \"\"\" out_list : List [ str ] = [] for element in predictions : # Remove consecutive repeated elements element = torch . unique_consecutive ( element ) # Map back to string out = self . vocab . decode_into_text ( element ) # Join prediction into one string out = \"\" . join ( out ) # Remove the blank and pad token from output out = out . replace ( self . vocab . blank_token , \"\" ) out = out . replace ( self . vocab . pad_token , \"\" ) out = out . replace ( self . vocab . start_token , \"\" ) out = out . replace ( self . vocab . end_token , \"\" ) out_list . append ( out ) return out_list","title":"decode_prediction()"},{"location":"api/Text%20Processing/vocab/","text":"Vocab __init__ ( self , initial_vocab_tokens , pad_token = '<pad>' , unknown_token = '<unk>' , start_token = '<bos>' , end_token = '<eos>' , nemo_compat = False ) special Class that represents a vocabulary, with the related methods to numericalize a sequence of tokens into numbers, and do the reverse mapping of numbers back to tokens. Parameters: Name Type Description Default initial_vocab_tokens List[str] Basic list of tokens that will be part of the vocabulary. DO NOT INCLUDE SPECIAL TOKENS THERE. Even the blank is automatically added by the class. Check docs required pad_token str Token that will represent padding. '<pad>' unknown_token str Token that will represent unknown elements. Notice that this is different than the blank used by ctc. '<unk>' start_token str Token that will represent the beginning of the sequence. '<bos>' end_token str Token that will represent the end of the sequence. '<eos>' nemo_compat bool Compatibility mode to work with original Nemo models. False Source code in thunder/text_processing/vocab.py def __init__ ( self , initial_vocab_tokens : List [ str ], pad_token : str = \"<pad>\" , unknown_token : str = \"<unk>\" , start_token : str = \"<bos>\" , end_token : str = \"<eos>\" , nemo_compat : bool = False , ): \"\"\"Class that represents a vocabulary, with the related methods to numericalize a sequence of tokens into numbers, and do the reverse mapping of numbers back to tokens. Args: initial_vocab_tokens : Basic list of tokens that will be part of the vocabulary. DO NOT INCLUDE SPECIAL TOKENS THERE. Even the blank is automatically added by the class. Check [`docs`](https://scart97.github.io/thunder-speech/quick%20reference%20guide/#how-to-get-the-initial_vocab_tokens-from-my-dataset) pad_token : Token that will represent padding. unknown_token : Token that will represent unknown elements. Notice that this is different than the blank used by ctc. start_token : Token that will represent the beginning of the sequence. end_token : Token that will represent the end of the sequence. nemo_compat: Compatibility mode to work with original Nemo models. \"\"\" super () . __init__ () self . pad_token = pad_token self . unknown_token = unknown_token self . start_token = start_token self . end_token = end_token # There's no problem if the blank_idx == pad_idx self . blank_token = self . pad_token self . nemo_compat = nemo_compat self . itos = initial_vocab_tokens + [ pad_token , unknown_token , start_token , end_token , ] self . stoi = { token : i for i , token in enumerate ( self . itos )} self . update_special_idx () add_special_tokens ( self , tokens ) Function to add the special start and end tokens to some tokenized text. Parameters: Name Type Description Default tokens List[str] Tokenized text required Returns: Type Description List[str] Text with the special tokens added. Source code in thunder/text_processing/vocab.py @torch . jit . export def add_special_tokens ( self , tokens : List [ str ]) -> List [ str ]: \"\"\"Function to add the special start and end tokens to some tokenized text. Args: tokens : Tokenized text Returns: Text with the special tokens added. \"\"\" if self . nemo_compat : return tokens return [ self . start_token ] + tokens + [ self . end_token ] decode_into_text ( self , indices ) Function to transform back a list of numbers into the corresponding tokens. Parameters: Name Type Description Default indices Tensor Numeric representation. Usually is the result of the model, after a greedy decoding required Returns: Type Description List[str] Corresponding tokens Source code in thunder/text_processing/vocab.py @torch . jit . export def decode_into_text ( self , indices : torch . Tensor ) -> List [ str ]: \"\"\"Function to transform back a list of numbers into the corresponding tokens. Args: indices : Numeric representation. Usually is the result of the model, after a greedy decoding Returns: Corresponding tokens \"\"\" return [ self . itos [ it ] for it in indices ] numericalize ( self , tokens ) Function to transform a list of tokens into the corresponding numeric representation. Parameters: Name Type Description Default tokens List[str] A single list of tokens to be transformed required Returns: Type Description Tensor The corresponding numeric representation Source code in thunder/text_processing/vocab.py def numericalize ( self , tokens : List [ str ]) -> torch . Tensor : \"\"\"Function to transform a list of tokens into the corresponding numeric representation. Args: tokens : A single list of tokens to be transformed Returns: The corresponding numeric representation \"\"\" if self . nemo_compat : # When in nemo_compat mode, there's no unknown token # So we filter out all of the tokens not in the vocab tokens = filter ( lambda x : x in self . itos , tokens ) return torch . tensor ( [ self . stoi . get ( it , self . unknown_idx ) for it in tokens ], dtype = torch . long )","title":"Vocab"},{"location":"api/Text%20Processing/vocab/#thunder.text_processing.vocab.Vocab","text":"","title":"Vocab"},{"location":"api/Text%20Processing/vocab/#thunder.text_processing.vocab.Vocab.__init__","text":"Class that represents a vocabulary, with the related methods to numericalize a sequence of tokens into numbers, and do the reverse mapping of numbers back to tokens. Parameters: Name Type Description Default initial_vocab_tokens List[str] Basic list of tokens that will be part of the vocabulary. DO NOT INCLUDE SPECIAL TOKENS THERE. Even the blank is automatically added by the class. Check docs required pad_token str Token that will represent padding. '<pad>' unknown_token str Token that will represent unknown elements. Notice that this is different than the blank used by ctc. '<unk>' start_token str Token that will represent the beginning of the sequence. '<bos>' end_token str Token that will represent the end of the sequence. '<eos>' nemo_compat bool Compatibility mode to work with original Nemo models. False Source code in thunder/text_processing/vocab.py def __init__ ( self , initial_vocab_tokens : List [ str ], pad_token : str = \"<pad>\" , unknown_token : str = \"<unk>\" , start_token : str = \"<bos>\" , end_token : str = \"<eos>\" , nemo_compat : bool = False , ): \"\"\"Class that represents a vocabulary, with the related methods to numericalize a sequence of tokens into numbers, and do the reverse mapping of numbers back to tokens. Args: initial_vocab_tokens : Basic list of tokens that will be part of the vocabulary. DO NOT INCLUDE SPECIAL TOKENS THERE. Even the blank is automatically added by the class. Check [`docs`](https://scart97.github.io/thunder-speech/quick%20reference%20guide/#how-to-get-the-initial_vocab_tokens-from-my-dataset) pad_token : Token that will represent padding. unknown_token : Token that will represent unknown elements. Notice that this is different than the blank used by ctc. start_token : Token that will represent the beginning of the sequence. end_token : Token that will represent the end of the sequence. nemo_compat: Compatibility mode to work with original Nemo models. \"\"\" super () . __init__ () self . pad_token = pad_token self . unknown_token = unknown_token self . start_token = start_token self . end_token = end_token # There's no problem if the blank_idx == pad_idx self . blank_token = self . pad_token self . nemo_compat = nemo_compat self . itos = initial_vocab_tokens + [ pad_token , unknown_token , start_token , end_token , ] self . stoi = { token : i for i , token in enumerate ( self . itos )} self . update_special_idx ()","title":"__init__()"},{"location":"api/Text%20Processing/vocab/#thunder.text_processing.vocab.Vocab.add_special_tokens","text":"Function to add the special start and end tokens to some tokenized text. Parameters: Name Type Description Default tokens List[str] Tokenized text required Returns: Type Description List[str] Text with the special tokens added. Source code in thunder/text_processing/vocab.py @torch . jit . export def add_special_tokens ( self , tokens : List [ str ]) -> List [ str ]: \"\"\"Function to add the special start and end tokens to some tokenized text. Args: tokens : Tokenized text Returns: Text with the special tokens added. \"\"\" if self . nemo_compat : return tokens return [ self . start_token ] + tokens + [ self . end_token ]","title":"add_special_tokens()"},{"location":"api/Text%20Processing/vocab/#thunder.text_processing.vocab.Vocab.decode_into_text","text":"Function to transform back a list of numbers into the corresponding tokens. Parameters: Name Type Description Default indices Tensor Numeric representation. Usually is the result of the model, after a greedy decoding required Returns: Type Description List[str] Corresponding tokens Source code in thunder/text_processing/vocab.py @torch . jit . export def decode_into_text ( self , indices : torch . Tensor ) -> List [ str ]: \"\"\"Function to transform back a list of numbers into the corresponding tokens. Args: indices : Numeric representation. Usually is the result of the model, after a greedy decoding Returns: Corresponding tokens \"\"\" return [ self . itos [ it ] for it in indices ]","title":"decode_into_text()"},{"location":"api/Text%20Processing/vocab/#thunder.text_processing.vocab.Vocab.numericalize","text":"Function to transform a list of tokens into the corresponding numeric representation. Parameters: Name Type Description Default tokens List[str] A single list of tokens to be transformed required Returns: Type Description Tensor The corresponding numeric representation Source code in thunder/text_processing/vocab.py def numericalize ( self , tokens : List [ str ]) -> torch . Tensor : \"\"\"Function to transform a list of tokens into the corresponding numeric representation. Args: tokens : A single list of tokens to be transformed Returns: The corresponding numeric representation \"\"\" if self . nemo_compat : # When in nemo_compat mode, there's no unknown token # So we filter out all of the tokens not in the vocab tokens = filter ( lambda x : x in self . itos , tokens ) return torch . tensor ( [ self . stoi . get ( it , self . unknown_idx ) for it in tokens ], dtype = torch . long )","title":"numericalize()"},{"location":"api/Wav2Vec/module/","text":"Wav2Vec2Module __init__ ( self , initial_vocab_tokens , model_name = 'facebook/wav2vec2-base' , gradient_checkpointing = False , decoder_dropout = 0.1 , learning_rate = 0.0003 , ** kwargs ) special Wav2Vec model for fine-tuning. Parameters: Name Type Description Default initial_vocab_tokens List[str] List of tokens to be used in the vocab, special tokens should not be included here. Check docs required model_name str Name of the original huggingface checkpoint to load from. 'facebook/wav2vec2-base' gradient_checkpointing bool Use gradient checkpointing to save memory at the expense of slower backward pass. False decoder_dropout float Dropout before the final decoding layer 0.1 learning_rate float Learning rate used on the optimizer. 0.0003 kwargs Dict[str, Any] Any other option that can be passed to the original Wav2Vec2Model.from_pretrained {} Source code in thunder/wav2vec/module.py def __init__ ( self , initial_vocab_tokens : List [ str ], model_name : str = \"facebook/wav2vec2-base\" , gradient_checkpointing : bool = False , decoder_dropout : float = 0.1 , learning_rate : float = 3e-4 , ** kwargs : Dict [ str , Any ], ): \"\"\"Wav2Vec model for fine-tuning. Args: initial_vocab_tokens : List of tokens to be used in the vocab, special tokens should not be included here. Check [`docs`](https://scart97.github.io/thunder-speech/quick%20reference%20guide/#how-to-get-the-initial_vocab_tokens-from-my-dataset) model_name : Name of the original huggingface checkpoint to load from. gradient_checkpointing : Use gradient checkpointing to save memory at the expense of slower backward pass. decoder_dropout : Dropout before the final decoding layer learning_rate : Learning rate used on the optimizer. kwargs: Any other option that can be passed to the original Wav2Vec2Model.from_pretrained \"\"\" super () . __init__ () self . save_hyperparameters () self . audio_transform = Wav2Vec2Preprocess () self . encoder = Wav2Vec2Model . from_pretrained ( model_name , gradient_checkpointing = gradient_checkpointing , ** kwargs , ) self . encoder . feature_extractor . _freeze_parameters () self . text_transform = self . build_text_transform ( initial_vocab_tokens ) self . decoder = self . build_decoder ( decoder_dropout , self . encoder . config . hidden_size , len ( self . text_transform . vocab ), ) # Metrics self . val_cer = CER () self . val_wer = WER () # Example input is one second of fake audio self . example_input_array = torch . randn (( 10 , 16000 )) build_decoder ( self , decoder_dropout , decoder_input_channels , vocab_size ) Overwrite this function if you want to change the model decoder. Parameters: Name Type Description Default decoder_dropout float Amount of dropout to be used in the decoder required decoder_input_channels int Number of input channels of the decoder. That is the number of channels of the features created by the encoder. required vocab_size int Number of output classes required Returns: Type Description Module Module that represents the decoder. Source code in thunder/wav2vec/module.py def build_decoder ( self , decoder_dropout : float , decoder_input_channels : int , vocab_size : int ) -> nn . Module : \"\"\"Overwrite this function if you want to change the model decoder. Args: decoder_dropout: Amount of dropout to be used in the decoder decoder_input_channels : Number of input channels of the decoder. That is the number of channels of the features created by the encoder. vocab_size : Number of output classes Returns: Module that represents the decoder. \"\"\" return nn . Sequential ( nn . Dropout ( decoder_dropout ), nn . Linear ( decoder_input_channels , vocab_size ), ) build_text_transform ( self , initial_vocab_tokens ) Overwrite this function if you want to change how the text processing happens inside the model. Parameters: Name Type Description Default initial_vocab_tokens List[str] List of tokens to create the vocabulary, special tokens should not be included here. required Returns: Type Description BatchTextTransformer The transform that will both encode the text and decode_prediction . Source code in thunder/wav2vec/module.py def build_text_transform ( self , initial_vocab_tokens : List [ str ] ) -> BatchTextTransformer : \"\"\"Overwrite this function if you want to change how the text processing happens inside the model. Args: initial_vocab_tokens : List of tokens to create the vocabulary, special tokens should not be included here. Returns: The transform that will both `encode` the text and `decode_prediction`. \"\"\" vocab = Vocab ( initial_vocab_tokens , nemo_compat = False ) return BatchTextTransformer ( vocab = vocab ) configure_optimizers ( self ) Configuring optimizers. Check the original lightning docs for more info. Returns: Type Description Optimizer Optimizer, and optionally the learning rate scheduler. Source code in thunder/wav2vec/module.py def configure_optimizers ( self ) -> torch . optim . Optimizer : \"\"\"Configuring optimizers. Check the original lightning docs for more info. Returns: Optimizer, and optionally the learning rate scheduler. \"\"\" return torch . optim . Adam ( filter ( lambda p : p . requires_grad , self . parameters ()), lr = self . hparams . learning_rate , ) forward ( self , audio ) Process the audio tensor to create the probabilities. Parameters: Name Type Description Default audio Tensor Audio tensor of shape [batch_size, time] required Returns: Type Description Tensor Tensor with the prediction probabilities. Source code in thunder/wav2vec/module.py def forward ( self , audio : Tensor ) -> Tensor : \"\"\"Process the audio tensor to create the probabilities. Args: audio : Audio tensor of shape [batch_size, time] Returns: Tensor with the prediction probabilities. \"\"\" features = self . audio_transform ( audio ) encoded_dict = self . encoder ( features ) probs = self . decoder ( encoded_dict . last_hidden_state ) # Change from (batch, time, #vocab) to (batch, #vocab, time) # that is expected by the rest of the library return probs . permute ( 0 , 2 , 1 ) predict ( self , x ) Use this function during inference to predict. Parameters: Name Type Description Default x Tensor Audio tensor of shape [batch_size, time] required Returns: Type Description List[str] A list of strings, each one contains the corresponding transcription to the original batch element. Source code in thunder/wav2vec/module.py @torch . jit . export def predict ( self , x : Tensor ) -> List [ str ]: \"\"\"Use this function during inference to predict. Args: x : Audio tensor of shape [batch_size, time] Returns: A list of strings, each one contains the corresponding transcription to the original batch element. \"\"\" pred = self ( x ) return self . text_transform . decode_prediction ( pred . argmax ( 1 )) training_step ( self , batch , batch_idx ) Training step. Check the original lightning docs for more information. Parameters: Name Type Description Default batch Tuple[torch.Tensor, torch.Tensor, List[str]] Tuple containing the batched audios, normalized lengths and the corresponding text labels. required batch_idx int Batch index required Returns: Type Description Tensor Training loss for that batch Source code in thunder/wav2vec/module.py def training_step ( self , batch : Tuple [ torch . Tensor , torch . Tensor , List [ str ]], batch_idx : int ) -> torch . Tensor : \"\"\"Training step. Check the original lightning docs for more information. Args: batch : Tuple containing the batched audios, normalized lengths and the corresponding text labels. batch_idx : Batch index Returns: Training loss for that batch \"\"\" audio , audio_lens , texts = batch y , y_lens = self . text_transform . encode ( texts , device = self . device ) probabilities = self ( audio ) loss = self . calculate_loss ( probabilities , y , audio_lens , y_lens ) self . log ( \"loss/train_loss\" , loss ) return loss validation_step ( self , batch , batch_idx ) Validation step. Check the original lightning docs for more information. Parameters: Name Type Description Default batch Tuple[torch.Tensor, torch.Tensor, List[str]] Tuple containing the batched audios, normalized lengths and the corresponding text labels. required batch_idx int Batch index required Returns: Type Description Tensor Validation loss for that batch Source code in thunder/wav2vec/module.py def validation_step ( self , batch : Tuple [ torch . Tensor , torch . Tensor , List [ str ]], batch_idx : int ) -> torch . Tensor : \"\"\"Validation step. Check the original lightning docs for more information. Args: batch : Tuple containing the batched audios, normalized lengths and the corresponding text labels. batch_idx : Batch index Returns: Validation loss for that batch \"\"\" audio , audio_lens , texts = batch y , y_lens = self . text_transform . encode ( texts , device = self . device ) probabilities = self ( audio ) loss = self . calculate_loss ( probabilities , y , audio_lens , y_lens ) decoded_preds = self . text_transform . decode_prediction ( probabilities . argmax ( 1 )) decoded_targets = self . text_transform . decode_prediction ( y ) self . val_cer ( decoded_preds , decoded_targets ) self . val_wer ( decoded_preds , decoded_targets ) self . log ( \"loss/val_loss\" , loss ) self . log ( \"metrics/cer\" , self . val_cer , on_epoch = True ) self . log ( \"metrics/wer\" , self . val_wer , on_epoch = True ) return loss","title":"Module"},{"location":"api/Wav2Vec/module/#thunder.wav2vec.module.Wav2Vec2Module","text":"","title":"Wav2Vec2Module"},{"location":"api/Wav2Vec/module/#thunder.wav2vec.module.Wav2Vec2Module.__init__","text":"Wav2Vec model for fine-tuning. Parameters: Name Type Description Default initial_vocab_tokens List[str] List of tokens to be used in the vocab, special tokens should not be included here. Check docs required model_name str Name of the original huggingface checkpoint to load from. 'facebook/wav2vec2-base' gradient_checkpointing bool Use gradient checkpointing to save memory at the expense of slower backward pass. False decoder_dropout float Dropout before the final decoding layer 0.1 learning_rate float Learning rate used on the optimizer. 0.0003 kwargs Dict[str, Any] Any other option that can be passed to the original Wav2Vec2Model.from_pretrained {} Source code in thunder/wav2vec/module.py def __init__ ( self , initial_vocab_tokens : List [ str ], model_name : str = \"facebook/wav2vec2-base\" , gradient_checkpointing : bool = False , decoder_dropout : float = 0.1 , learning_rate : float = 3e-4 , ** kwargs : Dict [ str , Any ], ): \"\"\"Wav2Vec model for fine-tuning. Args: initial_vocab_tokens : List of tokens to be used in the vocab, special tokens should not be included here. Check [`docs`](https://scart97.github.io/thunder-speech/quick%20reference%20guide/#how-to-get-the-initial_vocab_tokens-from-my-dataset) model_name : Name of the original huggingface checkpoint to load from. gradient_checkpointing : Use gradient checkpointing to save memory at the expense of slower backward pass. decoder_dropout : Dropout before the final decoding layer learning_rate : Learning rate used on the optimizer. kwargs: Any other option that can be passed to the original Wav2Vec2Model.from_pretrained \"\"\" super () . __init__ () self . save_hyperparameters () self . audio_transform = Wav2Vec2Preprocess () self . encoder = Wav2Vec2Model . from_pretrained ( model_name , gradient_checkpointing = gradient_checkpointing , ** kwargs , ) self . encoder . feature_extractor . _freeze_parameters () self . text_transform = self . build_text_transform ( initial_vocab_tokens ) self . decoder = self . build_decoder ( decoder_dropout , self . encoder . config . hidden_size , len ( self . text_transform . vocab ), ) # Metrics self . val_cer = CER () self . val_wer = WER () # Example input is one second of fake audio self . example_input_array = torch . randn (( 10 , 16000 ))","title":"__init__()"},{"location":"api/Wav2Vec/module/#thunder.wav2vec.module.Wav2Vec2Module.build_decoder","text":"Overwrite this function if you want to change the model decoder. Parameters: Name Type Description Default decoder_dropout float Amount of dropout to be used in the decoder required decoder_input_channels int Number of input channels of the decoder. That is the number of channels of the features created by the encoder. required vocab_size int Number of output classes required Returns: Type Description Module Module that represents the decoder. Source code in thunder/wav2vec/module.py def build_decoder ( self , decoder_dropout : float , decoder_input_channels : int , vocab_size : int ) -> nn . Module : \"\"\"Overwrite this function if you want to change the model decoder. Args: decoder_dropout: Amount of dropout to be used in the decoder decoder_input_channels : Number of input channels of the decoder. That is the number of channels of the features created by the encoder. vocab_size : Number of output classes Returns: Module that represents the decoder. \"\"\" return nn . Sequential ( nn . Dropout ( decoder_dropout ), nn . Linear ( decoder_input_channels , vocab_size ), )","title":"build_decoder()"},{"location":"api/Wav2Vec/module/#thunder.wav2vec.module.Wav2Vec2Module.build_text_transform","text":"Overwrite this function if you want to change how the text processing happens inside the model. Parameters: Name Type Description Default initial_vocab_tokens List[str] List of tokens to create the vocabulary, special tokens should not be included here. required Returns: Type Description BatchTextTransformer The transform that will both encode the text and decode_prediction . Source code in thunder/wav2vec/module.py def build_text_transform ( self , initial_vocab_tokens : List [ str ] ) -> BatchTextTransformer : \"\"\"Overwrite this function if you want to change how the text processing happens inside the model. Args: initial_vocab_tokens : List of tokens to create the vocabulary, special tokens should not be included here. Returns: The transform that will both `encode` the text and `decode_prediction`. \"\"\" vocab = Vocab ( initial_vocab_tokens , nemo_compat = False ) return BatchTextTransformer ( vocab = vocab )","title":"build_text_transform()"},{"location":"api/Wav2Vec/module/#thunder.wav2vec.module.Wav2Vec2Module.configure_optimizers","text":"Configuring optimizers. Check the original lightning docs for more info. Returns: Type Description Optimizer Optimizer, and optionally the learning rate scheduler. Source code in thunder/wav2vec/module.py def configure_optimizers ( self ) -> torch . optim . Optimizer : \"\"\"Configuring optimizers. Check the original lightning docs for more info. Returns: Optimizer, and optionally the learning rate scheduler. \"\"\" return torch . optim . Adam ( filter ( lambda p : p . requires_grad , self . parameters ()), lr = self . hparams . learning_rate , )","title":"configure_optimizers()"},{"location":"api/Wav2Vec/module/#thunder.wav2vec.module.Wav2Vec2Module.forward","text":"Process the audio tensor to create the probabilities. Parameters: Name Type Description Default audio Tensor Audio tensor of shape [batch_size, time] required Returns: Type Description Tensor Tensor with the prediction probabilities. Source code in thunder/wav2vec/module.py def forward ( self , audio : Tensor ) -> Tensor : \"\"\"Process the audio tensor to create the probabilities. Args: audio : Audio tensor of shape [batch_size, time] Returns: Tensor with the prediction probabilities. \"\"\" features = self . audio_transform ( audio ) encoded_dict = self . encoder ( features ) probs = self . decoder ( encoded_dict . last_hidden_state ) # Change from (batch, time, #vocab) to (batch, #vocab, time) # that is expected by the rest of the library return probs . permute ( 0 , 2 , 1 )","title":"forward()"},{"location":"api/Wav2Vec/module/#thunder.wav2vec.module.Wav2Vec2Module.predict","text":"Use this function during inference to predict. Parameters: Name Type Description Default x Tensor Audio tensor of shape [batch_size, time] required Returns: Type Description List[str] A list of strings, each one contains the corresponding transcription to the original batch element. Source code in thunder/wav2vec/module.py @torch . jit . export def predict ( self , x : Tensor ) -> List [ str ]: \"\"\"Use this function during inference to predict. Args: x : Audio tensor of shape [batch_size, time] Returns: A list of strings, each one contains the corresponding transcription to the original batch element. \"\"\" pred = self ( x ) return self . text_transform . decode_prediction ( pred . argmax ( 1 ))","title":"predict()"},{"location":"api/Wav2Vec/module/#thunder.wav2vec.module.Wav2Vec2Module.training_step","text":"Training step. Check the original lightning docs for more information. Parameters: Name Type Description Default batch Tuple[torch.Tensor, torch.Tensor, List[str]] Tuple containing the batched audios, normalized lengths and the corresponding text labels. required batch_idx int Batch index required Returns: Type Description Tensor Training loss for that batch Source code in thunder/wav2vec/module.py def training_step ( self , batch : Tuple [ torch . Tensor , torch . Tensor , List [ str ]], batch_idx : int ) -> torch . Tensor : \"\"\"Training step. Check the original lightning docs for more information. Args: batch : Tuple containing the batched audios, normalized lengths and the corresponding text labels. batch_idx : Batch index Returns: Training loss for that batch \"\"\" audio , audio_lens , texts = batch y , y_lens = self . text_transform . encode ( texts , device = self . device ) probabilities = self ( audio ) loss = self . calculate_loss ( probabilities , y , audio_lens , y_lens ) self . log ( \"loss/train_loss\" , loss ) return loss","title":"training_step()"},{"location":"api/Wav2Vec/module/#thunder.wav2vec.module.Wav2Vec2Module.validation_step","text":"Validation step. Check the original lightning docs for more information. Parameters: Name Type Description Default batch Tuple[torch.Tensor, torch.Tensor, List[str]] Tuple containing the batched audios, normalized lengths and the corresponding text labels. required batch_idx int Batch index required Returns: Type Description Tensor Validation loss for that batch Source code in thunder/wav2vec/module.py def validation_step ( self , batch : Tuple [ torch . Tensor , torch . Tensor , List [ str ]], batch_idx : int ) -> torch . Tensor : \"\"\"Validation step. Check the original lightning docs for more information. Args: batch : Tuple containing the batched audios, normalized lengths and the corresponding text labels. batch_idx : Batch index Returns: Validation loss for that batch \"\"\" audio , audio_lens , texts = batch y , y_lens = self . text_transform . encode ( texts , device = self . device ) probabilities = self ( audio ) loss = self . calculate_loss ( probabilities , y , audio_lens , y_lens ) decoded_preds = self . text_transform . decode_prediction ( probabilities . argmax ( 1 )) decoded_targets = self . text_transform . decode_prediction ( y ) self . val_cer ( decoded_preds , decoded_targets ) self . val_wer ( decoded_preds , decoded_targets ) self . log ( \"loss/val_loss\" , loss ) self . log ( \"metrics/cer\" , self . val_cer , on_epoch = True ) self . log ( \"metrics/wer\" , self . val_wer , on_epoch = True ) return loss","title":"validation_step()"},{"location":"api/Wav2Vec/transform/","text":"Wav2Vec2Preprocess __init__ ( self , div_guard = 1e-05 ) special Wav2Vec model preprocessing. It only consists of normalizing the audio. Parameters: Name Type Description Default div_guard float Guard value to prevent division by zero. 1e-05 Source code in thunder/wav2vec/transform.py def __init__ ( self , div_guard : float = 1e-5 ): \"\"\"Wav2Vec model preprocessing. It only consists of normalizing the audio. Args: div_guard : Guard value to prevent division by zero. \"\"\" super () . __init__ () self . div_guard = div_guard forward ( self , audio ) Applies the normalization Parameters: Name Type Description Default audio Tensor Audio tensor of shape [batch_size, time] required Returns: Type Description Tensor Normalized audio tensor with same shape as input Source code in thunder/wav2vec/transform.py def forward ( self , audio : torch . Tensor ) -> torch . Tensor : \"\"\"Applies the normalization Args: audio : Audio tensor of shape [batch_size, time] Returns: Normalized audio tensor with same shape as input \"\"\" mean = audio . mean ( 1 , keepdim = True ) . detach () std = ( audio . var ( 1 , keepdim = True ) . detach () + self . div_guard ) . sqrt () return ( audio - mean ) / std","title":"Transform"},{"location":"api/Wav2Vec/transform/#thunder.wav2vec.transform.Wav2Vec2Preprocess","text":"","title":"Wav2Vec2Preprocess"},{"location":"api/Wav2Vec/transform/#thunder.wav2vec.transform.Wav2Vec2Preprocess.__init__","text":"Wav2Vec model preprocessing. It only consists of normalizing the audio. Parameters: Name Type Description Default div_guard float Guard value to prevent division by zero. 1e-05 Source code in thunder/wav2vec/transform.py def __init__ ( self , div_guard : float = 1e-5 ): \"\"\"Wav2Vec model preprocessing. It only consists of normalizing the audio. Args: div_guard : Guard value to prevent division by zero. \"\"\" super () . __init__ () self . div_guard = div_guard","title":"__init__()"},{"location":"api/Wav2Vec/transform/#thunder.wav2vec.transform.Wav2Vec2Preprocess.forward","text":"Applies the normalization Parameters: Name Type Description Default audio Tensor Audio tensor of shape [batch_size, time] required Returns: Type Description Tensor Normalized audio tensor with same shape as input Source code in thunder/wav2vec/transform.py def forward ( self , audio : torch . Tensor ) -> torch . Tensor : \"\"\"Applies the normalization Args: audio : Audio tensor of shape [batch_size, time] Returns: Normalized audio tensor with same shape as input \"\"\" mean = audio . mean ( 1 , keepdim = True ) . detach () std = ( audio . var ( 1 , keepdim = True ) . detach () + self . div_guard ) . sqrt () return ( audio - mean ) / std","title":"forward()"}]}